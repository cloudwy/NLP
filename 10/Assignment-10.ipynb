{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习上课内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答一下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What is independent assumption in Naive bayes ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事件之间相互独立，没有关联联系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What is MAP(maximum a posterior) and ML(maximum likelihood) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAP: $h_{MAP}= \\mathop{\\arg\\max}_{h\\in H} p(h|D) = \\mathop{\\arg\\max}_{h\\in H}\\frac{p(D|h)·p(h)}{p(D)} = \\mathop{\\arg\\max}_{h\\in H} p(D|h)·p(h) $  \n",
    "\n",
    "若h分布均匀，可用ML: $h_{ML}=\\mathop{\\arg\\max}_{h\\in H} p(D|h)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is support vector in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "support vector为支持向量 -> 对边界确定有贡献的向量 -> $\\alpha_{i}>0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the intuition behind SVM ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个多维平面分布着正样本和负样本，如何找到一个超平面将正负样本分开。  \n",
    "- 超平面存在,能够将正负样本分开 -> 线性可分SVM\n",
    "- 超平面不存在 -> 正负样本彻底混乱不可分(核函数)，少数样本不可分(松弛变量)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Shortly describe what 'random' means in random forest ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 随机选取部分训练样本\n",
    "- 随机选取部分特征构建树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. What cariterion does XGBoost use to find the best split point in a tree ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择某个分割点 -> 计算该分割点下最优的权重$w_j=-\\frac{G_j}{Hj+\\lambda}$ -> 计算$obj=-\\frac{1}{2}\\sum_{j=1}^{T} \\frac{G_j^2}{H_j+\\lambda}+\\gamma T$ -> 最小的作为最佳分割点 -> 如果obj>0,则该树停止分裂 -> 建下一棵树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Practial part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem description: In this part you are going to build a classifier to detect if a piece of news is published by the Xinhua news agency (新华社）."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hints:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Firstly, you have to come up with a way to represent the news. (Vectorize the sentence, you can find different ways to do so online)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:  \n",
    "https://blog.csdn.net/miner_zhu/article/details/82844922  \n",
    "https://blog.csdn.net/XnCSD/article/details/86742377  \n",
    "https://www.cnblogs.com/jiangxinyang/category/1374701.html 文本分类实战  \n",
    "https://github.com/wavewangyue/text-classification 机器学习大乱斗 2017.05.05  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[1]Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filename = r'.\\data\\sqlResult_1558435.csv'\n",
    "content = pd.read_csv(filename,encoding='gb18030')\n",
    "article = content['content'].tolist() #共有89611篇,新华社78661，非新华社10950篇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith open(\\'.\\\\cleaned.txt\\',\\'w\\',encoding=\"utf-8\") as f:\\n    for a in article_cleaned:\\n        f.write(a+\\'\\n\\')\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 清洁数据\n",
    "import re\n",
    "def token(string):\n",
    "    return re.findall('\\w+',string) \n",
    "def article_clean(article_origin):\n",
    "    article_cleaned=[''.join(token(str(a))) for a in article_origin]\n",
    "    return article_cleaned\n",
    "article_cleaned = article_clean(article)\n",
    "'''\n",
    "with open('.\\data\\cleaned.txt','w',encoding=\"utf-8\") as f:\n",
    "    for a in article_cleaned:\n",
    "        f.write(a+'\\n')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\wy\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.961 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n"
     ]
    }
   ],
   "source": [
    "# 分词\n",
    "import jieba\n",
    "def cut(string):\n",
    "    cut_string = [' '.join(jieba.cut(string))]\n",
    "    return cut_string\n",
    "def cut_text(origin_file,cuted_file):\n",
    "    TOKEN = []\n",
    "    for i,line in enumerate((open(origin_file,encoding=\"utf-8\"))):\n",
    "        if i%10000==0:\n",
    "            print(i)\n",
    "        if line != \"\":\n",
    "            TOKEN += cut(line)\n",
    "    return TOKEN\n",
    "TOKEN = cut_text('.\\data\\cleaned.txt','.\\data\\cuted.txt')\n",
    "\n",
    "with open('.\\data\\cuted.txt','w',encoding=\"utf-8\") as f:\n",
    "    for a in TOKEN:\n",
    "        f.write(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-29 10:28:57,403 : INFO : collecting all words and their counts\n",
      "2020-03-29 10:28:57,405 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-29 10:28:58,753 : INFO : PROGRESS: at sentence #10000, processed 3684164 words, keeping 148557 word types\n",
      "2020-03-29 10:28:59,379 : INFO : PROGRESS: at sentence #20000, processed 5353913 words, keeping 174173 word types\n",
      "2020-03-29 10:28:59,961 : INFO : PROGRESS: at sentence #30000, processed 6948716 words, keeping 189746 word types\n",
      "2020-03-29 10:29:00,552 : INFO : PROGRESS: at sentence #40000, processed 8626790 words, keeping 206181 word types\n",
      "2020-03-29 10:29:01,191 : INFO : PROGRESS: at sentence #50000, processed 10484049 words, keeping 222534 word types\n",
      "2020-03-29 10:29:01,856 : INFO : PROGRESS: at sentence #60000, processed 12378660 words, keeping 237871 word types\n",
      "2020-03-29 10:29:02,440 : INFO : PROGRESS: at sentence #70000, processed 14084045 words, keeping 250282 word types\n",
      "2020-03-29 10:29:03,033 : INFO : PROGRESS: at sentence #80000, processed 15789941 words, keeping 262059 word types\n",
      "2020-03-29 10:29:03,646 : INFO : collected 273632 word types from a corpus of 17528643 raw words and 89609 sentences\n",
      "2020-03-29 10:29:03,646 : INFO : Loading a fresh vocabulary\n",
      "2020-03-29 10:29:04,049 : INFO : effective_min_count=5 retains 99789 unique words (36% of original 273632, drops 173843)\n",
      "2020-03-29 10:29:04,050 : INFO : effective_min_count=5 leaves 17232469 word corpus (98% of original 17528643, drops 296174)\n",
      "2020-03-29 10:29:04,405 : INFO : deleting the raw counts dictionary of 273632 items\n",
      "2020-03-29 10:29:04,413 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2020-03-29 10:29:04,413 : INFO : downsampling leaves estimated 15425401 word corpus (89.5% of prior 17232469)\n",
      "2020-03-29 10:29:04,799 : INFO : estimated required memory for 99789 words and 100 dimensions: 129725700 bytes\n",
      "2020-03-29 10:29:04,801 : INFO : resetting layer weights\n",
      "2020-03-29 10:29:25,086 : INFO : training model with 3 workers on 99789 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-29 10:29:26,115 : INFO : EPOCH 1 - PROGRESS: at 0.65% examples, 222802 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:29:27,116 : INFO : EPOCH 1 - PROGRESS: at 1.30% examples, 227378 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:29:28,161 : INFO : EPOCH 1 - PROGRESS: at 2.06% examples, 227778 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:29:29,205 : INFO : EPOCH 1 - PROGRESS: at 2.78% examples, 230876 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:29:30,210 : INFO : EPOCH 1 - PROGRESS: at 3.40% examples, 231752 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:29:31,213 : INFO : EPOCH 1 - PROGRESS: at 4.03% examples, 231809 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:29:32,220 : INFO : EPOCH 1 - PROGRESS: at 4.69% examples, 231541 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:29:33,252 : INFO : EPOCH 1 - PROGRESS: at 5.47% examples, 232140 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:29:34,284 : INFO : EPOCH 1 - PROGRESS: at 6.19% examples, 231725 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:29:35,313 : INFO : EPOCH 1 - PROGRESS: at 6.82% examples, 232038 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:29:36,330 : INFO : EPOCH 1 - PROGRESS: at 7.41% examples, 232230 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:29:37,371 : INFO : EPOCH 1 - PROGRESS: at 8.10% examples, 232413 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:29:38,412 : INFO : EPOCH 1 - PROGRESS: at 8.70% examples, 232800 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:29:39,448 : INFO : EPOCH 1 - PROGRESS: at 11.80% examples, 232595 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:29:40,497 : INFO : EPOCH 1 - PROGRESS: at 13.12% examples, 232379 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:29:41,506 : INFO : EPOCH 1 - PROGRESS: at 15.13% examples, 232083 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:29:42,539 : INFO : EPOCH 1 - PROGRESS: at 16.78% examples, 231585 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:29:43,573 : INFO : EPOCH 1 - PROGRESS: at 18.73% examples, 231583 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:29:44,587 : INFO : EPOCH 1 - PROGRESS: at 20.51% examples, 231675 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:29:45,590 : INFO : EPOCH 1 - PROGRESS: at 22.14% examples, 231003 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:29:46,602 : INFO : EPOCH 1 - PROGRESS: at 23.90% examples, 230525 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:29:47,651 : INFO : EPOCH 1 - PROGRESS: at 25.86% examples, 230054 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:29:48,668 : INFO : EPOCH 1 - PROGRESS: at 27.60% examples, 229897 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:29:49,683 : INFO : EPOCH 1 - PROGRESS: at 29.55% examples, 229720 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:29:50,703 : INFO : EPOCH 1 - PROGRESS: at 31.16% examples, 229610 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:29:51,706 : INFO : EPOCH 1 - PROGRESS: at 32.89% examples, 229306 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:29:52,723 : INFO : EPOCH 1 - PROGRESS: at 34.98% examples, 229201 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:29:53,725 : INFO : EPOCH 1 - PROGRESS: at 36.47% examples, 228623 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:29:54,733 : INFO : EPOCH 1 - PROGRESS: at 38.30% examples, 228624 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:29:55,752 : INFO : EPOCH 1 - PROGRESS: at 39.84% examples, 228541 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:29:56,753 : INFO : EPOCH 1 - PROGRESS: at 41.64% examples, 228329 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:29:57,765 : INFO : EPOCH 1 - PROGRESS: at 43.46% examples, 228299 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:29:58,780 : INFO : EPOCH 1 - PROGRESS: at 45.08% examples, 228239 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:29:59,794 : INFO : EPOCH 1 - PROGRESS: at 46.57% examples, 227983 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:00,797 : INFO : EPOCH 1 - PROGRESS: at 48.12% examples, 228007 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:01,797 : INFO : EPOCH 1 - PROGRESS: at 49.56% examples, 227615 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:02,822 : INFO : EPOCH 1 - PROGRESS: at 51.26% examples, 227498 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:03,835 : INFO : EPOCH 1 - PROGRESS: at 52.87% examples, 227238 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:30:04,850 : INFO : EPOCH 1 - PROGRESS: at 54.43% examples, 227211 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:05,863 : INFO : EPOCH 1 - PROGRESS: at 55.79% examples, 226995 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:30:06,884 : INFO : EPOCH 1 - PROGRESS: at 57.22% examples, 226919 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:07,928 : INFO : EPOCH 1 - PROGRESS: at 58.83% examples, 226961 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:30:08,968 : INFO : EPOCH 1 - PROGRESS: at 60.68% examples, 226814 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:30:09,983 : INFO : EPOCH 1 - PROGRESS: at 62.21% examples, 226778 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:11,047 : INFO : EPOCH 1 - PROGRESS: at 63.72% examples, 226552 words/s, in_qsize 5, out_qsize 1\n",
      "2020-03-29 10:30:12,069 : INFO : EPOCH 1 - PROGRESS: at 65.22% examples, 226524 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:30:13,094 : INFO : EPOCH 1 - PROGRESS: at 66.73% examples, 226455 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:14,170 : INFO : EPOCH 1 - PROGRESS: at 68.62% examples, 226205 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:15,175 : INFO : EPOCH 1 - PROGRESS: at 70.45% examples, 226276 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:16,235 : INFO : EPOCH 1 - PROGRESS: at 72.06% examples, 225954 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:30:17,304 : INFO : EPOCH 1 - PROGRESS: at 73.49% examples, 225437 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:18,382 : INFO : EPOCH 1 - PROGRESS: at 74.91% examples, 224745 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:19,416 : INFO : EPOCH 1 - PROGRESS: at 76.20% examples, 223785 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:20,424 : INFO : EPOCH 1 - PROGRESS: at 77.63% examples, 222956 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:21,481 : INFO : EPOCH 1 - PROGRESS: at 78.99% examples, 221960 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-29 10:30:22,564 : INFO : EPOCH 1 - PROGRESS: at 80.36% examples, 221482 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:23,592 : INFO : EPOCH 1 - PROGRESS: at 81.92% examples, 221104 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:24,647 : INFO : EPOCH 1 - PROGRESS: at 83.45% examples, 221068 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:25,683 : INFO : EPOCH 1 - PROGRESS: at 85.23% examples, 221091 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:26,702 : INFO : EPOCH 1 - PROGRESS: at 86.94% examples, 221169 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:30:27,739 : INFO : EPOCH 1 - PROGRESS: at 88.90% examples, 221168 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:28,776 : INFO : EPOCH 1 - PROGRESS: at 90.70% examples, 221044 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:30:29,801 : INFO : EPOCH 1 - PROGRESS: at 92.07% examples, 220563 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:30:30,826 : INFO : EPOCH 1 - PROGRESS: at 93.18% examples, 220090 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:30:31,843 : INFO : EPOCH 1 - PROGRESS: at 94.57% examples, 219541 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:32,903 : INFO : EPOCH 1 - PROGRESS: at 96.12% examples, 218771 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:30:33,934 : INFO : EPOCH 1 - PROGRESS: at 97.63% examples, 218496 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:34,951 : INFO : EPOCH 1 - PROGRESS: at 98.99% examples, 218500 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:35,629 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-29 10:30:35,637 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-29 10:30:35,686 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-29 10:30:35,687 : INFO : EPOCH - 1 : training on 17528643 raw words (15424935 effective words) took 70.6s, 218494 effective words/s\n",
      "2020-03-29 10:30:36,748 : INFO : EPOCH 2 - PROGRESS: at 0.59% examples, 199191 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:30:37,764 : INFO : EPOCH 2 - PROGRESS: at 1.17% examples, 197323 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:30:38,864 : INFO : EPOCH 2 - PROGRESS: at 1.65% examples, 179988 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:39,898 : INFO : EPOCH 2 - PROGRESS: at 2.35% examples, 190568 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:40,987 : INFO : EPOCH 2 - PROGRESS: at 3.02% examples, 195242 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:41,989 : INFO : EPOCH 2 - PROGRESS: at 3.56% examples, 199345 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:43,015 : INFO : EPOCH 2 - PROGRESS: at 4.19% examples, 201750 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:30:44,024 : INFO : EPOCH 2 - PROGRESS: at 4.85% examples, 204415 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:45,076 : INFO : EPOCH 2 - PROGRESS: at 5.55% examples, 204413 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:30:46,101 : INFO : EPOCH 2 - PROGRESS: at 6.24% examples, 206359 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:47,149 : INFO : EPOCH 2 - PROGRESS: at 6.84% examples, 207725 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:48,182 : INFO : EPOCH 2 - PROGRESS: at 7.37% examples, 208251 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:30:49,192 : INFO : EPOCH 2 - PROGRESS: at 8.02% examples, 208944 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:50,231 : INFO : EPOCH 2 - PROGRESS: at 8.58% examples, 209751 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:51,282 : INFO : EPOCH 2 - PROGRESS: at 11.11% examples, 210402 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:52,323 : INFO : EPOCH 2 - PROGRESS: at 12.75% examples, 210665 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:30:53,374 : INFO : EPOCH 2 - PROGRESS: at 14.32% examples, 211124 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:54,382 : INFO : EPOCH 2 - PROGRESS: at 16.19% examples, 211638 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:30:55,428 : INFO : EPOCH 2 - PROGRESS: at 17.73% examples, 212135 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:56,450 : INFO : EPOCH 2 - PROGRESS: at 19.79% examples, 212691 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:30:57,484 : INFO : EPOCH 2 - PROGRESS: at 21.31% examples, 213047 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:30:58,494 : INFO : EPOCH 2 - PROGRESS: at 23.14% examples, 213363 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:30:59,503 : INFO : EPOCH 2 - PROGRESS: at 24.96% examples, 213671 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:00,517 : INFO : EPOCH 2 - PROGRESS: at 26.99% examples, 213924 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:31:01,554 : INFO : EPOCH 2 - PROGRESS: at 28.52% examples, 213864 words/s, in_qsize 5, out_qsize 1\n",
      "2020-03-29 10:31:02,575 : INFO : EPOCH 2 - PROGRESS: at 30.13% examples, 214275 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:31:03,643 : INFO : EPOCH 2 - PROGRESS: at 31.84% examples, 214405 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:04,687 : INFO : EPOCH 2 - PROGRESS: at 33.96% examples, 214658 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:05,744 : INFO : EPOCH 2 - PROGRESS: at 35.94% examples, 214742 words/s, in_qsize 4, out_qsize 1\n",
      "2020-03-29 10:31:06,764 : INFO : EPOCH 2 - PROGRESS: at 37.58% examples, 215067 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:31:07,836 : INFO : EPOCH 2 - PROGRESS: at 39.21% examples, 215094 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:31:08,891 : INFO : EPOCH 2 - PROGRESS: at 40.88% examples, 215220 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:09,918 : INFO : EPOCH 2 - PROGRESS: at 42.75% examples, 215249 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:10,927 : INFO : EPOCH 2 - PROGRESS: at 44.37% examples, 215347 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:31:11,960 : INFO : EPOCH 2 - PROGRESS: at 45.81% examples, 215363 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:12,983 : INFO : EPOCH 2 - PROGRESS: at 47.29% examples, 215596 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:14,005 : INFO : EPOCH 2 - PROGRESS: at 48.76% examples, 215670 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:15,068 : INFO : EPOCH 2 - PROGRESS: at 50.60% examples, 215631 words/s, in_qsize 4, out_qsize 1\n",
      "2020-03-29 10:31:16,087 : INFO : EPOCH 2 - PROGRESS: at 52.25% examples, 215860 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:17,140 : INFO : EPOCH 2 - PROGRESS: at 53.95% examples, 215892 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:18,198 : INFO : EPOCH 2 - PROGRESS: at 55.32% examples, 215961 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:19,241 : INFO : EPOCH 2 - PROGRESS: at 56.71% examples, 216019 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:20,268 : INFO : EPOCH 2 - PROGRESS: at 58.13% examples, 216187 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:21,299 : INFO : EPOCH 2 - PROGRESS: at 59.89% examples, 216344 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:22,341 : INFO : EPOCH 2 - PROGRESS: at 61.46% examples, 216423 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:23,362 : INFO : EPOCH 2 - PROGRESS: at 63.12% examples, 216441 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:24,391 : INFO : EPOCH 2 - PROGRESS: at 64.64% examples, 216420 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:25,437 : INFO : EPOCH 2 - PROGRESS: at 66.04% examples, 216511 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:26,482 : INFO : EPOCH 2 - PROGRESS: at 67.72% examples, 216566 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:31:27,502 : INFO : EPOCH 2 - PROGRESS: at 69.74% examples, 216749 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:28,519 : INFO : EPOCH 2 - PROGRESS: at 71.42% examples, 216953 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:31:29,574 : INFO : EPOCH 2 - PROGRESS: at 72.97% examples, 217014 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:30,634 : INFO : EPOCH 2 - PROGRESS: at 74.61% examples, 217064 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:31,701 : INFO : EPOCH 2 - PROGRESS: at 76.20% examples, 217078 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:32,744 : INFO : EPOCH 2 - PROGRESS: at 77.99% examples, 217174 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:33,753 : INFO : EPOCH 2 - PROGRESS: at 79.41% examples, 217207 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-29 10:31:34,778 : INFO : EPOCH 2 - PROGRESS: at 81.01% examples, 217201 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:35,804 : INFO : EPOCH 2 - PROGRESS: at 82.79% examples, 217339 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:31:36,859 : INFO : EPOCH 2 - PROGRESS: at 84.42% examples, 217362 words/s, in_qsize 4, out_qsize 1\n",
      "2020-03-29 10:31:37,900 : INFO : EPOCH 2 - PROGRESS: at 86.17% examples, 217423 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:31:38,930 : INFO : EPOCH 2 - PROGRESS: at 87.97% examples, 217504 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:39,957 : INFO : EPOCH 2 - PROGRESS: at 89.92% examples, 217603 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:31:40,958 : INFO : EPOCH 2 - PROGRESS: at 91.42% examples, 217651 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:41,996 : INFO : EPOCH 2 - PROGRESS: at 92.89% examples, 217570 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:43,065 : INFO : EPOCH 2 - PROGRESS: at 94.57% examples, 217531 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:44,084 : INFO : EPOCH 2 - PROGRESS: at 96.37% examples, 217693 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:45,136 : INFO : EPOCH 2 - PROGRESS: at 98.04% examples, 217738 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:46,187 : INFO : EPOCH 2 - PROGRESS: at 99.50% examples, 217780 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:46,438 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-29 10:31:46,462 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-29 10:31:46,481 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-29 10:31:46,482 : INFO : EPOCH - 2 : training on 17528643 raw words (15426076 effective words) took 70.8s, 217907 effective words/s\n",
      "2020-03-29 10:31:47,508 : INFO : EPOCH 3 - PROGRESS: at 0.59% examples, 206321 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:48,513 : INFO : EPOCH 3 - PROGRESS: at 1.23% examples, 214466 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:49,535 : INFO : EPOCH 3 - PROGRESS: at 1.90% examples, 212824 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:50,595 : INFO : EPOCH 3 - PROGRESS: at 2.61% examples, 214318 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:51,640 : INFO : EPOCH 3 - PROGRESS: at 3.22% examples, 215461 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:31:52,648 : INFO : EPOCH 3 - PROGRESS: at 3.78% examples, 215126 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:53,672 : INFO : EPOCH 3 - PROGRESS: at 4.36% examples, 214223 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:31:54,714 : INFO : EPOCH 3 - PROGRESS: at 5.04% examples, 214625 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:31:55,777 : INFO : EPOCH 3 - PROGRESS: at 5.79% examples, 214713 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:31:56,807 : INFO : EPOCH 3 - PROGRESS: at 6.46% examples, 215768 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:57,816 : INFO : EPOCH 3 - PROGRESS: at 6.97% examples, 215279 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:31:58,854 : INFO : EPOCH 3 - PROGRESS: at 7.59% examples, 215885 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:31:59,857 : INFO : EPOCH 3 - PROGRESS: at 8.19% examples, 216122 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:32:00,884 : INFO : EPOCH 3 - PROGRESS: at 8.71% examples, 215928 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:01,907 : INFO : EPOCH 3 - PROGRESS: at 11.18% examples, 214885 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:02,927 : INFO : EPOCH 3 - PROGRESS: at 12.91% examples, 215184 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:32:03,970 : INFO : EPOCH 3 - PROGRESS: at 14.68% examples, 215010 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:32:04,976 : INFO : EPOCH 3 - PROGRESS: at 16.36% examples, 215331 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:05,992 : INFO : EPOCH 3 - PROGRESS: at 17.80% examples, 215078 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:07,041 : INFO : EPOCH 3 - PROGRESS: at 19.84% examples, 215213 words/s, in_qsize 4, out_qsize 1\n",
      "2020-03-29 10:32:08,071 : INFO : EPOCH 3 - PROGRESS: at 21.34% examples, 215497 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:09,132 : INFO : EPOCH 3 - PROGRESS: at 23.28% examples, 215591 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:10,155 : INFO : EPOCH 3 - PROGRESS: at 25.28% examples, 216052 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:11,191 : INFO : EPOCH 3 - PROGRESS: at 27.12% examples, 216296 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:12,240 : INFO : EPOCH 3 - PROGRESS: at 28.79% examples, 216445 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:32:13,283 : INFO : EPOCH 3 - PROGRESS: at 30.37% examples, 216604 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:14,325 : INFO : EPOCH 3 - PROGRESS: at 32.25% examples, 216810 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:15,324 : INFO : EPOCH 3 - PROGRESS: at 34.23% examples, 216686 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:32:16,370 : INFO : EPOCH 3 - PROGRESS: at 36.12% examples, 216822 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:32:17,405 : INFO : EPOCH 3 - PROGRESS: at 37.73% examples, 216989 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:32:18,466 : INFO : EPOCH 3 - PROGRESS: at 39.34% examples, 216999 words/s, in_qsize 4, out_qsize 1\n",
      "2020-03-29 10:32:19,492 : INFO : EPOCH 3 - PROGRESS: at 41.04% examples, 217265 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:20,492 : INFO : EPOCH 3 - PROGRESS: at 42.92% examples, 217388 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:32:21,507 : INFO : EPOCH 3 - PROGRESS: at 44.48% examples, 217160 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:22,513 : INFO : EPOCH 3 - PROGRESS: at 45.89% examples, 217305 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:23,533 : INFO : EPOCH 3 - PROGRESS: at 47.36% examples, 217259 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:32:24,619 : INFO : EPOCH 3 - PROGRESS: at 48.96% examples, 217151 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:25,682 : INFO : EPOCH 3 - PROGRESS: at 50.71% examples, 217083 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:32:26,708 : INFO : EPOCH 3 - PROGRESS: at 52.35% examples, 217232 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:32:27,741 : INFO : EPOCH 3 - PROGRESS: at 54.02% examples, 217320 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:28,762 : INFO : EPOCH 3 - PROGRESS: at 55.40% examples, 217345 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:29,794 : INFO : EPOCH 3 - PROGRESS: at 56.75% examples, 217436 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:30,811 : INFO : EPOCH 3 - PROGRESS: at 58.13% examples, 217424 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:31,821 : INFO : EPOCH 3 - PROGRESS: at 59.83% examples, 217474 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:32,842 : INFO : EPOCH 3 - PROGRESS: at 61.40% examples, 217444 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:32:33,911 : INFO : EPOCH 3 - PROGRESS: at 63.05% examples, 217390 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:32:34,921 : INFO : EPOCH 3 - PROGRESS: at 64.57% examples, 217435 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:32:35,937 : INFO : EPOCH 3 - PROGRESS: at 65.90% examples, 217274 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:36,977 : INFO : EPOCH 3 - PROGRESS: at 67.56% examples, 217341 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:38,015 : INFO : EPOCH 3 - PROGRESS: at 69.30% examples, 217448 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:39,034 : INFO : EPOCH 3 - PROGRESS: at 71.29% examples, 217607 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:40,058 : INFO : EPOCH 3 - PROGRESS: at 72.77% examples, 217649 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:32:41,060 : INFO : EPOCH 3 - PROGRESS: at 74.30% examples, 217570 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:42,079 : INFO : EPOCH 3 - PROGRESS: at 75.79% examples, 217609 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:43,083 : INFO : EPOCH 3 - PROGRESS: at 77.33% examples, 217546 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:44,135 : INFO : EPOCH 3 - PROGRESS: at 79.07% examples, 217565 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:45,137 : INFO : EPOCH 3 - PROGRESS: at 80.58% examples, 217646 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-29 10:32:46,208 : INFO : EPOCH 3 - PROGRESS: at 82.28% examples, 217616 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:47,292 : INFO : EPOCH 3 - PROGRESS: at 83.90% examples, 217540 words/s, in_qsize 4, out_qsize 1\n",
      "2020-03-29 10:32:48,336 : INFO : EPOCH 3 - PROGRESS: at 85.62% examples, 217574 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:32:49,386 : INFO : EPOCH 3 - PROGRESS: at 87.45% examples, 217592 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:50,407 : INFO : EPOCH 3 - PROGRESS: at 89.44% examples, 217717 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:32:51,457 : INFO : EPOCH 3 - PROGRESS: at 91.04% examples, 217736 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:52,488 : INFO : EPOCH 3 - PROGRESS: at 92.53% examples, 217671 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:53,505 : INFO : EPOCH 3 - PROGRESS: at 94.13% examples, 217801 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:32:54,512 : INFO : EPOCH 3 - PROGRESS: at 95.93% examples, 217855 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:55,549 : INFO : EPOCH 3 - PROGRESS: at 97.61% examples, 217689 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:32:56,592 : INFO : EPOCH 3 - PROGRESS: at 98.99% examples, 217751 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:32:57,235 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-29 10:32:57,277 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-29 10:32:57,297 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-29 10:32:57,298 : INFO : EPOCH - 3 : training on 17528643 raw words (15425703 effective words) took 70.8s, 217846 effective words/s\n",
      "2020-03-29 10:32:58,337 : INFO : EPOCH 4 - PROGRESS: at 0.59% examples, 203254 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:32:59,347 : INFO : EPOCH 4 - PROGRESS: at 1.23% examples, 212317 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:33:00,372 : INFO : EPOCH 4 - PROGRESS: at 1.90% examples, 211160 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:33:01,376 : INFO : EPOCH 4 - PROGRESS: at 2.59% examples, 214065 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:33:02,427 : INFO : EPOCH 4 - PROGRESS: at 3.16% examples, 211678 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:33:03,491 : INFO : EPOCH 4 - PROGRESS: at 3.75% examples, 212572 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:33:04,493 : INFO : EPOCH 4 - PROGRESS: at 4.33% examples, 212718 words/s, in_qsize 5, out_qsize 1\n",
      "2020-03-29 10:33:05,515 : INFO : EPOCH 4 - PROGRESS: at 4.92% examples, 210559 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:06,517 : INFO : EPOCH 4 - PROGRESS: at 5.59% examples, 210001 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:33:07,585 : INFO : EPOCH 4 - PROGRESS: at 6.21% examples, 207990 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:33:08,611 : INFO : EPOCH 4 - PROGRESS: at 6.73% examples, 206715 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:09,682 : INFO : EPOCH 4 - PROGRESS: at 7.25% examples, 206077 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:33:10,728 : INFO : EPOCH 4 - PROGRESS: at 7.87% examples, 205708 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:33:11,760 : INFO : EPOCH 4 - PROGRESS: at 8.33% examples, 204042 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:33:12,775 : INFO : EPOCH 4 - PROGRESS: at 8.80% examples, 203176 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:33:13,830 : INFO : EPOCH 4 - PROGRESS: at 11.22% examples, 201532 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:14,831 : INFO : EPOCH 4 - PROGRESS: at 12.82% examples, 200332 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:33:15,866 : INFO : EPOCH 4 - PROGRESS: at 14.23% examples, 200622 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:16,884 : INFO : EPOCH 4 - PROGRESS: at 16.12% examples, 201543 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:17,922 : INFO : EPOCH 4 - PROGRESS: at 17.55% examples, 201748 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:33:18,975 : INFO : EPOCH 4 - PROGRESS: at 19.58% examples, 202534 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:19,994 : INFO : EPOCH 4 - PROGRESS: at 21.15% examples, 203462 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:33:21,024 : INFO : EPOCH 4 - PROGRESS: at 22.80% examples, 203618 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:22,026 : INFO : EPOCH 4 - PROGRESS: at 24.67% examples, 204348 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:23,028 : INFO : EPOCH 4 - PROGRESS: at 26.65% examples, 205054 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:24,042 : INFO : EPOCH 4 - PROGRESS: at 28.29% examples, 205230 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:25,077 : INFO : EPOCH 4 - PROGRESS: at 29.82% examples, 205577 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:26,115 : INFO : EPOCH 4 - PROGRESS: at 31.59% examples, 206178 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:27,179 : INFO : EPOCH 4 - PROGRESS: at 33.59% examples, 206528 words/s, in_qsize 5, out_qsize 1\n",
      "2020-03-29 10:33:28,241 : INFO : EPOCH 4 - PROGRESS: at 35.44% examples, 206909 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:29,311 : INFO : EPOCH 4 - PROGRESS: at 37.20% examples, 207136 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:33:30,350 : INFO : EPOCH 4 - PROGRESS: at 38.88% examples, 207614 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:31,358 : INFO : EPOCH 4 - PROGRESS: at 40.34% examples, 208023 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:32,364 : INFO : EPOCH 4 - PROGRESS: at 42.13% examples, 208136 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:33,399 : INFO : EPOCH 4 - PROGRESS: at 44.02% examples, 208277 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:34,449 : INFO : EPOCH 4 - PROGRESS: at 45.44% examples, 208592 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:35,466 : INFO : EPOCH 4 - PROGRESS: at 46.84% examples, 208645 words/s, in_qsize 5, out_qsize 1\n",
      "2020-03-29 10:33:36,544 : INFO : EPOCH 4 - PROGRESS: at 48.36% examples, 208753 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:37,563 : INFO : EPOCH 4 - PROGRESS: at 49.99% examples, 209218 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:38,575 : INFO : EPOCH 4 - PROGRESS: at 51.61% examples, 209209 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:39,633 : INFO : EPOCH 4 - PROGRESS: at 53.28% examples, 209386 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:40,637 : INFO : EPOCH 4 - PROGRESS: at 54.69% examples, 209439 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:41,640 : INFO : EPOCH 4 - PROGRESS: at 56.10% examples, 209681 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:33:42,661 : INFO : EPOCH 4 - PROGRESS: at 57.57% examples, 209803 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:33:43,716 : INFO : EPOCH 4 - PROGRESS: at 58.92% examples, 209824 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:44,771 : INFO : EPOCH 4 - PROGRESS: at 60.80% examples, 209989 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:33:45,815 : INFO : EPOCH 4 - PROGRESS: at 62.28% examples, 210192 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:33:46,858 : INFO : EPOCH 4 - PROGRESS: at 63.83% examples, 210421 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:47,901 : INFO : EPOCH 4 - PROGRESS: at 65.29% examples, 210470 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:48,927 : INFO : EPOCH 4 - PROGRESS: at 66.73% examples, 210556 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:50,003 : INFO : EPOCH 4 - PROGRESS: at 68.62% examples, 210641 words/s, in_qsize 5, out_qsize 1\n",
      "2020-03-29 10:33:51,056 : INFO : EPOCH 4 - PROGRESS: at 70.45% examples, 210812 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:33:52,072 : INFO : EPOCH 4 - PROGRESS: at 72.06% examples, 210981 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:53,147 : INFO : EPOCH 4 - PROGRESS: at 73.49% examples, 210762 words/s, in_qsize 3, out_qsize 2\n",
      "2020-03-29 10:33:54,153 : INFO : EPOCH 4 - PROGRESS: at 74.88% examples, 210503 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:33:55,174 : INFO : EPOCH 4 - PROGRESS: at 76.34% examples, 210498 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:56,240 : INFO : EPOCH 4 - PROGRESS: at 78.16% examples, 210628 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:33:57,284 : INFO : EPOCH 4 - PROGRESS: at 79.64% examples, 210774 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-29 10:33:58,292 : INFO : EPOCH 4 - PROGRESS: at 81.36% examples, 210951 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:33:59,326 : INFO : EPOCH 4 - PROGRESS: at 83.01% examples, 211022 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:00,336 : INFO : EPOCH 4 - PROGRESS: at 84.63% examples, 211168 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:01,354 : INFO : EPOCH 4 - PROGRESS: at 86.21% examples, 211272 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:02,355 : INFO : EPOCH 4 - PROGRESS: at 87.97% examples, 211410 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:34:03,360 : INFO : EPOCH 4 - PROGRESS: at 89.89% examples, 211542 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:04,397 : INFO : EPOCH 4 - PROGRESS: at 91.32% examples, 211564 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:34:05,449 : INFO : EPOCH 4 - PROGRESS: at 92.89% examples, 211659 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:06,495 : INFO : EPOCH 4 - PROGRESS: at 94.57% examples, 211789 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:07,522 : INFO : EPOCH 4 - PROGRESS: at 96.32% examples, 211873 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:34:08,567 : INFO : EPOCH 4 - PROGRESS: at 97.98% examples, 212021 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:09,585 : INFO : EPOCH 4 - PROGRESS: at 99.40% examples, 212129 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:09,933 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-29 10:34:09,959 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-29 10:34:09,962 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-29 10:34:09,964 : INFO : EPOCH - 4 : training on 17528643 raw words (15423928 effective words) took 72.7s, 212268 effective words/s\n",
      "2020-03-29 10:34:11,003 : INFO : EPOCH 5 - PROGRESS: at 0.58% examples, 195450 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:12,042 : INFO : EPOCH 5 - PROGRESS: at 1.19% examples, 201207 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:34:13,063 : INFO : EPOCH 5 - PROGRESS: at 1.88% examples, 207281 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:14,125 : INFO : EPOCH 5 - PROGRESS: at 2.54% examples, 205799 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:15,175 : INFO : EPOCH 5 - PROGRESS: at 3.16% examples, 208413 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:34:16,237 : INFO : EPOCH 5 - PROGRESS: at 3.75% examples, 210021 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:17,334 : INFO : EPOCH 5 - PROGRESS: at 4.38% examples, 210067 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:34:18,358 : INFO : EPOCH 5 - PROGRESS: at 5.06% examples, 211496 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:19,387 : INFO : EPOCH 5 - PROGRESS: at 5.79% examples, 211772 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:20,407 : INFO : EPOCH 5 - PROGRESS: at 6.44% examples, 212488 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:21,422 : INFO : EPOCH 5 - PROGRESS: at 6.94% examples, 212210 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:34:22,448 : INFO : EPOCH 5 - PROGRESS: at 7.53% examples, 212507 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:23,456 : INFO : EPOCH 5 - PROGRESS: at 8.15% examples, 212956 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:34:24,489 : INFO : EPOCH 5 - PROGRESS: at 8.65% examples, 212398 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:25,492 : INFO : EPOCH 5 - PROGRESS: at 11.16% examples, 212900 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:34:26,525 : INFO : EPOCH 5 - PROGRESS: at 12.89% examples, 213134 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:34:27,534 : INFO : EPOCH 5 - PROGRESS: at 14.51% examples, 213520 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:28,559 : INFO : EPOCH 5 - PROGRESS: at 16.27% examples, 213247 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:29,603 : INFO : EPOCH 5 - PROGRESS: at 17.80% examples, 213646 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:30,646 : INFO : EPOCH 5 - PROGRESS: at 19.82% examples, 213953 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:31,664 : INFO : EPOCH 5 - PROGRESS: at 21.31% examples, 213994 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:34:32,701 : INFO : EPOCH 5 - PROGRESS: at 23.14% examples, 214015 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:33,757 : INFO : EPOCH 5 - PROGRESS: at 25.11% examples, 214216 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:34,770 : INFO : EPOCH 5 - PROGRESS: at 27.05% examples, 214410 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:35,772 : INFO : EPOCH 5 - PROGRESS: at 28.57% examples, 214659 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:36,829 : INFO : EPOCH 5 - PROGRESS: at 30.18% examples, 214762 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:34:37,886 : INFO : EPOCH 5 - PROGRESS: at 31.84% examples, 214636 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:34:38,937 : INFO : EPOCH 5 - PROGRESS: at 33.96% examples, 214834 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:34:39,977 : INFO : EPOCH 5 - PROGRESS: at 35.87% examples, 215048 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:41,041 : INFO : EPOCH 5 - PROGRESS: at 37.58% examples, 215054 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:42,133 : INFO : EPOCH 5 - PROGRESS: at 39.21% examples, 214933 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:34:43,167 : INFO : EPOCH 5 - PROGRESS: at 40.88% examples, 215209 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:44,186 : INFO : EPOCH 5 - PROGRESS: at 42.75% examples, 215291 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:34:45,219 : INFO : EPOCH 5 - PROGRESS: at 44.32% examples, 214998 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:34:46,284 : INFO : EPOCH 5 - PROGRESS: at 45.81% examples, 215074 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:34:47,324 : INFO : EPOCH 5 - PROGRESS: at 47.29% examples, 215218 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:34:48,342 : INFO : EPOCH 5 - PROGRESS: at 48.72% examples, 215085 words/s, in_qsize 4, out_qsize 1\n",
      "2020-03-29 10:34:49,360 : INFO : EPOCH 5 - PROGRESS: at 50.52% examples, 215316 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:34:50,369 : INFO : EPOCH 5 - PROGRESS: at 52.10% examples, 215402 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:51,396 : INFO : EPOCH 5 - PROGRESS: at 53.77% examples, 215399 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:52,398 : INFO : EPOCH 5 - PROGRESS: at 55.11% examples, 215301 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:53,414 : INFO : EPOCH 5 - PROGRESS: at 56.43% examples, 215320 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:34:54,437 : INFO : EPOCH 5 - PROGRESS: at 57.76% examples, 214970 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:34:55,498 : INFO : EPOCH 5 - PROGRESS: at 59.37% examples, 215030 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:56,517 : INFO : EPOCH 5 - PROGRESS: at 61.06% examples, 215223 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:57,535 : INFO : EPOCH 5 - PROGRESS: at 62.52% examples, 215115 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:58,592 : INFO : EPOCH 5 - PROGRESS: at 64.02% examples, 214985 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:34:59,664 : INFO : EPOCH 5 - PROGRESS: at 65.48% examples, 214985 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:35:00,701 : INFO : EPOCH 5 - PROGRESS: at 66.94% examples, 215126 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:35:01,723 : INFO : EPOCH 5 - PROGRESS: at 68.80% examples, 215008 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:35:02,735 : INFO : EPOCH 5 - PROGRESS: at 70.52% examples, 215084 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:35:03,749 : INFO : EPOCH 5 - PROGRESS: at 72.15% examples, 215031 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:35:04,807 : INFO : EPOCH 5 - PROGRESS: at 73.80% examples, 215106 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:35:05,813 : INFO : EPOCH 5 - PROGRESS: at 75.38% examples, 215225 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:35:06,864 : INFO : EPOCH 5 - PROGRESS: at 76.83% examples, 215030 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:35:07,886 : INFO : EPOCH 5 - PROGRESS: at 78.60% examples, 215097 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:35:08,893 : INFO : EPOCH 5 - PROGRESS: at 79.86% examples, 215004 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-29 10:35:09,910 : INFO : EPOCH 5 - PROGRESS: at 81.49% examples, 214942 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:35:10,915 : INFO : EPOCH 5 - PROGRESS: at 83.03% examples, 214917 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:35:11,940 : INFO : EPOCH 5 - PROGRESS: at 84.69% examples, 214948 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:35:12,956 : INFO : EPOCH 5 - PROGRESS: at 86.26% examples, 214987 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:35:13,995 : INFO : EPOCH 5 - PROGRESS: at 88.11% examples, 214948 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:35:14,996 : INFO : EPOCH 5 - PROGRESS: at 89.92% examples, 215046 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:35:16,019 : INFO : EPOCH 5 - PROGRESS: at 91.42% examples, 215055 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:35:17,056 : INFO : EPOCH 5 - PROGRESS: at 92.89% examples, 215022 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:35:18,127 : INFO : EPOCH 5 - PROGRESS: at 94.57% examples, 215017 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:35:19,158 : INFO : EPOCH 5 - PROGRESS: at 96.37% examples, 215168 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:35:20,171 : INFO : EPOCH 5 - PROGRESS: at 97.98% examples, 215245 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-29 10:35:21,193 : INFO : EPOCH 5 - PROGRESS: at 99.34% examples, 215174 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-29 10:35:21,556 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-29 10:35:21,588 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-29 10:35:21,620 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-29 10:35:21,621 : INFO : EPOCH - 5 : training on 17528643 raw words (15424573 effective words) took 71.7s, 215269 effective words/s\n",
      "2020-03-29 10:35:21,622 : INFO : training on a 87643215 raw words (77125215 effective words) took 356.5s, 216319 effective words/s\n",
      "2020-03-29 10:35:21,624 : INFO : storing 99789x100 projection weights into ./word2Vec.bin\n"
     ]
    }
   ],
   "source": [
    "# 训练word2vec模型\n",
    "import gensim\n",
    "import logging\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#设置输出日志\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "#直接用gemsim提供的API去读取txt文件，读取文件的API有LineSentence 和 Text8Corpus, PathLineSentences等\n",
    "sentences = word2vec.LineSentence('.\\data\\cuted.txt')\n",
    "#训练模型，词向量长度设置为100，采用skip-gram模型，模型保存为bin格式\n",
    "model = Word2Vec(sentences,size=100,sg=1)\n",
    "model.wv.save_word2vec_format(\".\\data\\word2Vec\"+\".bin\",binary=True) #获取词向量并存为二进制文件形式\n",
    "#从文件中获取词向量\n",
    "wordVec = gensim.models.KeyedVectors.load_word2vec_format(\".\\data\\word2Vec.bin\",binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建数据集\n",
    "import pandas as pd\n",
    "contents = []\n",
    "with open('.\\cuted.txt','r',encoding=\"utf-8\") as f:\n",
    "    a = f.readlines()\n",
    "    contents += a\n",
    "\n",
    "filename = r'.\\data\\sqlResult_1558435.csv' #...:the path of the file\n",
    "content = pd.read_csv(filename,encoding='gb18030')\n",
    "target = content['source'].tolist()\n",
    "target = [1 if i==\"新华社\" else 0 for i in target]\n",
    "\n",
    "data = []\n",
    "data.append(contents)\n",
    "data.append(target)\n",
    "np_data = np.array(data).T\n",
    "save = pd.DataFrame(np_data,columns=['content','target'])\n",
    "#save.to_csv('.\\dataset.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2] Data Processing** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import gensim\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195.60816194440415\n"
     ]
    }
   ],
   "source": [
    "#sequenceLength取平均值\n",
    "content = pd.read_csv('.\\data\\dataset.csv',encoding='utf-8')\n",
    "news = [line.strip().split() for line in content['content']]\n",
    "sum=0\n",
    "for i in range(len(news)):\n",
    "    sum += len(news[i])\n",
    "meanLength = sum/len(news)  #meanLength=195,因此我们取sequenceLength=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    sequenceLength = 200\n",
    "    dataSoource = '.\\data\\dataset.csv'\n",
    "    word2cevSource = '.\\data\\word2Vec.bin'\n",
    "    rate = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self._dataSource = config.dataSource\n",
    "        self._word2vecSource = config.word2vecSource\n",
    "        self._rate = config.rate\n",
    "        self._sequenceLength = config.sequenceLength\n",
    "        self._embeddingSize = 100  # 词向量的大小\n",
    "\n",
    "        self.trainNews = []\n",
    "        self.trainLabels = []\n",
    "        self.testNews = []\n",
    "        self.testLabels = []\n",
    "        self.wordEmbedding = None\n",
    "        self.labelList = []\n",
    "\n",
    "    def _readData(self, dataPath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        content = pd.read_csv(dataPath, encoding='utf-8')\n",
    "        news = [line.strip().split() for line in content['content']]\n",
    "        labels = content['target']\n",
    "        return news, labels\n",
    "\n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        根据所给语料，取出预训练好的word2vec中存在的词向量\n",
    "        \"\"\"\n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(self._word2vecSource, binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        # 添加\"PAD\"和\"UNK\",\"PAD\"用于补充序列长度，\"UNK\"用于代替词典中没有的词\n",
    "        vocab.append(\"PAD\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "\n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "        return vocab, np.array(wordEmbedding)\n",
    "\n",
    "    def _genVocabulary(self, news):\n",
    "        \"\"\"\n",
    "        生成词向量-索引和词汇-索引映射字典\n",
    "        若既有有标签数据集和无标签数据集，可以用全数据集(当同时有有标签数据集和无标签数据集时)\n",
    "        复杂情况下，可在该步添加去除停用词或低频词等操作，本次不添加这些操作\n",
    "        \"\"\"\n",
    "        words = [word for new in news for word in new]\n",
    "        wordCount = Counter(words)\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 1]  # 该步可用于去除低频次\n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "\n",
    "        word2idx = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        label2idx = dict(zip([\"非新华社\",\"新华社\"],list(range(2))))\n",
    "        self.labelList = list(range(2))\n",
    "\n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open('.\\data\\word2idx.json', 'w', encoding=\"utf-8\") as f:\n",
    "            json.dump(word2idx, f)\n",
    "        with open('.\\data\\label2idx.json', 'w', encoding=\"utf-8\") as f:\n",
    "            json.dump(label2idx, f)\n",
    "        return word2idx, label2idx\n",
    "\n",
    "    def _wordToIndex(self, news, word2idx):\n",
    "        \"\"\"\n",
    "        将词转换成索引\n",
    "        \"\"\"\n",
    "        newsIds = [[word2idx.get(item, word2idx[\"UNK\"]) for item in new] for new in news]\n",
    "        return newsIds\n",
    "\n",
    "    def _genTrainEvalData(self, x, y, word2idx):\n",
    "        \"\"\"\n",
    "        生成训练集和测试集\n",
    "        x - 经过转换成wordIndex的数据集\n",
    "        y - 标签\n",
    "        \"\"\"\n",
    "        # 设定固定序列长度，多截少补\n",
    "        news = []\n",
    "        for new in x:\n",
    "            if len(new) >= self._sequenceLength:\n",
    "                news.append(new[:self._sequenceLength])\n",
    "            else:\n",
    "                news.append(new + [word2idx[\"PAD\"]] * (self._sequenceLength - len(new)))\n",
    "        labels = y\n",
    "        x_train,x_test,y_train,y_test = train_test_split(news,labels,test_size=1-self._rate,random_state=1)\n",
    "        #assert len(news) == len(x_train) + len(x_test)\n",
    "        #assert len(labels) == len(x_test) + len(y_test)\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和测试集\n",
    "        \"\"\"\n",
    "        # 初始化数据集\n",
    "        news, labels = self._readData(self._dataSource)\n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        word2idx, label2idx = self._genVocabulary(news)\n",
    "        # 将标签和句子数值化\n",
    "        newsIds = self._wordToIndex(news, word2idx)\n",
    "        labelIds = labels\n",
    "        # 初始化训练集和测试集\n",
    "        x_train, y_train, x_test, y_test = self._genTrainEvalData(newsIds,labels,word2idx)\n",
    "        self.trainNews = x_train\n",
    "        self.trainLabels = y_train\n",
    "        self.testNews = x_test\n",
    "        self.testLabels = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3] Build Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Secondly,  pick a machine learning algorithm that you think is suitable for this task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You have completed all assignments in this week. The question below is optional. If you still have time, why don't try it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try differnt machine learning algorithms with different combinations of parameters in the practical part, and compare their performances (Better use some visualization techiniques)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
