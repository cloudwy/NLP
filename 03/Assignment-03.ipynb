{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment-03 First Step of Machine Learning: Model and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同学们，今天我们的学习了基本的机器学习概念，相比你已经对机器学习的这些方法有一个基本的认识了。值得说明的是，机器学习不仅仅是一系列方法，更重要的是一种思维体系，即：依据以往的、现有的数据，构建某种方法来解决未见过的问题。而且决策树，贝叶斯只是实现这个目标的一个方法，包括之后的神经网络。很有可能有一天，神经网络也会被淘汰，但是重要的是我们要理解机器学习的目标，就是尽可能的自动化解决未知的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1571556399207&di=4a97dc15ad08dd49d3748d1edf6109b3&imgtype=0&src=http%3A%2F%2Fc.hiphotos.baidu.com%2Fzhidao%2Fwh%3D450%2C600%2Fsign%3Dae742c6aedcd7b89e93932873a146e91%2F5d6034a85edf8db1b16050c40223dd54574e74c7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-1 Programming Review 编程回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Re-code the Linear-Regression Model using scikit-learning(10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>： \n",
    "> + 是否完成线性回归模型 (4')\n",
    "+ 能够进行预测新数据(3')\n",
    "+ 能够进行可视化操作(3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Complete the unfinished KNN Model using pure python to solve the previous Line-Regression problem. (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>:\n",
    "> + 是否完成了KNN模型 (4')\n",
    "+ 是否能够预测新的数据 (4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Re-code the Decision Tree, which could sort the features by salience. (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否实现了信息熵 (1' )\n",
    "+ 是否实现了最优先特征点的选择(5')\n",
    "+ 是否实现了持续的特征选择(6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Finish the K-Means using 2-D matplotlib (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否完成了KMeans模型，基于scikit-learning (3')\n",
    "+ 是否完成了可视化任务（5'）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-2 Question and Answer 问答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What's the *model*? why  all the models are wrong, but some are useful? (5 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A computer-based model is a computer program that is designed to simulate what might or what did happen in a situation. \"All models are wrong\" that is, every model is wrong because it is a simplification of reality. \"But some are useful\" means the simplification can help us solve a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对模型的理解是否正确,对模型的抽象性是否正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the underfitting and overfitting? List the reasons that could make model overfitting or underfitting. (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.cnblogs.com/dinol/p/11605696.html<br>\n",
    "解释bias,variance,overfitting,underfitting之间的关系<br>\n",
    "Overfitting occurs when the model or the algorithm fits the data too well. It shows **low bias but high variance**.<br>\n",
    "Reason: a small dataset/too many features/too much noise<br>\n",
    "\n",
    "Underfitting occurs when the model cannot capture the underlying trend of the data. It shows **high bias but low variance**.<br>\n",
    "Reason: too few features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对过拟合和欠拟合的理解是否正确 (3')\n",
    "+ 对欠拟合产生的原因是否理解正确(2')\n",
    "+ 对过拟合产生的原因是否理解正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What's the precision, recall, AUC, F1, F2score. What are they mainly target on? (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "$$Precision = \\frac{\\sum True\\, positive}{\\sum Predicted\\, condition\\, positive} = \\frac{TP}{TP+FP} $$\n",
    "\n",
    "$$Recall/Sensitivity = \\frac{\\sum True\\,positive}{\\sum Condition\\,positive} = \\frac{TP}{TP+FN}$$\n",
    "\n",
    "$$AUC = the\\,area\\,under\\,the\\,ROC\\,curve$$\n",
    "\n",
    "$$F_{\\beta} = (1+\\beta^{2})·\\frac{Precision·Recall}{(\\beta^{2}·Precision)+Recall}$$\n",
    "\n",
    "$$F_{1} score = 2*\\frac{Precision·Recall}{Precision+Recall}$$\n",
    "\n",
    "$$F_{2} score = 5*\\frac{Precision·Recall}{4·Precision+Recall}$$\n",
    "Precision表示在所有预测为1的样本中，预测正确的概率。<br>\n",
    "Recall表示在所有实际为1的样本中，预测正确的概率。<br>\n",
    "F1 score综合反映了Precision和Recall。<br>\n",
    "ROC曲线是以FP为横轴，TP为纵轴画出的曲线，其下的面积为AUC，如果AUC面积越大，则模型预测效果越好。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对precision, recall, AUC, F1, F2 理解是否正确(6‘)\n",
    "+ 对precision, recall, AUC, F1, F2的使用侧重点是否理解正确 (6’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Based on our course and yourself mind, what's the machine learning?  (8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning is a branch of AI that provides systems the ability to automatically learn and improve from experience without being explicitly programmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，是否能说出来机器学习这种思维方式和传统的分析式编程的区别（8'）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. \"正确定义了机器学习模型的评价标准(evaluation)， 问题基本上就已经解决一半\". 这句话是否正确？你是怎么看待的？ (8‘)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评价标准能判断模型学习的优劣程度，选对评价标准即指明了模型正确学习的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，主要看能理解评价指标对机器学习模型的重要性."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-03 Programming Practice 编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**助教补充资料:决策树**<br>\n",
    "https://github.com/pbharrin/machinelearninginaction/blob/master/Ch03/trees.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:https://blog.csdn.net/weixin_41059350/article/details/90242277"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In our course and previous practice, we complete some importance components of Decision Tree. In this problem, you need to build a **completed** Decision Tree Model. You show finish a `predicate()` function, which accepts three parameters **<gender, income, family_number>**, and outputs the predicated 'bought': 1 or 0.  (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from icecream import ic\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step1:创建数据**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = {\n",
    "    'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "    'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "    'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "    'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入数据：dict->DataFrame\n",
    "dataset = pd.DataFrame.from_dict(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'bought'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender', 'income', 'family_number']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = dataset.columns.tolist()\n",
    "labels.remove(target)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step2：计算Entropy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#信息熵\n",
    "def entropy(elements):\n",
    "    counter = Counter(elements)\n",
    "    probs = [counter[c]/len(elements) for c in set(elements)]\n",
    "    #ic(probs)\n",
    "    return - sum(p * np.log(p) for p in probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step3：选择最优特征**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_spliter(training_data:pd.DataFrame,target:str)->str:\n",
    "    x_fields = set(training_data.columns.tolist()) - {target}\n",
    "    #ic(x_fields)\n",
    "    spliter = None\n",
    "    min_entropy = float('inf')\n",
    "    \n",
    "    for f in x_fields:\n",
    "        ic(f)\n",
    "        #print('f:',type(f))\n",
    "        values = set(training_data[f])\n",
    "        ic(values)\n",
    "        for v in values:\n",
    "            #print('v:',type(v))\n",
    "            #sub_spliter_1\n",
    "            sub_spliter_1 = training_data[training_data[f]== v][target].tolist()\n",
    "            #ic(sub_spliter_1)\n",
    "            entropy_1 = entropy(sub_spliter_1)\n",
    "            #ic(entropy_1)\n",
    "            #sub_spliter_2\n",
    "            sub_spliter_2 = training_data[training_data[f]!= v][target].tolist()\n",
    "            #ic(sub_spliter_2)\n",
    "            entropy_2 = entropy(sub_spliter_2)\n",
    "            #ic(entropy_2)\n",
    "            #the sum of entropy\n",
    "            entropy_v = entropy_1 + entropy_2\n",
    "            #ic(entropy_v) \n",
    "            #compare\n",
    "            if entropy_v <= min_entropy:\n",
    "                min_entropy = entropy_v\n",
    "                spliter = (f,v)\n",
    "    print('spliter is: {}'.format(spliter))\n",
    "    print('the min entropy is: {}'.format(min_entropy)) \n",
    "    return spliter[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'family_number'\n",
      "ic| values: {1, 2}\n",
      "ic| f: 'gender'\n",
      "ic| values: {'F', 'M'}\n",
      "ic| f: 'income'\n",
      "ic| values: {'+10', '-10'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliter is: ('income', '-10')\n",
      "the min entropy is: 0.6730116670092565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'income'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_spliter(dataset,target='bought')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step4：按照给定特征划分数据集**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataset(training_data:pd.DataFrame,spliter:str,value):\n",
    "    subset = training_data[training_data[spliter] == value]\n",
    "    return subset.drop(spliter,axis=1) #删除dataFrame的某一列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset1 = splitDataset(dataset,spliter='family_number',value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender  bought\n",
       "0      F       1\n",
       "1      F       1\n",
       "3      F       0\n",
       "4      M       0\n",
       "5      M       0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset1.drop('income',axis=1) #删除某列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  bought\n",
       "1      F    -10       1\n",
       "3      F    +10       0\n",
       "4      M    +10       0\n",
       "5      M    +10       0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset1.drop(0,axis=0) #删除某行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step5:多数表决**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def majority(targetList:list):\n",
    "    classCount={}\n",
    "    for v in targetList:\n",
    "        if v not in classCount.keys():\n",
    "            classCount[v]=0\n",
    "        classCount[v]+=1\n",
    "    sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return sortedClassCount[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注释：\n",
    "\n",
    "    1.dict.items()\n",
    "    作用：是可以将字典中的所有项，以列表方式返回。因为字典是无序的，所以用items方法返回字典的所有项，也是没有顺序的。\n",
    "    2.operator.itemgetter()\n",
    "    operator模块提供的itemgetter函数用于获取对象的哪些维的数据，参数为一些序号.\n",
    "    3.sorted()函数，排序\n",
    "    list.sort()是对已经存在的列表进行操作，进而可以改变进行操作的列表；\n",
    "    sorted返回的是一个新的list，而不是在原来的基础上进行的操作\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  bought\n",
       "0      F    +10       1\n",
       "1      F    -10       1\n",
       "3      F    +10       0\n",
       "4      M    +10       0\n",
       "5      M    +10       0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = splitDataset(dataset,spliter='family_number',value=1)\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetList = subset['bought'].tolist()\n",
    "targetList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "majority(targetList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step6:创建树**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTree(training_data:pd.DataFrame,labels:list,target:str):\n",
    "    ic(labels)\n",
    "    targetList = training_data[target].tolist() #等于classList\n",
    "    labels1 = labels[:]\n",
    "    #类别完全相同则停止划分\n",
    "    if targetList.count(targetList[0]) == len(targetList):\n",
    "        return targetList[0]\n",
    "    #遍历完所有特征时返回出现次数最多的类别\n",
    "    if training_data.shape[1] == 1:\n",
    "        return majority(targetList)\n",
    "    bestFeat = best_spliter(training_data,target)\n",
    "    tree = {bestFeat:{}}\n",
    "    labels1.remove(bestFeat)\n",
    "    #得到bestFeat的所有值\n",
    "    featValue = training_data[bestFeat].tolist()\n",
    "    uniqueVal = set(featValue)\n",
    "    for v in uniqueVal:\n",
    "        sublabels = labels1\n",
    "        #运用递归方法\n",
    "        tree[bestFeat][v] = createTree(splitDataset(training_data,bestFeat,v),sublabels,target)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| labels: ['gender', 'income', 'family_number']\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1, 2}\n",
      "ic| f: 'gender'\n",
      "ic| values: {'F', 'M'}\n",
      "ic| f: 'income'\n",
      "ic| values: {'+10', '-10'}\n",
      "ic| labels: ['gender', 'family_number']\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1, 2}\n",
      "ic| f: 'gender'\n",
      "ic| values: {'F', 'M'}\n",
      "ic| "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliter is: ('income', '-10')\n",
      "the min entropy is: 0.6730116670092565\n",
      "spliter is: ('family_number', 2)\n",
      "the min entropy is: 0.5623351446188083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "labels: ['gender']\n",
      "ic| f: 'gender'\n",
      "ic| values: {'F', 'M'}\n",
      "ic| labels: []\n",
      "ic| labels: []\n",
      "ic| labels: ['gender']\n",
      "ic| labels: ['gender', 'family_number']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliter is: ('gender', 'M')\n",
      "the min entropy is: 0.6931471805599453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'income': {'+10': {'family_number': {1: {'gender': {'F': 1, 'M': 0}}, 2: 1}},\n",
       "  '-10': 1}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = createTree(dataset,labels,'bought')\n",
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'family_number': {1: {'gender': {'F': 1, 'M': 0}}, 2: 1}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree['income']['+10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step7:使用决策树分类**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tree,labels,sample):\n",
    "    sample1 = dict(zip(labels,sample))\n",
    "    #ic(testSample)\n",
    "    feat = list(tree.keys())[0]\n",
    "    ic(feat)\n",
    "    sub = tree[feat][sample1[feat]]\n",
    "    ic(sub)\n",
    "    if isinstance(sub,dict):\n",
    "        return predict(sub,labels,sample)\n",
    "    else:\n",
    "        return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| feat: 'income'\n",
      "ic| sub: {'family_number': {1: {'gender': {'F': 1, 'M': 0}}, 2: 1}}\n",
      "ic| feat: 'family_number'\n",
      "ic| sub: {'gender': {'F': 1, 'M': 0}}\n",
      "ic| feat: 'gender'\n",
      "ic| sub: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#<gender, income, family_number>\n",
    "sample = ('F','+10',1)\n",
    "predict(tree,labels,sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| feat: 'income'\n",
      "ic| sub: {'family_number': {1: {'gender': {'F': 1, 'M': 0}}, 2: 1}}\n",
      "ic| feat: 'family_number'\n",
      "ic| sub: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = ('M','+10',2)\n",
    "predict(tree,labels,sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否将之前的决策树模型的部分进行合并组装， predicate函数能够顺利运行(8')\n",
    "+ 是够能够输入未曾见过的X变量，例如gender, income, family_number 分别是： <M, -10, 1>, 模型能够预测出结果 (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 将上一节课(第二节课)的线性回归问题中的Loss函数改成\"绝对值\"，并且改变其偏导的求值方式，观察其结果的变化。(19 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step1:Loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$loss = \\frac{1}{n} \\sum{(\\left|y_i - \\hat{y_i}\\right|)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y,y_hat):\n",
    "    return sum(abs(y_i - y_hat_i) for y_i, y_hat_i in zip(list(y),list(y_hat)))/len(list(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step2:Partial derivatives**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{loss}}{\\partial{k}} = -\\frac{1}{n}\\sum sign(y_i-\\hat{y_i})*x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{loss}}{\\partial{b}} = -\\frac{1}{n}\\sum sign(y_i - \\hat{y_i})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define partial derivative \n",
    "def partial_derivative_k(x, y, y_hat): \n",
    "    n = len(y)\n",
    "    gradient = 0\n",
    "    for x_i, y_i, y_hat_i in zip(list(x),list(y),list(y_hat)):\n",
    "        gradient += np.sign(y_i-y_hat_i) * x_i\n",
    "    return -1/n * gradient\n",
    "\n",
    "def partial_derivative_b(y, y_hat):\n",
    "    n = len(y)\n",
    "    gradient = 0\n",
    "    for y_i, y_hat_i in zip(list(y),list(y_hat)):\n",
    "        gradient += np.sign(y_i-y_hat_i)\n",
    "    return -1/n * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step3:Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_boston()\n",
    "x,y=dataset['data'],dataset['target']\n",
    "X_rm = x[:,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step4:Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define target function\n",
    "def price(rm, k, b):\n",
    "    return k * rm + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, the loss is 139.09401246681074, parameters k is -4.317165812023703 and b is -89.42939742455647\n",
      "Iteration 1, the loss is 138.689046172984, parameters k is -4.254319468150185 and b is -89.41939742455646\n",
      "Iteration 2, the loss is 138.28407987915705, parameters k is -4.191473124276667 and b is -89.40939742455646\n",
      "Iteration 3, the loss is 137.87911358533015, parameters k is -4.12862678040315 and b is -89.39939742455645\n",
      "Iteration 4, the loss is 137.47414729150336, parameters k is -4.065780436529632 and b is -89.38939742455645\n",
      "Iteration 5, the loss is 137.06918099767665, parameters k is -4.0029340926561146 and b is -89.37939742455644\n",
      "Iteration 6, the loss is 136.66421470384972, parameters k is -3.9400877487825965 and b is -89.36939742455644\n",
      "Iteration 7, the loss is 136.2592484100228, parameters k is -3.8772414049090784 and b is -89.35939742455643\n",
      "Iteration 8, the loss is 135.85428211619595, parameters k is -3.8143950610355604 and b is -89.34939742455643\n",
      "Iteration 9, the loss is 135.4493158223691, parameters k is -3.7515487171620423 and b is -89.33939742455642\n",
      "Iteration 10, the loss is 135.04434952854217, parameters k is -3.6887023732885242 and b is -89.32939742455642\n",
      "Iteration 11, the loss is 134.6393832347153, parameters k is -3.625856029415006 and b is -89.31939742455641\n",
      "Iteration 12, the loss is 134.2344169408886, parameters k is -3.563009685541488 and b is -89.30939742455641\n",
      "Iteration 13, the loss is 133.82945064706166, parameters k is -3.50016334166797 and b is -89.2993974245564\n",
      "Iteration 14, the loss is 133.42448435323496, parameters k is -3.437316997794452 and b is -89.2893974245564\n",
      "Iteration 15, the loss is 133.0195180594081, parameters k is -3.374470653920934 and b is -89.2793974245564\n",
      "Iteration 16, the loss is 132.61455176558096, parameters k is -3.311624310047416 and b is -89.26939742455639\n",
      "Iteration 17, the loss is 132.20958547175417, parameters k is -3.248777966173898 and b is -89.25939742455638\n",
      "Iteration 18, the loss is 131.80461917792755, parameters k is -3.1859316223003797 and b is -89.24939742455638\n",
      "Iteration 19, the loss is 131.39965288410062, parameters k is -3.1230852784268617 and b is -89.23939742455637\n",
      "Iteration 20, the loss is 130.99468659027377, parameters k is -3.0602389345533436 and b is -89.22939742455637\n",
      "Iteration 21, the loss is 130.58972029644693, parameters k is -2.9973925906798256 and b is -89.21939742455636\n",
      "Iteration 22, the loss is 130.18475400262005, parameters k is -2.9345462468063075 and b is -89.20939742455636\n",
      "Iteration 23, the loss is 129.7797877087933, parameters k is -2.8716999029327894 and b is -89.19939742455635\n",
      "Iteration 24, the loss is 129.37482141496636, parameters k is -2.8088535590592714 and b is -89.18939742455635\n",
      "Iteration 25, the loss is 128.9698551211395, parameters k is -2.7460072151857533 and b is -89.17939742455634\n",
      "Iteration 26, the loss is 128.56488882731276, parameters k is -2.6831608713122352 and b is -89.16939742455634\n",
      "Iteration 27, the loss is 128.15992253348577, parameters k is -2.620314527438717 and b is -89.15939742455633\n",
      "Iteration 28, the loss is 127.75495623965892, parameters k is -2.557468183565199 and b is -89.14939742455633\n",
      "Iteration 29, the loss is 127.3499899458321, parameters k is -2.494621839691681 and b is -89.13939742455632\n",
      "Iteration 30, the loss is 126.94502365200525, parameters k is -2.431775495818163 and b is -89.12939742455632\n",
      "Iteration 31, the loss is 126.54005735817827, parameters k is -2.368929151944645 and b is -89.11939742455631\n",
      "Iteration 32, the loss is 126.13509106435154, parameters k is -2.306082808071127 and b is -89.1093974245563\n",
      "Iteration 33, the loss is 125.73012477052468, parameters k is -2.243236464197609 and b is -89.0993974245563\n",
      "Iteration 34, the loss is 125.32515847669788, parameters k is -2.1803901203240907 and b is -89.0893974245563\n",
      "Iteration 35, the loss is 124.92019218287098, parameters k is -2.1175437764505727 and b is -89.07939742455629\n",
      "Iteration 36, the loss is 124.51522588904419, parameters k is -2.0546974325770546 and b is -89.06939742455629\n",
      "Iteration 37, the loss is 124.11025959521737, parameters k is -1.9918510887035368 and b is -89.05939742455628\n",
      "Iteration 38, the loss is 123.7052933013904, parameters k is -1.929004744830019 and b is -89.04939742455628\n",
      "Iteration 39, the loss is 123.30032700756362, parameters k is -1.866158400956501 and b is -89.03939742455627\n",
      "Iteration 40, the loss is 122.89536071373676, parameters k is -1.8033120570829833 and b is -89.02939742455627\n",
      "Iteration 41, the loss is 122.4903944199099, parameters k is -1.7404657132094654 and b is -89.01939742455626\n",
      "Iteration 42, the loss is 122.08542812608289, parameters k is -1.6776193693359476 and b is -89.00939742455625\n",
      "Iteration 43, the loss is 121.68046183225623, parameters k is -1.6147730254624297 and b is -88.99939742455625\n",
      "Iteration 44, the loss is 121.27549553842925, parameters k is -1.551926681588912 and b is -88.98939742455624\n",
      "Iteration 45, the loss is 120.87052924460248, parameters k is -1.489080337715394 and b is -88.97939742455624\n",
      "Iteration 46, the loss is 120.46556295077572, parameters k is -1.4262339938418762 and b is -88.96939742455623\n",
      "Iteration 47, the loss is 120.06059665694875, parameters k is -1.3633876499683584 and b is -88.95939742455623\n",
      "Iteration 48, the loss is 119.65563036312183, parameters k is -1.3005413060948405 and b is -88.94939742455622\n",
      "Iteration 49, the loss is 119.25066406929504, parameters k is -1.2376949622213227 and b is -88.93939742455622\n",
      "Iteration 50, the loss is 118.84569777546828, parameters k is -1.1748486183478049 and b is -88.92939742455621\n",
      "Iteration 51, the loss is 118.44073148164148, parameters k is -1.112002274474287 and b is -88.91939742455621\n",
      "Iteration 52, the loss is 118.03576518781453, parameters k is -1.0491559306007692 and b is -88.9093974245562\n",
      "Iteration 53, the loss is 117.63079889398762, parameters k is -0.9863095867272513 and b is -88.8993974245562\n",
      "Iteration 54, the loss is 117.22583260016091, parameters k is -0.9234632428537335 and b is -88.8893974245562\n",
      "Iteration 55, the loss is 116.82086630633385, parameters k is -0.8606168989802156 and b is -88.87939742455619\n",
      "Iteration 56, the loss is 116.41590001250711, parameters k is -0.7977705551066978 and b is -88.86939742455618\n",
      "Iteration 57, the loss is 116.01093371868032, parameters k is -0.73492421123318 and b is -88.85939742455618\n",
      "Iteration 58, the loss is 115.6059674248534, parameters k is -0.6720778673596621 and b is -88.84939742455617\n",
      "Iteration 59, the loss is 115.20100113102666, parameters k is -0.6092315234861443 and b is -88.83939742455617\n",
      "Iteration 60, the loss is 114.79603483719968, parameters k is -0.5463851796126264 and b is -88.82939742455616\n",
      "Iteration 61, the loss is 114.39106854337285, parameters k is -0.4835388357391086 and b is -88.81939742455616\n",
      "Iteration 62, the loss is 113.98610224954606, parameters k is -0.42069249186559077 and b is -88.80939742455615\n",
      "Iteration 63, the loss is 113.58113595571919, parameters k is -0.3578461479920729 and b is -88.79939742455615\n",
      "Iteration 64, the loss is 113.17616966189217, parameters k is -0.2949998041185551 and b is -88.78939742455614\n",
      "Iteration 65, the loss is 112.77120336806558, parameters k is -0.23215346024503722 and b is -88.77939742455614\n",
      "Iteration 66, the loss is 112.36623707423868, parameters k is -0.16930711637151935 and b is -88.76939742455613\n",
      "Iteration 67, the loss is 111.96127078041168, parameters k is -0.10646077249800148 and b is -88.75939742455613\n",
      "Iteration 68, the loss is 111.55630448658485, parameters k is -0.043614428624483614 and b is -88.74939742455612\n",
      "Iteration 69, the loss is 111.15133819275806, parameters k is 0.019231915249034254 and b is -88.73939742455612\n",
      "Iteration 70, the loss is 110.7463718989312, parameters k is 0.08207825912255212 and b is -88.72939742455611\n",
      "Iteration 71, the loss is 110.34140560510428, parameters k is 0.14492460299607 and b is -88.7193974245561\n",
      "Iteration 72, the loss is 109.93643931127748, parameters k is 0.20777094686958786 and b is -88.7093974245561\n",
      "Iteration 73, the loss is 109.53147301745062, parameters k is 0.2706172907431057 and b is -88.6993974245561\n",
      "Iteration 74, the loss is 109.12650672362373, parameters k is 0.33346363461662354 and b is -88.68939742455609\n",
      "Iteration 75, the loss is 108.72154042979705, parameters k is 0.3963099784901414 and b is -88.67939742455609\n",
      "Iteration 76, the loss is 108.31657413597014, parameters k is 0.4591563223636592 and b is -88.66939742455608\n",
      "Iteration 77, the loss is 107.91160784214321, parameters k is 0.5220026662371771 and b is -88.65939742455608\n",
      "Iteration 78, the loss is 107.50664154831644, parameters k is 0.5848490101106949 and b is -88.64939742455607\n",
      "Iteration 79, the loss is 107.10167525448942, parameters k is 0.6476953539842127 and b is -88.63939742455607\n",
      "Iteration 80, the loss is 106.69670896066262, parameters k is 0.7105416978577306 and b is -88.62939742455606\n",
      "Iteration 81, the loss is 106.29174266683583, parameters k is 0.7733880417312484 and b is -88.61939742455606\n",
      "Iteration 82, the loss is 105.88677637300906, parameters k is 0.8362343856047663 and b is -88.60939742455605\n",
      "Iteration 83, the loss is 105.48181007918211, parameters k is 0.8990807294782841 and b is -88.59939742455605\n",
      "Iteration 84, the loss is 105.07684378535525, parameters k is 0.9619270733518019 and b is -88.58939742455604\n",
      "Iteration 85, the loss is 104.67187749152855, parameters k is 1.0247734172253198 and b is -88.57939742455603\n",
      "Iteration 86, the loss is 104.26691119770156, parameters k is 1.0876197610988376 and b is -88.56939742455603\n",
      "Iteration 87, the loss is 103.86194490387464, parameters k is 1.1504661049723555 and b is -88.55939742455602\n",
      "Iteration 88, the loss is 103.45697861004781, parameters k is 1.2133124488458733 and b is -88.54939742455602\n",
      "Iteration 89, the loss is 103.05201231622105, parameters k is 1.2761587927193911 and b is -88.53939742455601\n",
      "Iteration 90, the loss is 102.64704602239416, parameters k is 1.339005136592909 and b is -88.52939742455601\n",
      "Iteration 91, the loss is 102.24207972856752, parameters k is 1.4018514804664268 and b is -88.519397424556\n",
      "Iteration 92, the loss is 101.83711343474044, parameters k is 1.4646978243399447 and b is -88.509397424556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 93, the loss is 101.43214714091364, parameters k is 1.5275441682134625 and b is -88.499397424556\n",
      "Iteration 94, the loss is 101.02718084708684, parameters k is 1.5903905120869803 and b is -88.48939742455599\n",
      "Iteration 95, the loss is 100.62221455326006, parameters k is 1.6532368559604982 and b is -88.47939742455598\n",
      "Iteration 96, the loss is 100.2172482594332, parameters k is 1.716083199834016 and b is -88.46939742455598\n",
      "Iteration 97, the loss is 99.81228196560632, parameters k is 1.7789295437075339 and b is -88.45939742455597\n",
      "Iteration 98, the loss is 99.40731567177946, parameters k is 1.8417758875810517 and b is -88.44939742455597\n",
      "Iteration 99, the loss is 99.00234937795263, parameters k is 1.9046222314545695 and b is -88.43939742455596\n",
      "Iteration 100, the loss is 98.59738308412574, parameters k is 1.9674685753280874 and b is -88.42939742455596\n",
      "Iteration 101, the loss is 98.19241679029898, parameters k is 2.0303149192016052 and b is -88.41939742455595\n",
      "Iteration 102, the loss is 97.78745049647218, parameters k is 2.0931612630751233 and b is -88.40939742455595\n",
      "Iteration 103, the loss is 97.38248420264519, parameters k is 2.1560076069486414 and b is -88.39939742455594\n",
      "Iteration 104, the loss is 96.9775179088183, parameters k is 2.2188539508221594 and b is -88.38939742455594\n",
      "Iteration 105, the loss is 96.57255161499145, parameters k is 2.2817002946956775 and b is -88.37939742455593\n",
      "Iteration 106, the loss is 96.16758532116458, parameters k is 2.3445466385691955 and b is -88.36939742455593\n",
      "Iteration 107, the loss is 95.7626190273378, parameters k is 2.4073929824427136 and b is -88.35939742455592\n",
      "Iteration 108, the loss is 95.35765273351096, parameters k is 2.4702393263162317 and b is -88.34939742455592\n",
      "Iteration 109, the loss is 94.95268643968399, parameters k is 2.5330856701897497 and b is -88.33939742455591\n",
      "Iteration 110, the loss is 94.5477201458573, parameters k is 2.595932014063268 and b is -88.3293974245559\n",
      "Iteration 111, the loss is 94.14275385203032, parameters k is 2.658778357936786 and b is -88.3193974245559\n",
      "Iteration 112, the loss is 93.73778755820342, parameters k is 2.721624701810304 and b is -88.3093974245559\n",
      "Iteration 113, the loss is 93.3328212643766, parameters k is 2.784471045683822 and b is -88.29939742455589\n",
      "Iteration 114, the loss is 92.92785497054977, parameters k is 2.84731738955734 and b is -88.28939742455589\n",
      "Iteration 115, the loss is 92.52288867672297, parameters k is 2.910163733430858 and b is -88.27939742455588\n",
      "Iteration 116, the loss is 92.1179223828961, parameters k is 2.973010077304376 and b is -88.26939742455588\n",
      "Iteration 117, the loss is 91.71295608906935, parameters k is 3.0358564211778942 and b is -88.25939742455587\n",
      "Iteration 118, the loss is 91.30798979524236, parameters k is 3.0987027650514123 and b is -88.24939742455587\n",
      "Iteration 119, the loss is 90.90302350141548, parameters k is 3.1615491089249304 and b is -88.23939742455586\n",
      "Iteration 120, the loss is 90.49805720758862, parameters k is 3.2243954527984484 and b is -88.22939742455586\n",
      "Iteration 121, the loss is 90.09309091376183, parameters k is 3.2872417966719665 and b is -88.21939742455585\n",
      "Iteration 122, the loss is 89.68812461993494, parameters k is 3.3500881405454845 and b is -88.20939742455585\n",
      "Iteration 123, the loss is 89.28315832610818, parameters k is 3.4129344844190026 and b is -88.19939742455584\n",
      "Iteration 124, the loss is 88.87819203228123, parameters k is 3.4757808282925207 and b is -88.18939742455584\n",
      "Iteration 125, the loss is 88.47322573845442, parameters k is 3.5386271721660387 and b is -88.17939742455583\n",
      "Iteration 126, the loss is 88.06825944462769, parameters k is 3.601473516039557 and b is -88.16939742455583\n",
      "Iteration 127, the loss is 87.66329315080071, parameters k is 3.664319859913075 and b is -88.15939742455582\n",
      "Iteration 128, the loss is 87.2583268569738, parameters k is 3.727166203786593 and b is -88.14939742455581\n",
      "Iteration 129, the loss is 86.85336056314696, parameters k is 3.790012547660111 and b is -88.13939742455581\n",
      "Iteration 130, the loss is 86.44839426932016, parameters k is 3.852858891533629 and b is -88.1293974245558\n",
      "Iteration 131, the loss is 86.0434279754933, parameters k is 3.915705235407147 and b is -88.1193974245558\n",
      "Iteration 132, the loss is 85.63846168166647, parameters k is 3.978551579280665 and b is -88.1093974245558\n",
      "Iteration 133, the loss is 85.23349538783965, parameters k is 4.041397923154183 and b is -88.09939742455579\n",
      "Iteration 134, the loss is 84.82852909401274, parameters k is 4.104244267027701 and b is -88.08939742455578\n",
      "Iteration 135, the loss is 84.42356280018595, parameters k is 4.1670906109012185 and b is -88.07939742455578\n",
      "Iteration 136, the loss is 84.01859650635905, parameters k is 4.229936954774736 and b is -88.06939742455577\n",
      "Iteration 137, the loss is 83.61363021253227, parameters k is 4.292783298648254 and b is -88.05939742455577\n",
      "Iteration 138, the loss is 83.20866391870535, parameters k is 4.355629642521771 and b is -88.04939742455576\n",
      "Iteration 139, the loss is 82.80369762487852, parameters k is 4.418475986395289 and b is -88.03939742455576\n",
      "Iteration 140, the loss is 82.39873133105179, parameters k is 4.4813223302688066 and b is -88.02939742455575\n",
      "Iteration 141, the loss is 81.99376503722492, parameters k is 4.544168674142324 and b is -88.01939742455575\n",
      "Iteration 142, the loss is 81.58879874339802, parameters k is 4.607015018015842 and b is -88.00939742455574\n",
      "Iteration 143, the loss is 81.18383244957109, parameters k is 4.669861361889359 and b is -87.99939742455574\n",
      "Iteration 144, the loss is 80.77886615574427, parameters k is 4.732707705762877 and b is -87.98939742455573\n",
      "Iteration 145, the loss is 80.37389986191741, parameters k is 4.795554049636395 and b is -87.97939742455573\n",
      "Iteration 146, the loss is 79.96893356809065, parameters k is 4.858400393509912 and b is -87.96939742455572\n",
      "Iteration 147, the loss is 79.5639672742637, parameters k is 4.92124673738343 and b is -87.95939742455572\n",
      "Iteration 148, the loss is 79.1590009804369, parameters k is 4.9840930812569475 and b is -87.94939742455571\n",
      "Iteration 149, the loss is 78.75403468661005, parameters k is 5.046939425130465 and b is -87.93939742455571\n",
      "Iteration 150, the loss is 78.34906839278321, parameters k is 5.109785769003983 and b is -87.9293974245557\n",
      "Iteration 151, the loss is 77.94410209895638, parameters k is 5.1726321128775 and b is -87.9193974245557\n",
      "Iteration 152, the loss is 77.5391358051295, parameters k is 5.235478456751018 and b is -87.90939742455569\n",
      "Iteration 153, the loss is 77.13416951130274, parameters k is 5.298324800624536 and b is -87.89939742455569\n",
      "Iteration 154, the loss is 76.72920321747576, parameters k is 5.361171144498053 and b is -87.88939742455568\n",
      "Iteration 155, the loss is 76.32423692364893, parameters k is 5.424017488371571 and b is -87.87939742455568\n",
      "Iteration 156, the loss is 75.91927062982214, parameters k is 5.4868638322450884 and b is -87.86939742455567\n",
      "Iteration 157, the loss is 75.51430433599516, parameters k is 5.549710176118606 and b is -87.85939742455567\n",
      "Iteration 158, the loss is 75.10933804216842, parameters k is 5.612556519992124 and b is -87.84939742455566\n",
      "Iteration 159, the loss is 74.70437174834169, parameters k is 5.675402863865641 and b is -87.83939742455566\n",
      "Iteration 160, the loss is 74.29940545451474, parameters k is 5.738249207739159 and b is -87.82939742455565\n",
      "Iteration 161, the loss is 73.89443916068785, parameters k is 5.8010955516126765 and b is -87.81939742455565\n",
      "Iteration 162, the loss is 73.48947286686099, parameters k is 5.863941895486194 and b is -87.80939742455564\n",
      "Iteration 163, the loss is 73.08450657303416, parameters k is 5.926788239359712 and b is -87.79939742455564\n",
      "Iteration 164, the loss is 72.67954027920737, parameters k is 5.989634583233229 and b is -87.78939742455563\n",
      "Iteration 165, the loss is 72.27457398538044, parameters k is 6.052480927106747 and b is -87.77939742455563\n",
      "Iteration 166, the loss is 71.8696076915537, parameters k is 6.115327270980265 and b is -87.76939742455562\n",
      "Iteration 167, the loss is 71.4646413977267, parameters k is 6.178173614853782 and b is -87.75939742455562\n",
      "Iteration 168, the loss is 71.05967510389995, parameters k is 6.2410199587273 and b is -87.74939742455561\n",
      "Iteration 169, the loss is 70.65470881007309, parameters k is 6.3038663026008175 and b is -87.7393974245556\n",
      "Iteration 170, the loss is 70.24974251624622, parameters k is 6.366712646474335 and b is -87.7293974245556\n",
      "Iteration 171, the loss is 69.8447762224194, parameters k is 6.429558990347853 and b is -87.7193974245556\n",
      "Iteration 172, the loss is 69.43980992859247, parameters k is 6.49240533422137 and b is -87.70939742455559\n",
      "Iteration 173, the loss is 69.03484363476572, parameters k is 6.555251678094888 and b is -87.69939742455558\n",
      "Iteration 174, the loss is 68.62987734093882, parameters k is 6.618098021968406 and b is -87.68939742455558\n",
      "Iteration 175, the loss is 68.22491104711192, parameters k is 6.680944365841923 and b is -87.67939742455557\n",
      "Iteration 176, the loss is 67.8199447532851, parameters k is 6.743790709715441 and b is -87.66939742455557\n",
      "Iteration 177, the loss is 67.41497845945833, parameters k is 6.806637053588958 and b is -87.65939742455556\n",
      "Iteration 178, the loss is 67.01001216563144, parameters k is 6.869483397462476 and b is -87.64939742455556\n",
      "Iteration 179, the loss is 66.60504587180458, parameters k is 6.932329741335994 and b is -87.63939742455555\n",
      "Iteration 180, the loss is 66.20007957797779, parameters k is 6.995176085209511 and b is -87.62939742455555\n",
      "Iteration 181, the loss is 65.79511328415087, parameters k is 7.058022429083029 and b is -87.61939742455554\n",
      "Iteration 182, the loss is 65.39014699032401, parameters k is 7.1208687729565465 and b is -87.60939742455554\n",
      "Iteration 183, the loss is 64.98518069649721, parameters k is 7.183715116830064 and b is -87.59939742455553\n",
      "Iteration 184, the loss is 64.58021440267028, parameters k is 7.246561460703582 and b is -87.58939742455553\n",
      "Iteration 185, the loss is 64.17524810884349, parameters k is 7.309407804577099 and b is -87.57939742455552\n",
      "Iteration 186, the loss is 63.770281815016695, parameters k is 7.372254148450617 and b is -87.56939742455552\n",
      "Iteration 187, the loss is 63.365315521189764, parameters k is 7.435100492324135 and b is -87.55939742455551\n",
      "Iteration 188, the loss is 62.96034922736295, parameters k is 7.497946836197652 and b is -87.54939742455551\n",
      "Iteration 189, the loss is 62.55538293353613, parameters k is 7.56079318007117 and b is -87.5393974245555\n",
      "Iteration 190, the loss is 62.150416639709235, parameters k is 7.6236395239446875 and b is -87.5293974245555\n",
      "Iteration 191, the loss is 61.74545034588236, parameters k is 7.686485867818205 and b is -87.51939742455549\n",
      "Iteration 192, the loss is 61.340484052055544, parameters k is 7.749332211691723 and b is -87.50939742455549\n",
      "Iteration 193, the loss is 60.935517758228684, parameters k is 7.81217855556524 and b is -87.49939742455548\n",
      "Iteration 194, the loss is 60.530551464401874, parameters k is 7.875024899438758 and b is -87.48939742455548\n",
      "Iteration 195, the loss is 60.125585170574986, parameters k is 7.937871243312276 and b is -87.47939742455547\n",
      "Iteration 196, the loss is 59.72061887674819, parameters k is 8.000717587185793 and b is -87.46939742455547\n",
      "Iteration 197, the loss is 59.31565258292129, parameters k is 8.063563931059312 and b is -87.45939742455546\n",
      "Iteration 198, the loss is 58.91068628909447, parameters k is 8.12641027493283 and b is -87.44939742455546\n",
      "Iteration 199, the loss is 58.50571999526759, parameters k is 8.189256618806349 and b is -87.43939742455545\n",
      "Iteration 200, the loss is 58.10075370144068, parameters k is 8.252102962679867 and b is -87.42939742455545\n",
      "Iteration 201, the loss is 57.6957874076139, parameters k is 8.314949306553386 and b is -87.41939742455544\n",
      "Iteration 202, the loss is 57.290821113787004, parameters k is 8.377795650426904 and b is -87.40939742455544\n",
      "Iteration 203, the loss is 56.88585481996017, parameters k is 8.440641994300423 and b is -87.39939742455543\n",
      "Iteration 204, the loss is 56.48088852613329, parameters k is 8.503488338173941 and b is -87.38939742455543\n",
      "Iteration 205, the loss is 56.07592223230652, parameters k is 8.56633468204746 and b is -87.37939742455542\n",
      "Iteration 206, the loss is 55.67095593847961, parameters k is 8.629181025920978 and b is -87.36939742455542\n",
      "Iteration 207, the loss is 55.26598964465272, parameters k is 8.692027369794497 and b is -87.35939742455541\n",
      "Iteration 208, the loss is 54.86102335082589, parameters k is 8.754873713668015 and b is -87.3493974245554\n",
      "Iteration 209, the loss is 54.45605705699909, parameters k is 8.817720057541534 and b is -87.3393974245554\n",
      "Iteration 210, the loss is 54.05109076317216, parameters k is 8.880566401415052 and b is -87.3293974245554\n",
      "Iteration 211, the loss is 53.64612446934536, parameters k is 8.94341274528857 and b is -87.31939742455539\n",
      "Iteration 212, the loss is 53.24115817551849, parameters k is 9.00625908916209 and b is -87.30939742455539\n",
      "Iteration 213, the loss is 52.836191881691605, parameters k is 9.069105433035608 and b is -87.29939742455538\n",
      "Iteration 214, the loss is 52.43122558786482, parameters k is 9.131951776909126 and b is -87.28939742455537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 215, the loss is 52.02625929403796, parameters k is 9.194798120782645 and b is -87.27939742455537\n",
      "Iteration 216, the loss is 51.62129300021103, parameters k is 9.257644464656163 and b is -87.26939742455536\n",
      "Iteration 217, the loss is 51.2163267063842, parameters k is 9.320490808529682 and b is -87.25939742455536\n",
      "Iteration 218, the loss is 50.81136041255735, parameters k is 9.3833371524032 and b is -87.24939742455535\n",
      "Iteration 219, the loss is 50.40639411873049, parameters k is 9.446183496276719 and b is -87.23939742455535\n",
      "Iteration 220, the loss is 50.001427824903686, parameters k is 9.509029840150237 and b is -87.22939742455534\n",
      "Iteration 221, the loss is 49.59646153107685, parameters k is 9.571876184023756 and b is -87.21939742455534\n",
      "Iteration 222, the loss is 49.19149523724994, parameters k is 9.634722527897274 and b is -87.20939742455533\n",
      "Iteration 223, the loss is 48.78652894342309, parameters k is 9.697568871770793 and b is -87.19939742455533\n",
      "Iteration 224, the loss is 48.38156264959625, parameters k is 9.760415215644311 and b is -87.18939742455532\n",
      "Iteration 225, the loss is 47.97659635576936, parameters k is 9.82326155951783 and b is -87.17939742455532\n",
      "Iteration 226, the loss is 47.57163006194251, parameters k is 9.886107903391348 and b is -87.16939742455531\n",
      "Iteration 227, the loss is 47.166663768115676, parameters k is 9.948954247264867 and b is -87.15939742455531\n",
      "Iteration 228, the loss is 46.761697474288844, parameters k is 10.011800591138385 and b is -87.1493974245553\n",
      "Iteration 229, the loss is 46.35673118046195, parameters k is 10.074646935011904 and b is -87.1393974245553\n",
      "Iteration 230, the loss is 45.95176488663509, parameters k is 10.137493278885422 and b is -87.1293974245553\n",
      "Iteration 231, the loss is 45.546798592808244, parameters k is 10.200339622758941 and b is -87.11939742455529\n",
      "Iteration 232, the loss is 45.141832298981406, parameters k is 10.26318596663246 and b is -87.10939742455528\n",
      "Iteration 233, the loss is 44.736866005154525, parameters k is 10.326032310505978 and b is -87.09939742455528\n",
      "Iteration 234, the loss is 44.33189971132766, parameters k is 10.388878654379496 and b is -87.08939742455527\n",
      "Iteration 235, the loss is 43.92693341750085, parameters k is 10.451724998253015 and b is -87.07939742455527\n",
      "Iteration 236, the loss is 43.52196712367401, parameters k is 10.514571342126533 and b is -87.06939742455526\n",
      "Iteration 237, the loss is 43.11700082984716, parameters k is 10.577417686000052 and b is -87.05939742455526\n",
      "Iteration 238, the loss is 42.71203453602027, parameters k is 10.64026402987357 and b is -87.04939742455525\n",
      "Iteration 239, the loss is 42.3070682421934, parameters k is 10.703110373747089 and b is -87.03939742455525\n",
      "Iteration 240, the loss is 41.90210194836659, parameters k is 10.765956717620607 and b is -87.02939742455524\n",
      "Iteration 241, the loss is 41.497135654539704, parameters k is 10.828803061494126 and b is -87.01939742455524\n",
      "Iteration 242, the loss is 41.092169360712845, parameters k is 10.891649405367644 and b is -87.00939742455523\n",
      "Iteration 243, the loss is 40.687203066886, parameters k is 10.954495749241163 and b is -86.99939742455523\n",
      "Iteration 244, the loss is 40.282236773059125, parameters k is 11.017342093114681 and b is -86.98939742455522\n",
      "Iteration 245, the loss is 39.87727047923231, parameters k is 11.0801884369882 and b is -86.97939742455522\n",
      "Iteration 246, the loss is 39.47230418540545, parameters k is 11.143034780861719 and b is -86.96939742455521\n",
      "Iteration 247, the loss is 39.06733789157863, parameters k is 11.205881124735237 and b is -86.9593974245552\n",
      "Iteration 248, the loss is 38.6623715977517, parameters k is 11.268727468608756 and b is -86.9493974245552\n",
      "Iteration 249, the loss is 38.25740530392488, parameters k is 11.331573812482274 and b is -86.9393974245552\n",
      "Iteration 250, the loss is 37.852439010097996, parameters k is 11.394420156355793 and b is -86.92939742455519\n",
      "Iteration 251, the loss is 37.44747271627123, parameters k is 11.457266500229311 and b is -86.91939742455519\n",
      "Iteration 252, the loss is 37.042506422444276, parameters k is 11.52011284410283 and b is -86.90939742455518\n",
      "Iteration 253, the loss is 36.63754012861745, parameters k is 11.582959187976348 and b is -86.89939742455518\n",
      "Iteration 254, the loss is 36.232573834790585, parameters k is 11.645805531849867 and b is -86.88939742455517\n",
      "Iteration 255, the loss is 35.82760754096372, parameters k is 11.708651875723385 and b is -86.87939742455517\n",
      "Iteration 256, the loss is 35.422641247136916, parameters k is 11.771498219596904 and b is -86.86939742455516\n",
      "Iteration 257, the loss is 35.01767495331004, parameters k is 11.834344563470422 and b is -86.85939742455515\n",
      "Iteration 258, the loss is 34.612708659483204, parameters k is 11.89719090734394 and b is -86.84939742455515\n",
      "Iteration 259, the loss is 34.20774236565635, parameters k is 11.960037251217459 and b is -86.83939742455514\n",
      "Iteration 260, the loss is 33.8027760718295, parameters k is 12.022883595090978 and b is -86.82939742455514\n",
      "Iteration 261, the loss is 33.39780977800259, parameters k is 12.085729938964496 and b is -86.81939742455513\n",
      "Iteration 262, the loss is 32.992843484175765, parameters k is 12.148576282838015 and b is -86.80939742455513\n",
      "Iteration 263, the loss is 32.587877190348905, parameters k is 12.211422626711533 and b is -86.79939742455512\n",
      "Iteration 264, the loss is 32.18291089652205, parameters k is 12.274268970585052 and b is -86.78939742455512\n",
      "Iteration 265, the loss is 31.777944602695218, parameters k is 12.33711531445857 and b is -86.77939742455511\n",
      "Iteration 266, the loss is 31.37377777896953, parameters k is 12.399961658332089 and b is -86.76939742455511\n",
      "Iteration 267, the loss is 30.973240320322095, parameters k is 12.462460966632484 and b is -86.75943695024681\n",
      "Iteration 268, the loss is 30.57270286167471, parameters k is 12.52496027493288 and b is -86.74947647593851\n",
      "Iteration 269, the loss is 30.172165403027265, parameters k is 12.587459583233276 and b is -86.73951600163021\n",
      "Iteration 270, the loss is 29.771627944379844, parameters k is 12.649958891533672 and b is -86.7295555273219\n",
      "Iteration 271, the loss is 29.371090485732445, parameters k is 12.712458199834067 and b is -86.7195950530136\n",
      "Iteration 272, the loss is 28.970553027085, parameters k is 12.774957508134463 and b is -86.7096345787053\n",
      "Iteration 273, the loss is 28.57001556843758, parameters k is 12.837456816434859 and b is -86.699674104397\n",
      "Iteration 274, the loss is 28.169478109790184, parameters k is 12.899956124735255 and b is -86.6897136300887\n",
      "Iteration 275, the loss is 27.768940651142756, parameters k is 12.96245543303565 and b is -86.6797531557804\n",
      "Iteration 276, the loss is 27.36840319249533, parameters k is 13.024954741336046 and b is -86.6697926814721\n",
      "Iteration 277, the loss is 26.967865733847926, parameters k is 13.087454049636442 and b is -86.6598322071638\n",
      "Iteration 278, the loss is 26.567328275200516, parameters k is 13.149953357936838 and b is -86.6498717328555\n",
      "Iteration 279, the loss is 26.166790816553064, parameters k is 13.212452666237233 and b is -86.6399112585472\n",
      "Iteration 280, the loss is 25.766253357905658, parameters k is 13.274951974537629 and b is -86.6299507842389\n",
      "Iteration 281, the loss is 25.365715899258245, parameters k is 13.337451282838025 and b is -86.6199903099306\n",
      "Iteration 282, the loss is 24.965178440610828, parameters k is 13.39995059113842 and b is -86.6100298356223\n",
      "Iteration 283, the loss is 24.564640981963397, parameters k is 13.462449899438816 and b is -86.600069361314\n",
      "Iteration 284, the loss is 24.16410352331597, parameters k is 13.524949207739212 and b is -86.5901088870057\n",
      "Iteration 285, the loss is 23.763566064668563, parameters k is 13.587448516039608 and b is -86.5801484126974\n",
      "Iteration 286, the loss is 23.363028606021107, parameters k is 13.649947824340003 and b is -86.5701879383891\n",
      "Iteration 287, the loss is 22.962491147373708, parameters k is 13.7124471326404 and b is -86.5602274640808\n",
      "Iteration 288, the loss is 22.561953688726266, parameters k is 13.774946440940795 and b is -86.5502669897725\n",
      "Iteration 289, the loss is 22.161416230078892, parameters k is 13.83744574924119 and b is -86.5403065154642\n",
      "Iteration 290, the loss is 21.762297876189308, parameters k is 13.899945057541586 and b is -86.5303460411559\n",
      "Iteration 291, the loss is 21.37170154994168, parameters k is 13.961887251217476 and b is -86.52046461823099\n",
      "Iteration 292, the loss is 20.985133060765133, parameters k is 14.023284306553444 and b is -86.51066224668949\n",
      "Iteration 293, the loss is 20.59856457158858, parameters k is 14.084681361889412 and b is -86.500859875148\n",
      "Iteration 294, the loss is 20.21314588571218, parameters k is 14.14607841722538 and b is -86.4910575036065\n",
      "Iteration 295, the loss is 19.830234399474893, parameters k is 14.207183259122614 and b is -86.48129465775669\n",
      "Iteration 296, the loss is 19.44732291323764, parameters k is 14.268288101019847 and b is -86.47153181190689\n",
      "Iteration 297, the loss is 19.06441142700034, parameters k is 14.32939294291708 and b is -86.46176896605708\n",
      "Iteration 298, the loss is 18.68149994076304, parameters k is 14.390497784814313 and b is -86.45200612020727\n",
      "Iteration 299, the loss is 18.298588454525742, parameters k is 14.451602626711546 and b is -86.44224327435747\n",
      "Iteration 300, the loss is 17.91567696828845, parameters k is 14.51270746860878 and b is -86.43248042850766\n",
      "Iteration 301, the loss is 17.533339358822445, parameters k is 14.573812310506012 and b is -86.42271758265785\n",
      "Iteration 302, the loss is 17.15360631943076, parameters k is 14.63466284410285 and b is -86.41299426249975\n",
      "Iteration 303, the loss is 16.774418920140857, parameters k is 14.695513377699688 and b is -86.40327094234165\n",
      "Iteration 304, the loss is 16.39936397588662, parameters k is 14.756095373747119 and b is -86.39358714787525\n",
      "Iteration 305, the loss is 16.030235086569753, parameters k is 14.816156579280715 and b is -86.38398240479225\n",
      "Iteration 306, the loss is 15.664448497950723, parameters k is 14.875951025921031 and b is -86.37441718740095\n",
      "Iteration 307, the loss is 15.30403347152598, parameters k is 14.935231401415102 and b is -86.36493102139305\n",
      "Iteration 308, the loss is 14.943618445101224, parameters k is 14.994511776909173 and b is -86.35544485538514\n",
      "Iteration 309, the loss is 14.584305472604724, parameters k is 15.053792152403243 and b is -86.34595868937724\n",
      "Iteration 310, the loss is 14.234270836440208, parameters k is 15.11229240932024 and b is -86.33659110044444\n",
      "Iteration 311, the loss is 13.88640949490132, parameters k is 15.170529859913126 and b is -86.32726303720332\n",
      "Iteration 312, the loss is 13.539515240653321, parameters k is 15.228767310506012 and b is -86.31793497396221\n",
      "Iteration 313, the loss is 13.195582556342517, parameters k is 15.286749464656209 and b is -86.3086464364128\n",
      "Iteration 314, the loss is 12.854368230587584, parameters k is 15.344468575328145 and b is -86.2993974245551\n",
      "Iteration 315, the loss is 12.52117704443205, parameters k is 15.401922824340003 and b is -86.29018793838908\n",
      "Iteration 316, the loss is 12.201043679018694, parameters k is 15.458044108924984 and b is -86.28117608068158\n",
      "Iteration 317, the loss is 11.889416813294522, parameters k is 15.513351282838027 and b is -86.27228280004917\n",
      "Iteration 318, the loss is 11.585494391511393, parameters k is 15.567839919201663 and b is -86.26350809649186\n",
      "Iteration 319, the loss is 11.285253275113261, parameters k is 15.622065433035656 and b is -86.25477291862624\n",
      "Iteration 320, the loss is 10.994279633627631, parameters k is 15.675693911296525 and b is -86.24611679214404\n",
      "Iteration 321, the loss is 10.718149490640863, parameters k is 15.727896065446723 and b is -86.23765829412032\n",
      "Iteration 322, the loss is 10.45047828609725, parameters k is 15.779045373747119 and b is -86.22935789886341\n",
      "Iteration 323, the loss is 10.190893140302842, parameters k is 15.829670215644352 and b is -86.2211365549899\n",
      "Iteration 324, the loss is 9.940403977070083, parameters k is 15.879236954774788 and b is -86.21307331388317\n",
      "Iteration 325, the loss is 9.699864326262478, parameters k is 15.927717725525776 and b is -86.20516817554325\n",
      "Iteration 326, the loss is 9.468923249791537, parameters k is 15.975383041731309 and b is -86.19738161427843\n",
      "Iteration 327, the loss is 9.244745876691074, parameters k is 16.022327686000082 and b is -86.1897136300887\n",
      "Iteration 328, the loss is 9.029399316751071, parameters k is 16.068436796672017 and b is -86.18216422297408\n",
      "Iteration 329, the loss is 8.824691031234726, parameters k is 16.113321994300478 and b is -86.17481244431795\n",
      "Iteration 330, the loss is 8.628967454794967, parameters k is 16.157325413272808 and b is -86.16757924273692\n",
      "Iteration 331, the loss is 8.441800178236475, parameters k is 16.200022429083084 and b is -86.16054366961438\n",
      "Iteration 332, the loss is 8.259996728721609, parameters k is 16.242465255170043 and b is -86.15354762218355\n",
      "Iteration 333, the loss is 8.093462178234361, parameters k is 16.283043812482294 and b is -86.14682825459462\n",
      "Iteration 334, the loss is 7.936867276308467, parameters k is 16.32251193501194 and b is -86.14026698977248\n",
      "Iteration 335, the loss is 7.790893687955357, parameters k is 16.36041883224514 and b is -86.13394287910054\n",
      "Iteration 336, the loss is 7.652663005409436, parameters k is 16.39729588758111 and b is -86.12777687119541\n",
      "Iteration 337, the loss is 7.522197202150798, parameters k is 16.43334812078269 and b is -86.12172944036537\n",
      "Iteration 338, the loss is 7.404280006589039, parameters k is 16.46744430655344 and b is -86.11595868937722\n",
      "Iteration 339, the loss is 7.291889044865113, parameters k is 16.500519227502057 and b is -86.11034604115588\n",
      "Iteration 340, the loss is 7.183069120575141, parameters k is 16.533120670189803 and b is -86.10481244431793\n",
      "Iteration 341, the loss is 7.078318509823083, parameters k is 16.565207053589013 and b is -86.09935789886339\n",
      "Iteration 342, the loss is 6.978466285426091, parameters k is 16.59656474133605 and b is -86.09402193048395\n",
      "Iteration 343, the loss is 6.8850345988896695, parameters k is 16.627124089162137 and b is -86.0888045391796\n",
      "Iteration 344, the loss is 6.799009389424339, parameters k is 16.656338852007988 and b is -86.08378477633376\n",
      "Iteration 345, the loss is 6.720612465242188, parameters k is 16.684159267027752 and b is -86.0789626419464\n",
      "Iteration 346, the loss is 6.649649523216654, parameters k is 16.710959919201667 and b is -86.07429861032585\n",
      "Iteration 347, the loss is 6.587158579500952, parameters k is 16.73600037374712 and b is -86.0699112585472\n",
      "Iteration 348, the loss is 6.53038414675221, parameters k is 16.759773318411153 and b is -86.06572153522704\n",
      "Iteration 349, the loss is 6.4805645355848736, parameters k is 16.782227211691787 and b is -86.06172944036538\n",
      "Iteration 350, the loss is 6.435984620794394, parameters k is 16.803161935011946 and b is -86.05797449965392\n",
      "Iteration 351, the loss is 6.395071138896986, parameters k is 16.82333701406333 and b is -86.05433813601756\n",
      "Iteration 352, the loss is 6.356683312355557, parameters k is 16.842676757146336 and b is -86.0508203494563\n",
      "Iteration 353, the loss is 6.3194911096189665, parameters k is 16.861732508134477 and b is -86.04734208858673\n",
      "Iteration 354, the loss is 6.28377556655506, parameters k is 16.8805354923242 and b is -86.04390335340887\n",
      "Iteration 355, the loss is 6.252566553021308, parameters k is 16.89823262671155 and b is -86.0406227209978\n",
      "Iteration 356, the loss is 6.224245904657695, parameters k is 16.914920551612735 and b is -86.03750019135353\n",
      "Iteration 357, the loss is 6.197990866463529, parameters k is 16.931082705762933 and b is -86.03445671309267\n",
      "Iteration 358, the loss is 6.17501242464296, parameters k is 16.946242567423013 and b is -86.03157133759859\n",
      "Iteration 359, the loss is 6.153704849772709, parameters k is 16.96082871366807 and b is -86.02876501348793\n",
      "Iteration 360, the loss is 6.134722004949077, parameters k is 16.97442418797637 and b is -86.02611679214405\n",
      "Iteration 361, the loss is 6.116997859764838, parameters k is 16.987494800624592 and b is -86.02354762218357\n",
      "Iteration 362, the loss is 6.100604763906406, parameters k is 17.000053871770838 and b is -86.0210575036065\n",
      "Iteration 363, the loss is 6.084505428844928, parameters k is 17.012612942917084 and b is -86.01856738502943\n",
      "Iteration 364, the loss is 6.069220178787876, parameters k is 17.02488719192894 and b is -86.01611679214405\n",
      "Iteration 365, the loss is 6.0554776434937905, parameters k is 17.03641539350997 and b is -86.01378477633376\n",
      "Iteration 366, the loss is 6.042223838103584, parameters k is 17.047697350031708 and b is -86.01149228621519\n",
      "Iteration 367, the loss is 6.029608216811656, parameters k is 17.058979306553447 and b is -86.00919979609661\n",
      "Iteration 368, the loss is 6.019364666790559, parameters k is 17.069210274932892 and b is -86.00706540874484\n",
      "Iteration 369, the loss is 6.010063181122888, parameters k is 17.078641678094947 and b is -86.00504959846816\n",
      "Iteration 370, the loss is 6.001009359320624, parameters k is 17.088073081257 and b is -86.00303378819149\n",
      "Iteration 371, the loss is 5.993623860646492, parameters k is 17.096529820387435 and b is -86.0011760806816\n",
      "Iteration 372, the loss is 5.987653465268119, parameters k is 17.104493357936843 and b is -85.99939742455513\n",
      "Iteration 373, the loss is 5.983237729848482, parameters k is 17.111170057541585 and b is -85.99781639688715\n",
      "Iteration 374, the loss is 5.979511230002799, parameters k is 17.11709679667202 and b is -85.99635394629426\n",
      "Iteration 375, the loss is 5.975784730157106, parameters k is 17.123023535802453 and b is -85.99489149570138\n",
      "Iteration 376, the loss is 5.972138141236082, parameters k is 17.128950274932887 and b is -85.9934290451085\n",
      "Iteration 377, the loss is 5.968899585463518, parameters k is 17.134566618806403 and b is -85.99200612020731\n",
      "Iteration 378, the loss is 5.965828059661484, parameters k is 17.139933318411146 and b is -85.99062272099782\n",
      "Iteration 379, the loss is 5.962756533859447, parameters k is 17.145300018015888 and b is -85.98923932178833\n",
      "Iteration 380, the loss is 5.959916685970108, parameters k is 17.15066671762063 and b is -85.98785592257885\n",
      "Iteration 381, the loss is 5.957759901667837, parameters k is 17.15529934607913 and b is -85.98659110044446\n",
      "Iteration 382, the loss is 5.9559254421348164, parameters k is 17.159618575328142 and b is -85.98536580400177\n",
      "Iteration 383, the loss is 5.9545355852877675, parameters k is 17.16317859509099 and b is -85.98425908463419\n",
      "Iteration 384, the loss is 5.953302407091533, parameters k is 17.166738614853838 and b is -85.9831523652666\n",
      "Iteration 385, the loss is 5.952352398847417, parameters k is 17.16968823935977 and b is -85.98212469728242\n",
      "Iteration 386, the loss is 5.951707494225234, parameters k is 17.172146440940796 and b is -85.98117608068162\n",
      "Iteration 387, the loss is 5.951154658542308, parameters k is 17.174351006158187 and b is -85.98026698977253\n",
      "Iteration 388, the loss is 5.950784286702059, parameters k is 17.176087310506013 and b is -85.97943695024684\n",
      "Iteration 389, the loss is 5.950454877928053, parameters k is 17.17782361485384 and b is -85.97860691072114\n",
      "Iteration 390, the loss is 5.950175045700976, parameters k is 17.179297863865695 and b is -85.97781639688715\n",
      "Iteration 391, the loss is 5.949895213473905, parameters k is 17.180772112877552 and b is -85.97702588305316\n",
      "Iteration 392, the loss is 5.949615381246844, parameters k is 17.18224636188941 and b is -85.97623536921917\n",
      "Iteration 393, the loss is 5.949366304380352, parameters k is 17.183720610901265 and b is -85.97544485538518\n",
      "Iteration 394, the loss is 5.949157764074725, parameters k is 17.18495406939929 and b is -85.97469386724289\n",
      "Iteration 395, the loss is 5.94895874023734, parameters k is 17.186187527897314 and b is -85.9739428791006\n",
      "Iteration 396, the loss is 5.948809755945095, parameters k is 17.187179326316286 and b is -85.97323141665001\n",
      "Iteration 397, the loss is 5.948660771652845, parameters k is 17.18817112473526 and b is -85.97251995419941\n",
      "Iteration 398, the loss is 5.948511787360602, parameters k is 17.189162923154232 and b is -85.97180849174882\n",
      "Iteration 399, the loss is 5.948362803068348, parameters k is 17.190154721573204 and b is -85.97109702929822\n",
      "Iteration 400, the loss is 5.948213818776107, parameters k is 17.191146519992177 and b is -85.97038556684763\n",
      "Iteration 401, the loss is 5.948064834483854, parameters k is 17.19213831841115 and b is -85.96967410439703\n",
      "Iteration 402, the loss is 5.947916076038992, parameters k is 17.193130116830122 and b is -85.96896264194643\n",
      "Iteration 403, the loss is 5.947816519479401, parameters k is 17.193867725525774 and b is -85.96829070518754\n",
      "Iteration 404, the loss is 5.947716962919817, parameters k is 17.194605334221425 and b is -85.96761876842865\n",
      "Iteration 405, the loss is 5.947617406360237, parameters k is 17.195342942917076 and b is -85.96694683166976\n",
      "Iteration 406, the loss is 5.94751784980064, parameters k is 17.196080551612727 and b is -85.96627489491087\n",
      "Iteration 407, the loss is 5.947457171047179, parameters k is 17.19681816030838 and b is -85.96560295815198\n",
      "Iteration 408, the loss is 5.947416319348161, parameters k is 17.19705691524909 and b is -85.96501007277648\n",
      "Iteration 409, the loss is 5.9473754676491435, parameters k is 17.1972956701898 and b is -85.96441718740098\n",
      "Iteration 410, the loss is 5.947334615950124, parameters k is 17.19753442513051 and b is -85.96382430202549\n",
      "Iteration 411, the loss is 5.947293764251105, parameters k is 17.19777318007122 and b is -85.96323141664999\n",
      "Iteration 412, the loss is 5.94725291255208, parameters k is 17.19801193501193 and b is -85.9626385312745\n",
      "Iteration 413, the loss is 5.947212060853072, parameters k is 17.19825068995264 and b is -85.962045645899\n",
      "Iteration 414, the loss is 5.9471776583482505, parameters k is 17.19848944489335 and b is -85.9614527605235\n",
      "Iteration 415, the loss is 5.947147035154283, parameters k is 17.19848444489335 and b is -85.96089940083971\n",
      "Iteration 416, the loss is 5.9471164119603275, parameters k is 17.198479444893348 and b is -85.96034604115592\n",
      "Iteration 417, the loss is 5.947085788766357, parameters k is 17.198474444893346 and b is -85.95979268147212\n",
      "Iteration 418, the loss is 5.9470551655723884, parameters k is 17.198469444893345 and b is -85.95923932178833\n",
      "Iteration 419, the loss is 5.947024542378429, parameters k is 17.198464444893343 and b is -85.95868596210454\n",
      "Iteration 420, the loss is 5.946993919184465, parameters k is 17.19845944489334 and b is -85.95813260242075\n",
      "Iteration 421, the loss is 5.946963295990502, parameters k is 17.19845444489334 and b is -85.95757924273695\n",
      "Iteration 422, the loss is 5.946932672796537, parameters k is 17.19844944489334 and b is -85.95702588305316\n",
      "Iteration 423, the loss is 5.946902049602579, parameters k is 17.198444444893337 and b is -85.95647252336937\n",
      "Iteration 424, the loss is 5.946871426408602, parameters k is 17.198439444893335 and b is -85.95591916368558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 425, the loss is 5.9468408032146405, parameters k is 17.198434444893334 and b is -85.95536580400179\n",
      "Iteration 426, the loss is 5.9468101800206705, parameters k is 17.198429444893332 and b is -85.954812444318\n",
      "Iteration 427, the loss is 5.9467795568267094, parameters k is 17.19842444489333 and b is -85.9542590846342\n",
      "Iteration 428, the loss is 5.9467489336327475, parameters k is 17.19841944489333 and b is -85.95370572495041\n",
      "Iteration 429, the loss is 5.94671831043878, parameters k is 17.198414444893327 and b is -85.95315236526662\n",
      "Iteration 430, the loss is 5.946687687244819, parameters k is 17.198409444893326 and b is -85.95259900558283\n",
      "Iteration 431, the loss is 5.946657064050847, parameters k is 17.198404444893324 and b is -85.95204564589903\n",
      "Iteration 432, the loss is 5.946626440856888, parameters k is 17.198399444893322 and b is -85.95149228621524\n",
      "Iteration 433, the loss is 5.946595817662923, parameters k is 17.19839444489332 and b is -85.95093892653145\n",
      "Iteration 434, the loss is 5.9465651944689615, parameters k is 17.19838944489332 and b is -85.95038556684766\n",
      "Iteration 435, the loss is 5.946534571274983, parameters k is 17.198384444893318 and b is -85.94983220716387\n",
      "Iteration 436, the loss is 5.946503948081035, parameters k is 17.198379444893316 and b is -85.94927884748007\n",
      "Iteration 437, the loss is 5.946473324887059, parameters k is 17.198374444893314 and b is -85.94872548779628\n",
      "Iteration 438, the loss is 5.946442701693091, parameters k is 17.198369444893313 and b is -85.94817212811249\n",
      "Iteration 439, the loss is 5.946412078499144, parameters k is 17.19836444489331 and b is -85.9476187684287\n",
      "Iteration 440, the loss is 5.9463814553051675, parameters k is 17.19835944489331 and b is -85.9470654087449\n",
      "Iteration 441, the loss is 5.9463508321112, parameters k is 17.198354444893308 and b is -85.94651204906111\n",
      "Iteration 442, the loss is 5.946320208917233, parameters k is 17.198349444893307 and b is -85.94595868937732\n",
      "Iteration 443, the loss is 5.946289585723275, parameters k is 17.198344444893305 and b is -85.94540532969353\n",
      "Iteration 444, the loss is 5.946258962529311, parameters k is 17.198339444893303 and b is -85.94485197000974\n",
      "Iteration 445, the loss is 5.946228339335345, parameters k is 17.1983344448933 and b is -85.94429861032594\n",
      "Iteration 446, the loss is 5.946197716141379, parameters k is 17.1983294448933 and b is -85.94374525064215\n",
      "Iteration 447, the loss is 5.946167092947415, parameters k is 17.1983244448933 and b is -85.94319189095836\n",
      "Iteration 448, the loss is 5.94613646975345, parameters k is 17.198319444893297 and b is -85.94263853127457\n",
      "Iteration 449, the loss is 5.946105846559483, parameters k is 17.198314444893295 and b is -85.94208517159078\n",
      "Iteration 450, the loss is 5.946075223365524, parameters k is 17.198309444893294 and b is -85.94153181190698\n",
      "Iteration 451, the loss is 5.946044600171551, parameters k is 17.198304444893292 and b is -85.94097845222319\n",
      "Iteration 452, the loss is 5.94601397697759, parameters k is 17.19829944489329 and b is -85.9404250925394\n",
      "Iteration 453, the loss is 5.945983353783625, parameters k is 17.19829444489329 and b is -85.93987173285561\n",
      "Iteration 454, the loss is 5.945952730589667, parameters k is 17.198289444893287 and b is -85.93931837317182\n",
      "Iteration 455, the loss is 5.945922107395697, parameters k is 17.198284444893286 and b is -85.93876501348802\n",
      "Iteration 456, the loss is 5.94589148420173, parameters k is 17.198279444893284 and b is -85.93821165380423\n",
      "Iteration 457, the loss is 5.945860861007773, parameters k is 17.198274444893283 and b is -85.93765829412044\n",
      "Iteration 458, the loss is 5.9458302378138015, parameters k is 17.19826944489328 and b is -85.93710493443665\n",
      "Iteration 459, the loss is 5.945799614619836, parameters k is 17.19826444489328 and b is -85.93655157475285\n",
      "Iteration 460, the loss is 5.945768991425873, parameters k is 17.198259444893278 and b is -85.93599821506906\n",
      "Iteration 461, the loss is 5.945738368231909, parameters k is 17.198254444893276 and b is -85.93544485538527\n",
      "Iteration 462, the loss is 5.945707745037941, parameters k is 17.198249444893275 and b is -85.93489149570148\n",
      "Iteration 463, the loss is 5.945677121843976, parameters k is 17.198244444893273 and b is -85.93433813601769\n",
      "Iteration 464, the loss is 5.945646498650009, parameters k is 17.19823944489327 and b is -85.9337847763339\n",
      "Iteration 465, the loss is 5.945615875456056, parameters k is 17.19823444489327 and b is -85.9332314166501\n",
      "Iteration 466, the loss is 5.94558525226208, parameters k is 17.19822944489327 and b is -85.93267805696631\n",
      "Iteration 467, the loss is 5.945554629068117, parameters k is 17.198224444893267 and b is -85.93212469728252\n",
      "Iteration 468, the loss is 5.945524005874158, parameters k is 17.198219444893265 and b is -85.93157133759873\n",
      "Iteration 469, the loss is 5.945493382680194, parameters k is 17.198214444893264 and b is -85.93101797791493\n",
      "Iteration 470, the loss is 5.945462759486227, parameters k is 17.198209444893262 and b is -85.93046461823114\n",
      "Iteration 471, the loss is 5.945432493102699, parameters k is 17.19820444489326 and b is -85.92991125854735\n",
      "Iteration 472, the loss is 5.945403581436965, parameters k is 17.19796173738338 and b is -85.92939742455525\n",
      "Iteration 473, the loss is 5.9453729582429995, parameters k is 17.19795673738338 and b is -85.92884406487146\n",
      "Iteration 474, the loss is 5.9453430901614315, parameters k is 17.197951737383377 and b is -85.92829070518766\n",
      "Iteration 475, the loss is 5.945313780193738, parameters k is 17.197709029873497 and b is -85.92777687119556\n",
      "Iteration 476, the loss is 5.945283156999779, parameters k is 17.197704029873496 and b is -85.92722351151177\n",
      "Iteration 477, the loss is 5.945253687220169, parameters k is 17.197699029873494 and b is -85.92667015182798\n",
      "Iteration 478, the loss is 5.9452239789505255, parameters k is 17.197456322363614 and b is -85.92615631783588\n",
      "Iteration 479, the loss is 5.9451933557565475, parameters k is 17.197451322363612 and b is -85.92560295815208\n",
      "Iteration 480, the loss is 5.945164284278903, parameters k is 17.19744632236361 and b is -85.92504959846829\n",
      "Iteration 481, the loss is 5.945134177707292, parameters k is 17.19720361485373 and b is -85.92453576447619\n",
      "Iteration 482, the loss is 5.94510355451333, parameters k is 17.19719861485373 and b is -85.9239824047924\n",
      "Iteration 483, the loss is 5.94507488133763, parameters k is 17.197193614853727 and b is -85.9234290451086\n",
      "Iteration 484, the loss is 5.9450443764640735, parameters k is 17.196950907343847 and b is -85.9229152111165\n",
      "Iteration 485, the loss is 5.945014033251667, parameters k is 17.196945907343846 and b is -85.92236185143271\n",
      "Iteration 486, the loss is 5.9449851984148125, parameters k is 17.196703199833966 and b is -85.92184801744061\n",
      "Iteration 487, the loss is 5.94495457522085, parameters k is 17.196698199833964 and b is -85.92129465775682\n",
      "Iteration 488, the loss is 5.944924630310397, parameters k is 17.196693199833962 and b is -85.92074129807303\n",
      "Iteration 489, the loss is 5.944895397171584, parameters k is 17.196450492324082 and b is -85.92022746408092\n",
      "Iteration 490, the loss is 5.94486477397762, parameters k is 17.19644549232408 and b is -85.91967410439713\n",
      "Iteration 491, the loss is 5.9448352273691265, parameters k is 17.19644049232408 and b is -85.91912074471334\n",
      "Iteration 492, the loss is 5.944805595928365, parameters k is 17.1961977848142 and b is -85.91860691072124\n",
      "Iteration 493, the loss is 5.944774972734398, parameters k is 17.196192784814198 and b is -85.91805355103745\n",
      "Iteration 494, the loss is 5.944745824427858, parameters k is 17.196187784814196 and b is -85.91750019135365\n",
      "Iteration 495, the loss is 5.944715794685137, parameters k is 17.195945077304316 and b is -85.91698635736155\n",
      "Iteration 496, the loss is 5.944685171491172, parameters k is 17.195940077304314 and b is -85.91643299767776\n",
      "Iteration 497, the loss is 5.944656421486594, parameters k is 17.195935077304313 and b is -85.91587963799397\n",
      "Iteration 498, the loss is 5.94462599344192, parameters k is 17.195692369794433 and b is -85.91536580400187\n",
      "Iteration 499, the loss is 5.94459557340062, parameters k is 17.19568736979443 and b is -85.91481244431807\n",
      "Iteration 500, the loss is 5.944566815392661, parameters k is 17.19544466228455 and b is -85.91429861032597\n",
      "Iteration 501, the loss is 5.944536192198694, parameters k is 17.19543966228455 and b is -85.91374525064218\n",
      "Iteration 502, the loss is 5.944506170459354, parameters k is 17.195434662284548 and b is -85.91319189095839\n",
      "Iteration 503, the loss is 5.944477014149441, parameters k is 17.195191954774668 and b is -85.91267805696629\n",
      "Iteration 504, the loss is 5.944446390955468, parameters k is 17.195186954774666 and b is -85.9121246972825\n",
      "Iteration 505, the loss is 5.9444167675180895, parameters k is 17.195181954774664 and b is -85.9115713375987\n",
      "Iteration 506, the loss is 5.944387212906215, parameters k is 17.194939247264784 and b is -85.9110575036066\n",
      "Iteration 507, the loss is 5.94435658971225, parameters k is 17.194934247264783 and b is -85.91050414392281\n",
      "Iteration 508, the loss is 5.944327364576822, parameters k is 17.19492924726478 and b is -85.90995078423902\n",
      "Iteration 509, the loss is 5.9442974116629905, parameters k is 17.1946865397549 and b is -85.90943695024691\n",
      "Iteration 510, the loss is 5.94426678846902, parameters k is 17.1946815397549 and b is -85.90888359056312\n",
      "Iteration 511, the loss is 5.9442379616355545, parameters k is 17.194676539754898 and b is -85.90833023087933\n",
      "Iteration 512, the loss is 5.94420761041976, parameters k is 17.194433832245018 and b is -85.90781639688723\n",
      "Iteration 513, the loss is 5.944177113549583, parameters k is 17.194428832245016 and b is -85.90726303720344\n",
      "Iteration 514, the loss is 5.944148432370512, parameters k is 17.194186124735136 and b is -85.90674920321133\n",
      "Iteration 515, the loss is 5.944117809176538, parameters k is 17.194181124735135 and b is -85.90619584352754\n",
      "Iteration 516, the loss is 5.944087710608316, parameters k is 17.194176124735133 and b is -85.90564248384375\n",
      "Iteration 517, the loss is 5.944058631127275, parameters k is 17.193933417225253 and b is -85.90512864985165\n",
      "Iteration 518, the loss is 5.944028007933319, parameters k is 17.19392841722525 and b is -85.90457529016786\n",
      "Iteration 519, the loss is 5.9439983076670515, parameters k is 17.19392341722525 and b is -85.90402193048406\n",
      "Iteration 520, the loss is 5.943968829884054, parameters k is 17.19368070971537 and b is -85.90350809649196\n",
      "Iteration 521, the loss is 5.943938206690098, parameters k is 17.193675709715368 and b is -85.90295473680817\n",
      "Iteration 522, the loss is 5.943908904725793, parameters k is 17.193670709715366 and b is -85.90240137712438\n",
      "Iteration 523, the loss is 5.943879028640829, parameters k is 17.193428002205486 and b is -85.90188754313228\n",
      "Iteration 524, the loss is 5.943848405446869, parameters k is 17.193423002205485 and b is -85.90133418344848\n",
      "Iteration 525, the loss is 5.943819501784525, parameters k is 17.193418002205483 and b is -85.90078082376469\n",
      "Iteration 526, the loss is 5.943789227397608, parameters k is 17.193175294695603 and b is -85.90026698977259\n",
      "Iteration 527, the loss is 5.943758653698555, parameters k is 17.1931702946956 and b is -85.8997136300888\n",
      "Iteration 528, the loss is 5.943730049348353, parameters k is 17.19292758718572 and b is -85.8991997960967\n",
      "Iteration 529, the loss is 5.943699426154393, parameters k is 17.19292258718572 and b is -85.8986464364129\n",
      "Iteration 530, the loss is 5.943669250757283, parameters k is 17.19291758718572 and b is -85.89809307672911\n",
      "Iteration 531, the loss is 5.943640248105126, parameters k is 17.192674879675838 and b is -85.89757924273701\n",
      "Iteration 532, the loss is 5.943609624911164, parameters k is 17.192669879675837 and b is -85.89702588305322\n",
      "Iteration 533, the loss is 5.943579847816018, parameters k is 17.192664879675835 and b is -85.89647252336943\n",
      "Iteration 534, the loss is 5.943550446861906, parameters k is 17.192422172165955 and b is -85.89595868937732\n",
      "Iteration 535, the loss is 5.943519823667942, parameters k is 17.192417172165953 and b is -85.89540532969353\n",
      "Iteration 536, the loss is 5.943490444874753, parameters k is 17.19241217216595 and b is -85.89485197000974\n",
      "Iteration 537, the loss is 5.94346064561868, parameters k is 17.19216946465607 and b is -85.89433813601764\n",
      "Iteration 538, the loss is 5.943430022424718, parameters k is 17.19216446465607 and b is -85.89378477633385\n",
      "Iteration 539, the loss is 5.943401041933483, parameters k is 17.19215946465607 and b is -85.89323141665005\n",
      "Iteration 540, the loss is 5.943370844375459, parameters k is 17.19191675714619 and b is -85.89271758265795\n",
      "Iteration 541, the loss is 5.9433402211814945, parameters k is 17.191911757146187 and b is -85.89216422297416\n",
      "Iteration 542, the loss is 5.943311638992222, parameters k is 17.191906757146185 and b is -85.89161086329037\n",
      "Iteration 543, the loss is 5.94328104313223, parameters k is 17.191664049636305 and b is -85.89109702929827\n",
      "Iteration 544, the loss is 5.943250790906244, parameters k is 17.191659049636304 and b is -85.89054366961447\n",
      "Iteration 545, the loss is 5.943221865082974, parameters k is 17.191416342126423 and b is -85.89002983562237\n",
      "Iteration 546, the loss is 5.943191241889004, parameters k is 17.191411342126422 and b is -85.88947647593858\n",
      "Iteration 547, the loss is 5.943161387964979, parameters k is 17.19140634212642 and b is -85.88892311625479\n",
      "Iteration 548, the loss is 5.943132063839751, parameters k is 17.19116363461654 and b is -85.88840928226269\n",
      "Iteration 549, the loss is 5.943101440645784, parameters k is 17.19115863461654 and b is -85.8878559225789\n",
      "Iteration 550, the loss is 5.943071985023711, parameters k is 17.191153634616537 and b is -85.8873025628951\n",
      "Iteration 551, the loss is 5.943042262596528, parameters k is 17.190910927106657 and b is -85.886788728903\n",
      "Iteration 552, the loss is 5.94301163940256, parameters k is 17.190905927106655 and b is -85.88623536921921\n",
      "Iteration 553, the loss is 5.942982582082446, parameters k is 17.190900927106654 and b is -85.88568200953542\n",
      "Iteration 554, the loss is 5.9429524613532925, parameters k is 17.190658219596774 and b is -85.88516817554331\n",
      "Iteration 555, the loss is 5.9429218381593385, parameters k is 17.190653219596772 and b is -85.88461481585952\n",
      "Iteration 556, the loss is 5.942893179141178, parameters k is 17.19064821959677 and b is -85.88406145617573\n",
      "Iteration 557, the loss is 5.942862660110081, parameters k is 17.19040551208689 and b is -85.88354762218363\n",
      "Iteration 558, the loss is 5.942832331055207, parameters k is 17.19040051208689 and b is -85.88299426249984\n",
      "Iteration 559, the loss is 5.942803482060817, parameters k is 17.19015780457701 and b is -85.88248042850773\n",
      "Iteration 560, the loss is 5.942772858866858, parameters k is 17.190152804577007 and b is -85.88192706882394\n",
      "Iteration 561, the loss is 5.942742928113939, parameters k is 17.190147804577006 and b is -85.88137370914015\n",
      "Iteration 562, the loss is 5.9427136808175955, parameters k is 17.189905097067125 and b is -85.88085987514805\n",
      "Iteration 563, the loss is 5.942683057623631, parameters k is 17.189900097067124 and b is -85.88030651546426\n",
      "Iteration 564, the loss is 5.942653525172675, parameters k is 17.189895097067122 and b is -85.87975315578046\n",
      "Iteration 565, the loss is 5.942623879574374, parameters k is 17.189652389557242 and b is -85.87923932178836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 566, the loss is 5.942593256380407, parameters k is 17.18964738955724 and b is -85.87868596210457\n",
      "Iteration 567, the loss is 5.942564122231414, parameters k is 17.18964238955724 and b is -85.87813260242078\n",
      "Iteration 568, the loss is 5.9425340783311515, parameters k is 17.18939968204736 and b is -85.87761876842868\n",
      "Iteration 569, the loss is 5.942503455137187, parameters k is 17.189394682047357 and b is -85.87706540874488\n",
      "Iteration 570, the loss is 5.942474719290141, parameters k is 17.189389682047356 and b is -85.87651204906109\n",
      "Iteration 571, the loss is 5.942444277087937, parameters k is 17.189146974537476 and b is -85.87599821506899\n",
      "Iteration 572, the loss is 5.942413871204169, parameters k is 17.189141974537474 and b is -85.8754448553852\n",
      "Iteration 573, the loss is 5.942385099038672, parameters k is 17.188899267027594 and b is -85.8749310213931\n",
      "Iteration 574, the loss is 5.9423544758447004, parameters k is 17.188894267027592 and b is -85.8743776617093\n",
      "Iteration 575, the loss is 5.942324468262903, parameters k is 17.18888926702759 and b is -85.87382430202551\n",
      "Iteration 576, the loss is 5.942295297795444, parameters k is 17.18864655951771 and b is -85.87331046803341\n",
      "Iteration 577, the loss is 5.94226467460148, parameters k is 17.18864155951771 and b is -85.87275710834962\n",
      "Iteration 578, the loss is 5.942235065321634, parameters k is 17.188636559517708 and b is -85.87220374866583\n",
      "Iteration 579, the loss is 5.942205496552223, parameters k is 17.188393852007827 and b is -85.87168991467372\n",
      "Iteration 580, the loss is 5.942174873358254, parameters k is 17.188388852007826 and b is -85.87113655498993\n",
      "Iteration 581, the loss is 5.942145662380372, parameters k is 17.188383852007824 and b is -85.87058319530614\n",
      "Iteration 582, the loss is 5.942115695308997, parameters k is 17.188141144497944 and b is -85.87006936131404\n",
      "Iteration 583, the loss is 5.942085072115021, parameters k is 17.188136144497943 and b is -85.86951600163025\n",
      "Iteration 584, the loss is 5.942056259439106, parameters k is 17.18813114449794 and b is -85.86896264194645\n",
      "Iteration 585, the loss is 5.942025894065767, parameters k is 17.18788843698806 and b is -85.86844880795435\n",
      "Iteration 586, the loss is 5.941995411353129, parameters k is 17.18788343698806 and b is -85.86789544827056\n",
      "Iteration 587, the loss is 5.941966716016507, parameters k is 17.18764072947818 and b is -85.86738161427846\n",
      "Iteration 588, the loss is 5.941936092822546, parameters k is 17.187635729478178 and b is -85.86682825459467\n",
      "Iteration 589, the loss is 5.941906008411867, parameters k is 17.187630729478176 and b is -85.86627489491087\n",
      "Iteration 590, the loss is 5.941876914773289, parameters k is 17.187388021968296 and b is -85.86576106091877\n",
      "Iteration 591, the loss is 5.941846291579324, parameters k is 17.187383021968294 and b is -85.86520770123498\n",
      "Iteration 592, the loss is 5.9418166054706045, parameters k is 17.187378021968293 and b is -85.86465434155119\n",
      "Iteration 593, the loss is 5.941787113530068, parameters k is 17.187135314458413 and b is -85.86414050755909\n",
      "Iteration 594, the loss is 5.941756490336101, parameters k is 17.18713031445841 and b is -85.8635871478753\n",
      "Iteration 595, the loss is 5.941727202529331, parameters k is 17.18712531445841 and b is -85.8630337881915\n",
      "Iteration 596, the loss is 5.941697312286842, parameters k is 17.18688260694853 and b is -85.8625199541994\n",
      "Iteration 597, the loss is 5.94166668909287, parameters k is 17.186877606948528 and b is -85.86196659451561\n",
      "Iteration 598, the loss is 5.941637799588069, parameters k is 17.186872606948526 and b is -85.86141323483182\n",
      "Iteration 599, the loss is 5.94160751104362, parameters k is 17.186629899438646 and b is -85.86089940083971\n",
      "Iteration 600, the loss is 5.941576951502101, parameters k is 17.186624899438645 and b is -85.86034604115592\n",
      "Iteration 601, the loss is 5.941548332994359, parameters k is 17.186382191928764 and b is -85.85983220716382\n",
      "Iteration 602, the loss is 5.941517709800391, parameters k is 17.186377191928763 and b is -85.85927884748003\n",
      "Iteration 603, the loss is 5.941487548560822, parameters k is 17.18637219192876 and b is -85.85872548779624\n",
      "Iteration 604, the loss is 5.941458531751129, parameters k is 17.18612948441888 and b is -85.85821165380413\n",
      "Iteration 605, the loss is 5.941427908557169, parameters k is 17.18612448441888 and b is -85.85765829412034\n",
      "Iteration 606, the loss is 5.941398145619569, parameters k is 17.186119484418878 and b is -85.85710493443655\n",
      "Iteration 607, the loss is 5.94136873050791, parameters k is 17.185876776908998 and b is -85.85659110044445\n",
      "Iteration 608, the loss is 5.941338107313944, parameters k is 17.185871776908996 and b is -85.85603774076066\n",
      "Iteration 609, the loss is 5.941308742678299, parameters k is 17.185866776908995 and b is -85.85548438107686\n",
      "Iteration 610, the loss is 5.941278929264686, parameters k is 17.185624069399115 and b is -85.85497054708476\n",
      "Iteration 611, the loss is 5.941248306070723, parameters k is 17.185619069399113 and b is -85.85441718740097\n",
      "Iteration 612, the loss is 5.941219339737034, parameters k is 17.18561406939911 and b is -85.85386382771718\n",
      "Iteration 613, the loss is 5.941189128021465, parameters k is 17.18537136188923 and b is -85.85334999372508\n",
      "Iteration 614, the loss is 5.941158504827495, parameters k is 17.18536636188923 and b is -85.85279663404128\n",
      "Iteration 615, the loss is 5.941129936795761, parameters k is 17.18536136188923 and b is -85.85224327435749\n",
      "Iteration 616, the loss is 5.941099326778239, parameters k is 17.185118654379348 and b is -85.85172944036539\n",
      "Iteration 617, the loss is 5.9410690887097894, parameters k is 17.185113654379347 and b is -85.8511760806816\n",
      "Iteration 618, the loss is 5.941040148728983, parameters k is 17.184870946869466 and b is -85.8506622466895\n",
      "Iteration 619, the loss is 5.941009525535011, parameters k is 17.184865946869465 and b is -85.8501088870057\n",
      "Iteration 620, the loss is 5.940979685768525, parameters k is 17.184860946869463 and b is -85.84955552732191\n",
      "Iteration 621, the loss is 5.940950347485756, parameters k is 17.184618239359583 and b is -85.84904169332981\n",
      "Iteration 622, the loss is 5.940919724291788, parameters k is 17.18461323935958 and b is -85.84848833364602\n",
      "Iteration 623, the loss is 5.940890282827256, parameters k is 17.18460823935958 and b is -85.84793497396223\n",
      "Iteration 624, the loss is 5.940860546242533, parameters k is 17.1843655318497 and b is -85.84742113997012\n",
      "Iteration 625, the loss is 5.940829923048569, parameters k is 17.1843605318497 and b is -85.84686778028633\n",
      "Iteration 626, the loss is 5.940800879885995, parameters k is 17.184355531849697 and b is -85.84631442060254\n",
      "Iteration 627, the loss is 5.9407707449993135, parameters k is 17.184112824339817 and b is -85.84580058661044\n",
      "Iteration 628, the loss is 5.940740121805345, parameters k is 17.184107824339815 and b is -85.84524722692665\n",
      "Iteration 629, the loss is 5.940711476944725, parameters k is 17.184102824339814 and b is -85.84469386724285\n",
      "Iteration 630, the loss is 5.940680943756085, parameters k is 17.183860116829933 and b is -85.84418003325075\n",
      "Iteration 631, the loss is 5.940650628858757, parameters k is 17.183855116829932 and b is -85.84362667356696\n",
      "Iteration 632, the loss is 5.94062176570682, parameters k is 17.18361240932005 and b is -85.84311283957486\n",
      "Iteration 633, the loss is 5.940591142512858, parameters k is 17.18360740932005 and b is -85.84255947989107\n",
      "Iteration 634, the loss is 5.94056122591749, parameters k is 17.18360240932005 and b is -85.84200612020727\n",
      "Iteration 635, the loss is 5.940531964463603, parameters k is 17.18335970181017 and b is -85.84149228621517\n",
      "Iteration 636, the loss is 5.940501341269639, parameters k is 17.183354701810167 and b is -85.84093892653138\n",
      "Iteration 637, the loss is 5.940471822976214, parameters k is 17.183349701810165 and b is -85.84038556684759\n",
      "Iteration 638, the loss is 5.940442163220375, parameters k is 17.183106994300285 and b is -85.83987173285549\n",
      "Iteration 639, the loss is 5.940411540026421, parameters k is 17.183101994300284 and b is -85.8393183731717\n",
      "Iteration 640, the loss is 5.940382420034955, parameters k is 17.183096994300282 and b is -85.8387650134879\n",
      "Iteration 641, the loss is 5.940352361977153, parameters k is 17.182854286790402 and b is -85.8382511794958\n",
      "Iteration 642, the loss is 5.940321738783191, parameters k is 17.1828492867904 and b is -85.837697819812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 643, the loss is 5.940293017093686, parameters k is 17.1828442867904 and b is -85.83714446012821\n",
      "Iteration 644, the loss is 5.9402625607339345, parameters k is 17.18260157928052 and b is -85.83663062613611\n",
      "Iteration 645, the loss is 5.940232169007715, parameters k is 17.182596579280517 and b is -85.83607726645232\n",
      "Iteration 646, the loss is 5.940203382684666, parameters k is 17.182353871770637 and b is -85.83556343246022\n",
      "Iteration 647, the loss is 5.940172759490705, parameters k is 17.182348871770635 and b is -85.83501007277643\n",
      "Iteration 648, the loss is 5.940142766066455, parameters k is 17.182343871770634 and b is -85.83445671309264\n",
      "Iteration 649, the loss is 5.940113581441448, parameters k is 17.182101164260754 and b is -85.83394287910053\n",
      "Iteration 650, the loss is 5.940082958247485, parameters k is 17.182096164260752 and b is -85.83338951941674\n",
      "Iteration 651, the loss is 5.940053363125182, parameters k is 17.18209116426075 and b is -85.83283615973295\n",
      "Iteration 652, the loss is 5.940023780198227, parameters k is 17.18184845675087 and b is -85.83232232574085\n",
      "Iteration 653, the loss is 5.939993157004258, parameters k is 17.18184345675087 and b is -85.83176896605706\n",
      "Iteration 654, the loss is 5.93996396018392, parameters k is 17.181838456750867 and b is -85.83121560637326\n",
      "Iteration 655, the loss is 5.939933978955002, parameters k is 17.181595749240987 and b is -85.83070177238116\n",
      "Iteration 656, the loss is 5.939903355761028, parameters k is 17.181590749240986 and b is -85.83014841269737\n",
      "Iteration 657, the loss is 5.939874557242655, parameters k is 17.181585749240984 and b is -85.82959505301358\n",
      "Iteration 658, the loss is 5.93984417771177, parameters k is 17.181343041731104 and b is -85.82908121902148\n",
      "Iteration 659, the loss is 5.939813709156677, parameters k is 17.181338041731102 and b is -85.82852785933768\n",
      "Iteration 660, the loss is 5.939784999662521, parameters k is 17.181095334221222 and b is -85.82801402534558\n",
      "Iteration 661, the loss is 5.939754376468553, parameters k is 17.18109033422122 and b is -85.82746066566179\n",
      "Iteration 662, the loss is 5.939724306215414, parameters k is 17.18108533422122 and b is -85.826907305978\n",
      "Iteration 663, the loss is 5.939695198419295, parameters k is 17.18084262671134 and b is -85.8263934719859\n",
      "Iteration 664, the loss is 5.939664575225331, parameters k is 17.180837626711337 and b is -85.8258401123021\n",
      "Iteration 665, the loss is 5.939634903274148, parameters k is 17.180832626711336 and b is -85.82528675261831\n",
      "Iteration 666, the loss is 5.9396053971760665, parameters k is 17.180589919201456 and b is -85.82477291862621\n",
      "Iteration 667, the loss is 5.939574773982103, parameters k is 17.180584919201454 and b is -85.82421955894242\n",
      "Iteration 668, the loss is 5.9395455003328745, parameters k is 17.180579919201453 and b is -85.82366619925862\n",
      "Iteration 669, the loss is 5.939515595932849, parameters k is 17.180337211691572 and b is -85.82315236526652\n",
      "Iteration 670, the loss is 5.939484972738882, parameters k is 17.18033221169157 and b is -85.82259900558273\n",
      "Iteration 671, the loss is 5.939456097391609, parameters k is 17.18032721169157 and b is -85.82204564589894\n",
      "Iteration 672, the loss is 5.93942579468962, parameters k is 17.18008450418169 and b is -85.82153181190684\n",
      "Iteration 673, the loss is 5.939395249305643, parameters k is 17.180079504181688 and b is -85.82097845222304\n",
      "Iteration 674, the loss is 5.939366616640367, parameters k is 17.179836796671808 and b is -85.82046461823094\n",
      "Iteration 675, the loss is 5.939335993446399, parameters k is 17.179831796671806 and b is -85.81991125854715\n",
      "Iteration 676, the loss is 5.939305846364374, parameters k is 17.179826796671804 and b is -85.81935789886336\n",
      "Iteration 677, the loss is 5.939276815397136, parameters k is 17.179584089161924 and b is -85.81884406487126\n",
      "Iteration 678, the loss is 5.939246192203176, parameters k is 17.179579089161923 and b is -85.81829070518747\n",
      "Iteration 679, the loss is 5.939216443423118, parameters k is 17.17957408916192 and b is -85.81773734550367\n",
      "Iteration 680, the loss is 5.939187014153919, parameters k is 17.17933138165204 and b is -85.81722351151157\n",
      "Iteration 681, the loss is 5.93915639095995, parameters k is 17.17932638165204 and b is -85.81667015182778\n",
      "Iteration 682, the loss is 5.939127040481839, parameters k is 17.179321381652038 and b is -85.81611679214399\n",
      "Iteration 683, the loss is 5.939097212910697, parameters k is 17.179078674142158 and b is -85.81560295815189\n",
      "Iteration 684, the loss is 5.9390665897167345, parameters k is 17.179073674142156 and b is -85.8150495984681\n",
      "Iteration 685, the loss is 5.939037637540575, parameters k is 17.179068674142155 and b is -85.8144962387843\n",
      "Iteration 686, the loss is 5.9390074116674745, parameters k is 17.178825966632274 and b is -85.8139824047922\n",
      "Iteration 687, the loss is 5.9389767894546, parameters k is 17.178820966632273 and b is -85.8134290451084\n",
      "Iteration 688, the loss is 5.9389482336182065, parameters k is 17.178578259122393 and b is -85.8129152111163\n",
      "Iteration 689, the loss is 5.938917610424243, parameters k is 17.17857325912239 and b is -85.81236185143251\n",
      "Iteration 690, the loss is 5.9388873865133345, parameters k is 17.17856825912239 and b is -85.81180849174872\n",
      "Iteration 691, the loss is 5.938858432374984, parameters k is 17.17832555161251 and b is -85.81129465775662\n",
      "Iteration 692, the loss is 5.938827809181023, parameters k is 17.178320551612508 and b is -85.81074129807283\n",
      "Iteration 693, the loss is 5.938797983572075, parameters k is 17.178315551612506 and b is -85.81018793838903\n",
      "Iteration 694, the loss is 5.938768631131764, parameters k is 17.178072844102626 and b is -85.80967410439693\n",
      "Iteration 695, the loss is 5.938738007937801, parameters k is 17.178067844102625 and b is -85.80912074471314\n",
      "Iteration 696, the loss is 5.938708580630799, parameters k is 17.178062844102623 and b is -85.80856738502935\n",
      "Iteration 697, the loss is 5.938678829888542, parameters k is 17.177820136592743 and b is -85.80805355103725\n",
      "Iteration 698, the loss is 5.938648206694579, parameters k is 17.17781513659274 and b is -85.80750019135345\n",
      "Iteration 699, the loss is 5.93861917768954, parameters k is 17.17781013659274 and b is -85.80694683166966\n",
      "Iteration 700, the loss is 5.93858902864532, parameters k is 17.17756742908286 and b is -85.80643299767756\n",
      "Iteration 701, the loss is 5.93855840545136, parameters k is 17.177562429082858 and b is -85.80587963799377\n",
      "Iteration 702, the loss is 5.938529774748275, parameters k is 17.177557429082857 and b is -85.80532627830998\n",
      "Iteration 703, the loss is 5.938499227402092, parameters k is 17.177314721572976 and b is -85.80481244431788\n",
      "Iteration 704, the loss is 5.938468926662303, parameters k is 17.177309721572975 and b is -85.80425908463408\n",
      "Iteration 705, the loss is 5.938440049352832, parameters k is 17.177067014063095 and b is -85.80374525064198\n",
      "Iteration 706, the loss is 5.938409426158869, parameters k is 17.177062014063093 and b is -85.80319189095819\n",
      "Iteration 707, the loss is 5.938379523721036, parameters k is 17.17705701406309 and b is -85.8026385312744\n",
      "Iteration 708, the loss is 5.938350248109611, parameters k is 17.17681430655321 and b is -85.8021246972823\n",
      "Iteration 709, the loss is 5.938319624915643, parameters k is 17.17680930655321 and b is -85.8015713375985\n",
      "Iteration 710, the loss is 5.938290120779769, parameters k is 17.17680430655321 and b is -85.80101797791471\n",
      "Iteration 711, the loss is 5.938260446866384, parameters k is 17.17656159904333 and b is -85.80050414392261\n",
      "Iteration 712, the loss is 5.938229823672421, parameters k is 17.176556599043327 and b is -85.79995078423882\n",
      "Iteration 713, the loss is 5.938200717838504, parameters k is 17.176551599043325 and b is -85.79939742455502\n",
      "Iteration 714, the loss is 5.938170645623164, parameters k is 17.176308891533445 and b is -85.79888359056292\n",
      "Iteration 715, the loss is 5.938140022429198, parameters k is 17.176303891533443 and b is -85.79833023087913\n",
      "Iteration 716, the loss is 5.938111314897239, parameters k is 17.176298891533442 and b is -85.79777687119534\n",
      "Iteration 717, the loss is 5.938080844379942, parameters k is 17.17605618402356 and b is -85.79726303720324\n",
      "Iteration 718, the loss is 5.93805046681126, parameters k is 17.17605118402356 and b is -85.79670967751944\n",
      "Iteration 719, the loss is 5.938021666330683, parameters k is 17.17580847651368 and b is -85.79619584352734\n",
      "Iteration 720, the loss is 5.937991043136715, parameters k is 17.17580347651368 and b is -85.79564248384355\n",
      "Iteration 721, the loss is 5.937961063869999, parameters k is 17.175798476513677 and b is -85.79508912415976\n",
      "Iteration 722, the loss is 5.937931865087462, parameters k is 17.175555769003797 and b is -85.79457529016766\n",
      "Iteration 723, the loss is 5.937901241893491, parameters k is 17.175550769003795 and b is -85.79402193048386\n",
      "Iteration 724, the loss is 5.937871660928732, parameters k is 17.175545769003794 and b is -85.79346857080007\n",
      "Iteration 725, the loss is 5.93784206384423, parameters k is 17.175303061493914 and b is -85.79295473680797\n",
      "Iteration 726, the loss is 5.937811440650266, parameters k is 17.175298061493912 and b is -85.79240137712418\n",
      "Iteration 727, the loss is 5.9377822579874575, parameters k is 17.17529306149391 and b is -85.79184801744039\n",
      "Iteration 728, the loss is 5.9377522626010135, parameters k is 17.17505035398403 and b is -85.79133418344828\n",
      "Iteration 729, the loss is 5.937721639407046, parameters k is 17.17504535398403 and b is -85.79078082376449\n",
      "Iteration 730, the loss is 5.937692855046193, parameters k is 17.175040353984027 and b is -85.7902274640807\n",
      "Iteration 731, the loss is 5.9376624613577835, parameters k is 17.174797646474147 and b is -85.7897136300886\n",
      "Iteration 732, the loss is 5.937632006960226, parameters k is 17.174792646474145 and b is -85.7891602704048\n",
      "Iteration 733, the loss is 5.937603283308529, parameters k is 17.174549938964265 and b is -85.7886464364127\n",
      "Iteration 734, the loss is 5.937572660114566, parameters k is 17.174544938964264 and b is -85.78809307672891\n",
      "Iteration 735, the loss is 5.937542604018957, parameters k is 17.174539938964262 and b is -85.78753971704512\n",
      "Iteration 736, the loss is 5.937513482065298, parameters k is 17.174297231454382 and b is -85.78702588305302\n",
      "Iteration 737, the loss is 5.937482858871343, parameters k is 17.17429223145438 and b is -85.78647252336923\n",
      "Iteration 738, the loss is 5.937453201077697, parameters k is 17.17428723145438 and b is -85.78591916368543\n",
      "Iteration 739, the loss is 5.9374236808220715, parameters k is 17.1740445239445 and b is -85.78540532969333\n",
      "Iteration 740, the loss is 5.937393057628109, parameters k is 17.174039523944497 and b is -85.78485197000954\n",
      "Iteration 741, the loss is 5.937363798136421, parameters k is 17.174034523944496 and b is -85.78429861032575\n",
      "Iteration 742, the loss is 5.937333879578853, parameters k is 17.173791816434615 and b is -85.78378477633365\n",
      "Iteration 743, the loss is 5.937303256384884, parameters k is 17.173786816434614 and b is -85.78323141664985\n",
      "Iteration 744, the loss is 5.937274395195161, parameters k is 17.173781816434612 and b is -85.78267805696606\n",
      "Iteration 745, the loss is 5.937244078335634, parameters k is 17.173539108924732 and b is -85.78216422297396\n",
      "Iteration 746, the loss is 5.937213547109185, parameters k is 17.17353410892473 and b is -85.78161086329017\n",
      "Iteration 747, the loss is 5.93718490028637, parameters k is 17.17329140141485 and b is -85.78109702929807\n",
      "Iteration 748, the loss is 5.9371542770924055, parameters k is 17.17328640141485 and b is -85.78054366961427\n",
      "Iteration 749, the loss is 5.937124144167925, parameters k is 17.173281401414847 and b is -85.77999030993048\n",
      "Iteration 750, the loss is 5.937095099043151, parameters k is 17.173038693904967 and b is -85.77947647593838\n",
      "Iteration 751, the loss is 5.937064475849185, parameters k is 17.173033693904966 and b is -85.77892311625459\n",
      "Iteration 752, the loss is 5.9370347412266495, parameters k is 17.173028693904964 and b is -85.7783697565708\n",
      "Iteration 753, the loss is 5.9370052977999235, parameters k is 17.172785986395084 and b is -85.7778559225787\n",
      "Iteration 754, the loss is 5.9369746746059615, parameters k is 17.172780986395082 and b is -85.7773025628949\n",
      "Iteration 755, the loss is 5.936945338285384, parameters k is 17.17277598639508 and b is -85.77674920321111\n",
      "Iteration 756, the loss is 5.936915496556698, parameters k is 17.1725332788852 and b is -85.77623536921901\n",
      "Iteration 757, the loss is 5.936884873362738, parameters k is 17.1725282788852 and b is -85.77568200953522\n",
      "Iteration 758, the loss is 5.936855935344128, parameters k is 17.172523278885198 and b is -85.77512864985142\n",
      "Iteration 759, the loss is 5.93682569531348, parameters k is 17.172280571375317 and b is -85.77461481585932\n",
      "Iteration 760, the loss is 5.936795087258151, parameters k is 17.172275571375316 and b is -85.77406145617553\n",
      "Iteration 761, the loss is 5.936766517264214, parameters k is 17.172032863865436 and b is -85.77354762218343\n",
      "Iteration 762, the loss is 5.936735894070247, parameters k is 17.172027863865434 and b is -85.77299426249964\n",
      "Iteration 763, the loss is 5.936705684316888, parameters k is 17.172022863865433 and b is -85.77244090281584\n",
      "Iteration 764, the loss is 5.936676716020997, parameters k is 17.171780156355553 and b is -85.77192706882374\n",
      "Iteration 765, the loss is 5.936646092827031, parameters k is 17.17177515635555 and b is -85.77137370913995\n",
      "Iteration 766, the loss is 5.936616281375618, parameters k is 17.17177015635555 and b is -85.77082034945616\n",
      "Iteration 767, the loss is 5.936586914777776, parameters k is 17.17152744884567 and b is -85.77030651546406\n",
      "Iteration 768, the loss is 5.936556291583804, parameters k is 17.171522448845668 and b is -85.76975315578026\n",
      "Iteration 769, the loss is 5.936526878434352, parameters k is 17.171517448845666 and b is -85.76919979609647\n",
      "Iteration 770, the loss is 5.936497113534544, parameters k is 17.171274741335786 and b is -85.76868596210437\n",
      "Iteration 771, the loss is 5.936466490340584, parameters k is 17.171269741335784 and b is -85.76813260242058\n",
      "Iteration 772, the loss is 5.9364374754930855, parameters k is 17.171264741335783 and b is -85.76757924273679\n",
      "Iteration 773, the loss is 5.936407312291323, parameters k is 17.171022033825903 and b is -85.76706540874468\n",
      "Iteration 774, the loss is 5.936376689097355, parameters k is 17.1710170338259 and b is -85.76651204906089\n",
      "Iteration 775, the loss is 5.936348072551817, parameters k is 17.1710120338259 and b is -85.7659586893771\n",
      "Iteration 776, the loss is 5.936317511048101, parameters k is 17.17076932631602 and b is -85.765444855385\n",
      "Iteration 777, the loss is 5.936287224465846, parameters k is 17.170764326316018 and b is -85.7648914957012\n",
      "Iteration 778, the loss is 5.936258332998841, parameters k is 17.170521618806138 and b is -85.7643776617091\n",
      "Iteration 779, the loss is 5.936227709804871, parameters k is 17.170516618806136 and b is -85.76382430202531\n",
      "Iteration 780, the loss is 5.936197821524581, parameters k is 17.170511618806135 and b is -85.76327094234152\n",
      "Iteration 781, the loss is 5.9361685317556185, parameters k is 17.170268911296255 and b is -85.76275710834942\n",
      "Iteration 782, the loss is 5.936137908561657, parameters k is 17.170263911296253 and b is -85.76220374866563\n",
      "Iteration 783, the loss is 5.936108418583321, parameters k is 17.17025891129625 and b is -85.76165038898183\n",
      "Iteration 784, the loss is 5.936078730512388, parameters k is 17.17001620378637 and b is -85.76113655498973\n",
      "Iteration 785, the loss is 5.936048107318427, parameters k is 17.17001120378637 and b is -85.76058319530594\n",
      "Iteration 786, the loss is 5.936019015642054, parameters k is 17.170006203786368 and b is -85.76002983562215\n",
      "Iteration 787, the loss is 5.935988929269172, parameters k is 17.169763496276488 and b is -85.75951600163005\n",
      "Iteration 788, the loss is 5.935958306075203, parameters k is 17.169758496276486 and b is -85.75896264194625\n",
      "Iteration 789, the loss is 5.935929612700788, parameters k is 17.169753496276485 and b is -85.75840928226246\n",
      "Iteration 790, the loss is 5.935899128025949, parameters k is 17.169510788766605 and b is -85.75789544827036\n",
      "Iteration 791, the loss is 5.935868764614803, parameters k is 17.169505788766603 and b is -85.75734208858657\n",
      "Iteration 792, the loss is 5.935839949976685, parameters k is 17.169263081256723 and b is -85.75682825459447\n",
      "Iteration 793, the loss is 5.93580932678272, parameters k is 17.16925808125672 and b is -85.75627489491067\n",
      "Iteration 794, the loss is 5.935779361673547, parameters k is 17.16925308125672 and b is -85.75572153522688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 795, the loss is 5.935750148733461, parameters k is 17.16901037374684 and b is -85.75520770123478\n",
      "Iteration 796, the loss is 5.9357195255395006, parameters k is 17.169005373746838 and b is -85.75465434155099\n",
      "Iteration 797, the loss is 5.935689958732275, parameters k is 17.169000373746837 and b is -85.7541009818672\n",
      "Iteration 798, the loss is 5.935660347490236, parameters k is 17.168757666236957 and b is -85.7535871478751\n",
      "Iteration 799, the loss is 5.93562972429627, parameters k is 17.168752666236955 and b is -85.7530337881913\n",
      "Iteration 800, the loss is 5.935600555791012, parameters k is 17.168747666236953 and b is -85.75248042850751\n",
      "Iteration 801, the loss is 5.9355705462470105, parameters k is 17.168504958727073 and b is -85.75196659451541\n",
      "Iteration 802, the loss is 5.935539923053047, parameters k is 17.16849995872707 and b is -85.75141323483162\n",
      "Iteration 803, the loss is 5.935511152849744, parameters k is 17.16849495872707 and b is -85.75085987514782\n",
      "Iteration 804, the loss is 5.935480745003788, parameters k is 17.16825225121719 and b is -85.75034604115572\n",
      "Iteration 805, the loss is 5.935450304763778, parameters k is 17.16824725121719 and b is -85.74979268147193\n",
      "Iteration 806, the loss is 5.9354215669545285, parameters k is 17.16800454370731 and b is -85.74927884747983\n",
      "Iteration 807, the loss is 5.935390943760571, parameters k is 17.167999543707307 and b is -85.74872548779604\n",
      "Iteration 808, the loss is 5.935360901822506, parameters k is 17.167994543707305 and b is -85.74817212811224\n",
      "Iteration 809, the loss is 5.935331765711303, parameters k is 17.167751836197425 and b is -85.74765829412014\n",
      "Iteration 810, the loss is 5.93530114251734, parameters k is 17.167746836197423 and b is -85.74710493443635\n",
      "Iteration 811, the loss is 5.935271498881243, parameters k is 17.167741836197422 and b is -85.74655157475256\n",
      "Iteration 812, the loss is 5.935241964468086, parameters k is 17.167499128687542 and b is -85.74603774076046\n",
      "Iteration 813, the loss is 5.9352113412741225, parameters k is 17.16749412868754 and b is -85.74548438107666\n",
      "Iteration 814, the loss is 5.935182095939976, parameters k is 17.16748912868754 and b is -85.74493102139287\n",
      "Iteration 815, the loss is 5.935152163224861, parameters k is 17.16724642117766 and b is -85.74441718740077\n",
      "Iteration 816, the loss is 5.935121540030895, parameters k is 17.167241421177657 and b is -85.74386382771698\n",
      "Iteration 817, the loss is 5.935092692998709, parameters k is 17.167236421177655 and b is -85.74331046803319\n",
      "Iteration 818, the loss is 5.935062361981638, parameters k is 17.166993713667775 and b is -85.74279663404108\n",
      "Iteration 819, the loss is 5.935031844912737, parameters k is 17.166988713667774 and b is -85.74224327435729\n",
      "Iteration 820, the loss is 5.935003183932381, parameters k is 17.166746006157894 and b is -85.74172944036519\n",
      "Iteration 821, the loss is 5.934972560738411, parameters k is 17.166741006157892 and b is -85.7411760806814\n",
      "Iteration 822, the loss is 5.93494244197147, parameters k is 17.16673600615789 and b is -85.7406227209976\n",
      "Iteration 823, the loss is 5.934913382689153, parameters k is 17.16649329864801 and b is -85.7401088870055\n",
      "Iteration 824, the loss is 5.934882759495196, parameters k is 17.16648829864801 and b is -85.73955552732171\n",
      "Iteration 825, the loss is 5.934853039030199, parameters k is 17.166483298648007 and b is -85.73900216763792\n",
      "Iteration 826, the loss is 5.934823581445934, parameters k is 17.166240591138127 and b is -85.73848833364582\n",
      "Iteration 827, the loss is 5.934792958251973, parameters k is 17.166235591138125 and b is -85.73793497396203\n",
      "Iteration 828, the loss is 5.934763636088938, parameters k is 17.166230591138124 and b is -85.73738161427823\n",
      "Iteration 829, the loss is 5.934733780202709, parameters k is 17.165987883628244 and b is -85.73686778028613\n",
      "Iteration 830, the loss is 5.934703157008745, parameters k is 17.165982883628242 and b is -85.73631442060234\n",
      "Iteration 831, the loss is 5.934674233147674, parameters k is 17.16597788362824 and b is -85.73576106091855\n",
      "Iteration 832, the loss is 5.934643978959483, parameters k is 17.16573517611836 and b is -85.73524722692645\n",
      "Iteration 833, the loss is 5.934613385061698, parameters k is 17.16573017611836 and b is -85.73469386724265\n",
      "Iteration 834, the loss is 5.934584800910229, parameters k is 17.16548746860848 and b is -85.73418003325055\n",
      "Iteration 835, the loss is 5.934554177716258, parameters k is 17.165482468608477 and b is -85.73362667356676\n",
      "Iteration 836, the loss is 5.934523982120432, parameters k is 17.165477468608476 and b is -85.73307331388297\n",
      "Iteration 837, the loss is 5.934494999667003, parameters k is 17.165234761098596 and b is -85.73255947989087\n",
      "Iteration 838, the loss is 5.934464376473031, parameters k is 17.165229761098594 and b is -85.73200612020707\n",
      "Iteration 839, the loss is 5.934434579179169, parameters k is 17.165224761098592 and b is -85.73145276052328\n",
      "Iteration 840, the loss is 5.934405198423775, parameters k is 17.164982053588712 and b is -85.73093892653118\n",
      "Iteration 841, the loss is 5.934374575229811, parameters k is 17.16497705358871 and b is -85.73038556684739\n",
      "Iteration 842, the loss is 5.934345176237902, parameters k is 17.16497205358871 and b is -85.7298322071636\n",
      "Iteration 843, the loss is 5.934315397180552, parameters k is 17.16472934607883 and b is -85.7293183731715\n",
      "Iteration 844, the loss is 5.934284773986591, parameters k is 17.164724346078827 and b is -85.7287650134877\n",
      "Iteration 845, the loss is 5.934255773296634, parameters k is 17.164719346078826 and b is -85.72821165380391\n",
      "Iteration 846, the loss is 5.934225595937325, parameters k is 17.164476638568946 and b is -85.72769781981181\n",
      "Iteration 847, the loss is 5.934194972743369, parameters k is 17.164471638568944 and b is -85.72714446012802\n",
      "Iteration 848, the loss is 5.934166370355366, parameters k is 17.164466638568943 and b is -85.72659110044422\n",
      "Iteration 849, the loss is 5.934135794694101, parameters k is 17.164223931059063 and b is -85.72607726645212\n",
      "Iteration 850, the loss is 5.9341055222693955, parameters k is 17.16421893105906 and b is -85.72552390676833\n",
      "Iteration 851, the loss is 5.934076616644846, parameters k is 17.16397622354918 and b is -85.72501007277623\n",
      "Iteration 852, the loss is 5.934045993450878, parameters k is 17.16397122354918 and b is -85.72445671309244\n",
      "Iteration 853, the loss is 5.9340161193281284, parameters k is 17.163966223549178 and b is -85.72390335340864\n",
      "Iteration 854, the loss is 5.933986815401628, parameters k is 17.163723516039298 and b is -85.72338951941654\n",
      "Iteration 855, the loss is 5.933956192207663, parameters k is 17.163718516039296 and b is -85.72283615973275\n",
      "Iteration 856, the loss is 5.933926716386866, parameters k is 17.163713516039294 and b is -85.72228280004896\n",
      "Iteration 857, the loss is 5.933897014158396, parameters k is 17.163470808529414 and b is -85.72176896605686\n",
      "Iteration 858, the loss is 5.9338663909644405, parameters k is 17.163465808529413 and b is -85.72121560637306\n",
      "Iteration 859, the loss is 5.933837313445593, parameters k is 17.16346080852941 and b is -85.72066224668927\n",
      "Iteration 860, the loss is 5.933807212915177, parameters k is 17.16321810101953 and b is -85.72014841269717\n",
      "Iteration 861, the loss is 5.933776589721216, parameters k is 17.16321310101953 and b is -85.71959505301338\n",
      "Iteration 862, the loss is 5.933747910504334, parameters k is 17.163208101019528 and b is -85.71904169332959\n",
      "Iteration 863, the loss is 5.93371741167195, parameters k is 17.162965393509648 and b is -85.71852785933748\n",
      "Iteration 864, the loss is 5.933687062418355, parameters k is 17.162960393509646 and b is -85.71797449965369\n",
      "Iteration 865, the loss is 5.933658233622696, parameters k is 17.162717685999766 and b is -85.71746066566159\n",
      "Iteration 866, the loss is 5.933627610428729, parameters k is 17.162712685999765 and b is -85.7169073059778\n",
      "Iteration 867, the loss is 5.933597659477095, parameters k is 17.162707685999763 and b is -85.716353946294\n",
      "Iteration 868, the loss is 5.933568432379472, parameters k is 17.162464978489883 and b is -85.7158401123019\n",
      "Iteration 869, the loss is 5.933537809185508, parameters k is 17.16245997848988 and b is -85.71528675261811\n",
      "Iteration 870, the loss is 5.93350825653583, parameters k is 17.16245497848988 and b is -85.71473339293432\n",
      "Iteration 871, the loss is 5.933478631136245, parameters k is 17.16221227098 and b is -85.71421955894222\n",
      "Iteration 872, the loss is 5.93344800794228, parameters k is 17.162207270979998 and b is -85.71366619925843\n",
      "Iteration 873, the loss is 5.933418853594555, parameters k is 17.162202270979996 and b is -85.71311283957463\n",
      "Iteration 874, the loss is 5.9333888298930235, parameters k is 17.161959563470116 and b is -85.71259900558253\n",
      "Iteration 875, the loss is 5.933358206699058, parameters k is 17.161954563470115 and b is -85.71204564589874\n",
      "Iteration 876, the loss is 5.933329450653289, parameters k is 17.161949563470113 and b is -85.71149228621495\n",
      "Iteration 877, the loss is 5.933299028649799, parameters k is 17.161706855960233 and b is -85.71097845222285\n",
      "Iteration 878, the loss is 5.933268602567319, parameters k is 17.16170185596023 and b is -85.71042509253905\n",
      "Iteration 879, the loss is 5.933239850600539, parameters k is 17.16145914845035 and b is -85.70991125854695\n",
      "Iteration 880, the loss is 5.933209227406571, parameters k is 17.16145414845035 and b is -85.70935789886316\n",
      "Iteration 881, the loss is 5.933179199626055, parameters k is 17.161449148450348 and b is -85.70880453917937\n",
      "Iteration 882, the loss is 5.9331500493573195, parameters k is 17.161206440940468 and b is -85.70829070518727\n",
      "Iteration 883, the loss is 5.933119426163353, parameters k is 17.161201440940467 and b is -85.70773734550347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 884, the loss is 5.933089796684782, parameters k is 17.161196440940465 and b is -85.70718398581968\n",
      "Iteration 885, the loss is 5.933060248114094, parameters k is 17.160953733430585 and b is -85.70667015182758\n",
      "Iteration 886, the loss is 5.9330296249201195, parameters k is 17.160948733430583 and b is -85.70611679214379\n",
      "Iteration 887, the loss is 5.933000393743519, parameters k is 17.16094373343058 and b is -85.70556343246\n",
      "Iteration 888, the loss is 5.932970446870869, parameters k is 17.1607010259207 and b is -85.7050495984679\n",
      "Iteration 889, the loss is 5.932939823676909, parameters k is 17.1606960259207 and b is -85.7044962387841\n",
      "Iteration 890, the loss is 5.932910990802244, parameters k is 17.1606910259207 and b is -85.70394287910031\n",
      "Iteration 891, the loss is 5.932880645627644, parameters k is 17.16044831841082 and b is -85.70342904510821\n",
      "Iteration 892, the loss is 5.932850142716287, parameters k is 17.160443318410817 and b is -85.70287568542442\n",
      "Iteration 893, the loss is 5.932821467578389, parameters k is 17.160200610900937 and b is -85.70236185143231\n",
      "Iteration 894, the loss is 5.93279084438442, parameters k is 17.160195610900935 and b is -85.70180849174852\n",
      "Iteration 895, the loss is 5.932760739775016, parameters k is 17.160190610900933 and b is -85.70125513206473\n",
      "Iteration 896, the loss is 5.932731666335164, parameters k is 17.159947903391053 and b is -85.70074129807263\n",
      "Iteration 897, the loss is 5.932701043141193, parameters k is 17.15994290339105 and b is -85.70018793838884\n",
      "Iteration 898, the loss is 5.932671336833754, parameters k is 17.15993790339105 and b is -85.69963457870504\n",
      "Iteration 899, the loss is 5.932641865091942, parameters k is 17.15969519588117 and b is -85.69912074471294\n",
      "Iteration 900, the loss is 5.932611241897971, parameters k is 17.15969019588117 and b is -85.69856738502915\n",
      "Iteration 901, the loss is 5.932581933892484, parameters k is 17.159685195881167 and b is -85.69801402534536\n",
      "Iteration 902, the loss is 5.932552063848708, parameters k is 17.159442488371287 and b is -85.69750019135326\n",
      "Iteration 903, the loss is 5.932521440654747, parameters k is 17.159437488371285 and b is -85.69694683166946\n",
      "Iteration 904, the loss is 5.932492530951218, parameters k is 17.159432488371284 and b is -85.69639347198567\n",
      "Iteration 905, the loss is 5.932462262605491, parameters k is 17.159189780861404 and b is -85.69587963799357\n",
      "Iteration 906, the loss is 5.932431682865238, parameters k is 17.159184780861402 and b is -85.69532627830978\n",
      "Iteration 907, the loss is 5.932403084556231, parameters k is 17.158942073351522 and b is -85.69481244431768\n",
      "Iteration 908, the loss is 5.9323724613622675, parameters k is 17.15893707335152 and b is -85.69425908463388\n",
      "Iteration 909, the loss is 5.932342279923974, parameters k is 17.15893207335152 and b is -85.69370572495009\n",
      "Iteration 910, the loss is 5.9323132833130074, parameters k is 17.15868936584164 and b is -85.69319189095799\n",
      "Iteration 911, the loss is 5.932282660119041, parameters k is 17.158684365841637 and b is -85.6926385312742\n",
      "Iteration 912, the loss is 5.932252876982707, parameters k is 17.158679365841635 and b is -85.6920851715904\n",
      "Iteration 913, the loss is 5.932223482069777, parameters k is 17.158436658331755 and b is -85.6915713375983\n",
      "Iteration 914, the loss is 5.932192858875823, parameters k is 17.158431658331754 and b is -85.69101797791451\n",
      "Iteration 915, the loss is 5.932163474041444, parameters k is 17.158426658331752 and b is -85.69046461823072\n",
      "Iteration 916, the loss is 5.932133680826557, parameters k is 17.158183950821872 and b is -85.68995078423862\n",
      "Iteration 917, the loss is 5.932103057632592, parameters k is 17.15817895082187 and b is -85.68939742455483\n",
      "Iteration 918, the loss is 5.93207407110018, parameters k is 17.15817395082187 and b is -85.68884406487103\n",
      "Iteration 919, the loss is 5.932043879583339, parameters k is 17.15793124331199 and b is -85.68833023087893\n",
      "Iteration 920, the loss is 5.93201325638937, parameters k is 17.157926243311987 and b is -85.68777687119514\n",
      "Iteration 921, the loss is 5.931984668158918, parameters k is 17.157921243311986 and b is -85.68722351151135\n",
      "Iteration 922, the loss is 5.931954078340109, parameters k is 17.157678535802106 and b is -85.68670967751925\n",
      "Iteration 923, the loss is 5.931923820072942, parameters k is 17.157673535802104 and b is -85.68615631783545\n",
      "Iteration 924, the loss is 5.931894900290853, parameters k is 17.157430828292224 and b is -85.68564248384335\n",
      "Iteration 925, the loss is 5.931864277096886, parameters k is 17.157425828292222 and b is -85.68508912415956\n",
      "Iteration 926, the loss is 5.931834417131675, parameters k is 17.15742082829222 and b is -85.68453576447577\n",
      "Iteration 927, the loss is 5.931805099047628, parameters k is 17.15717812078234 and b is -85.68402193048367\n",
      "Iteration 928, the loss is 5.931774475853663, parameters k is 17.15717312078234 and b is -85.68346857079987\n",
      "Iteration 929, the loss is 5.931745014190411, parameters k is 17.157168120782337 and b is -85.68291521111608\n",
      "Iteration 930, the loss is 5.931715297804408, parameters k is 17.156925413272457 and b is -85.68240137712398\n",
      "Iteration 931, the loss is 5.931684674610439, parameters k is 17.156920413272456 and b is -85.68184801744019\n",
      "Iteration 932, the loss is 5.931655611249139, parameters k is 17.156915413272454 and b is -85.6812946577564\n",
      "Iteration 933, the loss is 5.931625496561184, parameters k is 17.156672705762574 and b is -85.6807808237643\n",
      "Iteration 934, the loss is 5.931594873367216, parameters k is 17.156667705762572 and b is -85.6802274640805\n",
      "Iteration 935, the loss is 5.931566208307872, parameters k is 17.15666270576257 and b is -85.67967410439671\n",
      "Iteration 936, the loss is 5.931535695317957, parameters k is 17.15641999825269 and b is -85.67916027040461\n",
      "Iteration 937, the loss is 5.931505360221901, parameters k is 17.15641499825269 and b is -85.67860691072082\n",
      "Iteration 938, the loss is 5.9314765172687, parameters k is 17.15617229074281 and b is -85.67809307672871\n",
      "Iteration 939, the loss is 5.931445894074741, parameters k is 17.156167290742808 and b is -85.67753971704492\n",
      "Iteration 940, the loss is 5.931415957280641, parameters k is 17.156162290742806 and b is -85.67698635736113\n",
      "Iteration 941, the loss is 5.931386716025473, parameters k is 17.155919583232926 and b is -85.67647252336903\n",
      "Iteration 942, the loss is 5.93135609283151, parameters k is 17.155914583232924 and b is -85.67591916368524\n",
      "Iteration 943, the loss is 5.931326554339369, parameters k is 17.155909583232923 and b is -85.67536580400144\n",
      "Iteration 944, the loss is 5.931296914782252, parameters k is 17.155666875723043 and b is -85.67485197000934\n",
      "Iteration 945, the loss is 5.931266291588287, parameters k is 17.15566187572304 and b is -85.67429861032555\n",
      "Iteration 946, the loss is 5.931237151398096, parameters k is 17.15565687572304 and b is -85.67374525064176\n",
      "Iteration 947, the loss is 5.931207113539028, parameters k is 17.15541416821316 and b is -85.67323141664966\n",
      "Iteration 948, the loss is 5.931176490345067, parameters k is 17.155409168213158 and b is -85.67267805696586\n",
      "Iteration 949, the loss is 5.931147748456837, parameters k is 17.155404168213156 and b is -85.67212469728207\n",
      "Iteration 950, the loss is 5.931117312295801, parameters k is 17.155161460703276 and b is -85.67161086328997\n",
      "Iteration 951, the loss is 5.931086900370871, parameters k is 17.155156460703274 and b is -85.67105750360618\n",
      "Iteration 952, the loss is 5.931058134246541, parameters k is 17.154913753193394 and b is -85.67054366961408\n",
      "Iteration 953, the loss is 5.931027511052585, parameters k is 17.154908753193393 and b is -85.66999030993028\n",
      "Iteration 954, the loss is 5.930997497429599, parameters k is 17.15490375319339 and b is -85.66943695024649\n",
      "Iteration 955, the loss is 5.930968333003328, parameters k is 17.15466104568351 and b is -85.66892311625439\n",
      "Iteration 956, the loss is 5.930937709809356, parameters k is 17.15465604568351 and b is -85.6683697565706\n",
      "Iteration 957, the loss is 5.930908094488327, parameters k is 17.154651045683508 and b is -85.6678163968868\n",
      "Iteration 958, the loss is 5.930878531760103, parameters k is 17.154408338173628 and b is -85.6673025628947\n",
      "Iteration 959, the loss is 5.930847908566134, parameters k is 17.154403338173626 and b is -85.66674920321091\n",
      "Iteration 960, the loss is 5.9308186915470635, parameters k is 17.154398338173625 and b is -85.66619584352712\n",
      "Iteration 961, the loss is 5.930788730516875, parameters k is 17.154155630663745 and b is -85.66568200953502\n",
      "Iteration 962, the loss is 5.930758107322918, parameters k is 17.154150630663743 and b is -85.66512864985123\n",
      "Iteration 963, the loss is 5.9307292886058045, parameters k is 17.15414563066374 and b is -85.66457529016743\n",
      "Iteration 964, the loss is 5.930698929273649, parameters k is 17.15390292315386 and b is -85.66406145617533\n",
      "Iteration 965, the loss is 5.9306684405198276, parameters k is 17.15389792315386 and b is -85.66350809649154\n",
      "Iteration 966, the loss is 5.930639751224391, parameters k is 17.15365521564398 and b is -85.66299426249944\n",
      "Iteration 967, the loss is 5.930609128030429, parameters k is 17.153650215643978 and b is -85.66244090281565\n",
      "Iteration 968, the loss is 5.930579037578563, parameters k is 17.153645215643976 and b is -85.66188754313185\n",
      "Iteration 969, the loss is 5.930549949981171, parameters k is 17.153402508134096 and b is -85.66137370913975\n",
      "Iteration 970, the loss is 5.930519326787206, parameters k is 17.153397508134095 and b is -85.66082034945596\n",
      "Iteration 971, the loss is 5.930489634637295, parameters k is 17.153392508134093 and b is -85.66026698977217\n",
      "Iteration 972, the loss is 5.930460148737945, parameters k is 17.153149800624213 and b is -85.65975315578007\n",
      "Iteration 973, the loss is 5.930429525543979, parameters k is 17.15314480062421 and b is -85.65919979609627\n",
      "Iteration 974, the loss is 5.9304002316960265, parameters k is 17.15313980062421 and b is -85.65864643641248\n",
      "Iteration 975, the loss is 5.930370347494724, parameters k is 17.15289709311433 and b is -85.65813260242038\n",
      "Iteration 976, the loss is 5.930339724300754, parameters k is 17.15289209311433 and b is -85.65757924273659\n",
      "Iteration 977, the loss is 5.930310828754756, parameters k is 17.152887093114327 and b is -85.6570258830528\n",
      "Iteration 978, the loss is 5.930280546251496, parameters k is 17.152644385604447 and b is -85.6565120490607\n",
      "Iteration 979, the loss is 5.930249980668781, parameters k is 17.152639385604445 and b is -85.6559586893769\n",
      "Iteration 980, the loss is 5.930221368202241, parameters k is 17.152396678094565 and b is -85.6554448553848\n",
      "Iteration 981, the loss is 5.930190745008276, parameters k is 17.152391678094563 and b is -85.65489149570101\n",
      "Iteration 982, the loss is 5.9301605777275235, parameters k is 17.15238667809456 and b is -85.65433813601722\n",
      "Iteration 983, the loss is 5.9301315669590124, parameters k is 17.15214397058468 and b is -85.65382430202511\n",
      "Iteration 984, the loss is 5.930100943765047, parameters k is 17.15213897058468 and b is -85.65327094234132\n",
      "Iteration 985, the loss is 5.930071174786252, parameters k is 17.15213397058468 and b is -85.65271758265753\n",
      "Iteration 986, the loss is 5.930041765715794, parameters k is 17.1518912630748 and b is -85.65220374866543\n",
      "Iteration 987, the loss is 5.930011142521825, parameters k is 17.151886263074797 and b is -85.65165038898164\n",
      "Iteration 988, the loss is 5.929981771844995, parameters k is 17.151881263074795 and b is -85.65109702929784\n",
      "Iteration 989, the loss is 5.929951964472563, parameters k is 17.151638555564915 and b is -85.65058319530574\n",
      "Iteration 990, the loss is 5.92992134127861, parameters k is 17.151633555564914 and b is -85.65002983562195\n",
      "Iteration 991, the loss is 5.929892368903724, parameters k is 17.151628555564912 and b is -85.64947647593816\n",
      "Iteration 992, the loss is 5.929862163229342, parameters k is 17.151385848055032 and b is -85.64896264194606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 993, the loss is 5.929831540035374, parameters k is 17.15138084805503 and b is -85.64840928226226\n",
      "Iteration 994, the loss is 5.929802965962462, parameters k is 17.15137584805503 and b is -85.64785592257847\n",
      "Iteration 995, the loss is 5.929772361986118, parameters k is 17.15113314054515 and b is -85.64734208858637\n",
      "Iteration 996, the loss is 5.929742117876483, parameters k is 17.151128140545147 and b is -85.64678872890258\n",
      "Iteration 997, the loss is 5.929713183936862, parameters k is 17.150885433035267 and b is -85.64627489491048\n",
      "Iteration 998, the loss is 5.929682560742897, parameters k is 17.150880433035265 and b is -85.64572153522668\n",
      "Iteration 999, the loss is 5.929652714935225, parameters k is 17.150875433035264 and b is -85.64516817554289\n"
     ]
    }
   ],
   "source": [
    "#参数初始化\n",
    "k = random.random() * 200 - 100  # -100 100\n",
    "b = random.random() * 200 - 100  # -100 100\n",
    "learning_rate = 1e-2\n",
    "iteration_num = 1000\n",
    "losses = []\n",
    "for i in range(iteration_num):\n",
    "    price_use_current_parameters = [price(r, k, b) for r in X_rm]  # \\hat{y}\n",
    "    #计算loss\n",
    "    current_loss = loss(y, price_use_current_parameters)\n",
    "    losses.append(current_loss)\n",
    "    print(\"Iteration {}, the loss is {}, parameters k is {} and b is {}\".format(i,current_loss,k,b))\n",
    "    #更新梯度\n",
    "    k_gradient = partial_derivative_k(X_rm, y, price_use_current_parameters)\n",
    "    b_gradient = partial_derivative_b(y, price_use_current_parameters)\n",
    "    k = k + (-1 * k_gradient) * learning_rate\n",
    "    b = b + (-1 * b_gradient) * learning_rate\n",
    "best_k = k\n",
    "best_b = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a6509cfa58>]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAHadJREFUeJzt3Xl0VYW99vHvLwPEMAcCQogkgYRKnY0Ijkyxar0Od2lfvbal1V7qLXXA+67Wrvuut2/Xeu9d7b1dgrRqS52wWmur3mptqy+TIg7U4IAgSAIECDKEeSYDv/eP7NgUAxnOOdnn7PN81so6Z++zD+fZ2awnv+wzxNwdERGJroywA4iISGKp6EVEIk5FLyIScSp6EZGIU9GLiEScil5EJOJU9CIiEaeiFxGJOBW9iEjEZYUdAGDQoEFeVFQUdgwRkZSybNmyHe6e3952SVH0RUVFVFZWhh1DRCSlmNmGjmynUzciIhGnohcRiTgVvYhIxLVb9Gb2mJltN7MVbdz2P83MzWxQsGxmNtvMqs1suZmdl4jQIiLScR2Z6J8Arjx+pZkVAhXAxlarrwJKg69pwMOxRxQRkVi0W/TuvhjY1cZNM4HvAa3/csl1wJPe7B2gv5kNjUtSERHpki6dozeza4HN7v7hcTcVAJtaLdcG60REJCSdLnozywX+Dfjfbd3cxro2/1ahmU0zs0ozq6yrq+tsDABqdhzkR39cSUPTsS7dX0QkHXRloh8JFAMfmlkNMBx4z8xOpXmCL2y17XDg07b+EXef4+7l7l6en9/uG7vatG7HAR5/s4YX3qvt0v1FRNJBp4ve3T9y98HuXuTuRTSX+3nuvhV4Cfh68OqbccBed98S38h/M3H0YM4u7M/sBdXUN2qqFxFpS0deXvkM8DYw2sxqzez2k2z+Z2AdUA38CvhOXFKeOBszppSyec9hfr9sU/t3EBFJQ+1+1o2739LO7UWtrjswPfZYHXd5WT7nndafny+s5sbzh9MzK7M7H15EJOml/DtjzYx7K0azZe8Rnn1XU72IyPFSvugBLh41kLFFeTy4qJojDU1hxxERSSqRKHozY0ZFGdv2HeU3Sze2fwcRkTQSiaIHGD9yIONLBvLw62s5XK+pXkSkRWSKHmBGRRl1+4/y9NIOfRa/iEhaiFTRjy3O49LSQTz82loO1TeGHUdEJClEqugB7plSxs6D9Tz5tqZ6ERGIYNGfP2IAE0bn88vX13LgqKZ6EZHIFT3AjCll7D7UwNy3asKOIiISukgW/dmF/Zly+mDmLF7HviMNYccREQlVJIsems/V7z3cwONLasKOIiISqsgW/RkF/bhizBAeWbKOvYc01YtI+ops0UPzVL//SCOPLlkXdhQRkdBEuujHDOvL1WeeymNv1rD7YH3YcUREQhHpoge4e3IZB+sb+dUbmupFJD1FvuhHn9qHa84axhNv1bDzwNGw44iIdLvIFz3A3ZNLOdLQxBxN9SKShtKi6EcN7s115xTw5FsbqNuvqV5E0ktaFD3AXZNLqW86xi9fXxt2FBGRbpU2RV88qBc3nFvAr9/ZwPZ9R8KOIyLSbdKm6AHumlRK4zHnodc01YtI+mi36M3sMTPbbmYrWq37LzNbbWbLzey/zax/q9t+YGbVZvaJmX0pUcG74rSBudx0/nB+89eNbNl7OOw4IiLdoiMT/RPAlcetmwec4e5nAWuAHwCY2RjgZuCLwX0eMrPMuKWNg+kTR+HuPLRIU72IpId2i97dFwO7jlv3/9y95cPe3wGGB9evA37r7kfdfT1QDYyNY96YFeblclN5Ib99dyOb92iqF5Hoi8c5+tuAvwTXC4BNrW6rDdYllekTR2EYP19YHXYUEZGEi6nozezfgEbg6ZZVbWzmJ7jvNDOrNLPKurq6WGJ0WkH/U7h5bCG/r9zEpl2HuvWxRUS6W5eL3symAtcAt7p7S5nXAoWtNhsOfNrW/d19jruXu3t5fn5+V2N02XcmjCIjw/jZwqpuf2wRke7UpaI3syuB7wPXunvrkfgl4GYz62lmxUAp8NfYY8bfqf1yuPXC03j+vc3U7DgYdhwRkYTpyMsrnwHeBkabWa2Z3Q78HOgDzDOzD8zsFwDuvhL4HfAx8Aow3d2bEpY+Rv8yYSTZmcZsTfUiEmFZ7W3g7re0sfrRk2z/78C/xxKquwzuk8PXxo3g0SXr+e7EUZTk9w47kohI3KXVO2Pb8u3LR9IzK5PZCzTVi0g0pX3RD+rdk6kXFfHih59SvX1/2HFEROIu7YseYNplJeRmZzJrvqZ6EYkeFT2Q16sH37y4mD99tIXVW/eFHUdEJK5U9IFvXVpM7x5ZPKCpXkQiRkUf6J/bg29eUsxfVmxl5ad7w44jIhI3KvpWbr+kmD45WTpXLyKRoqJvpd8p2fzzpSXM+3gbH9VqqheRaFDRH+ebFxfR75RsZs5fE3YUEZG4UNEfp09ONtMuK2Hh6u28v3F32HFERGKmom/D1IuKyOvVg5k6Vy8iEaCib0Pvnll8+7ISFq+pY9mGXe3fQUQkianoT+Br40cwqHcPZs7TVC8iqU1FfwK5PbK44/KRLKnewdJ1O8OOIyLSZSr6k/jquBHk9+mpV+CISEpT0Z9ETnYm0yeM5J11u3hr7Y6w44iIdImKvh03jz2NU/vmMHPeGv72p3FFRFKHir4dOdmZTJ84kndrdrOkWlO9iKQeFX0HfOWCQob1y+F+TfUikoJU9B3QMyuT704q5f2Ne3htTV3YcUREOkVF30E3nj+c4QNO0bl6EUk57Ra9mT1mZtvNbEWrdXlmNs/MqoLLAcF6M7PZZlZtZsvN7LxEhu9OPbIyuGtSKctr97Jg1faw44iIdFhHJvongCuPW3cfsMDdS4EFwTLAVUBp8DUNeDg+MZPDDecVMGJgrs7Vi0hKabfo3X0xcPwHvlwHzA2uzwWub7X+SW/2DtDfzIbGK2zYsjObp/qPt+zj1ZXbwo4jItIhXT1HP8TdtwAEl4OD9QXAplbb1QbrPsfMpplZpZlV1tWlzhOc150zjJJBvZg1fw3HjmmqF5HkF+8nY62NdW22obvPcfdydy/Pz8+Pc4zEycrM4O4ppazeup9XVm4NO46ISLu6WvTbWk7JBJctz07WAoWtthsOfNr1eMnpmrOGMWpwb2bOW0OTpnoRSXJdLfqXgKnB9anAi63Wfz149c04YG/LKZ4oycww7plSStX2A/zpo8jtnohETEdeXvkM8DYw2sxqzex24MdAhZlVARXBMsCfgXVANfAr4DsJSZ0Erj5jKKOH9GHWfE31IpLcstrbwN1vOcFNk9vY1oHpsYZKBRkZxoyKUu546j1e+nAzN5w7POxIIiJt0jtjY3DFmFM5fWhfHphfRWPTsbDjiIi0SUUfg4wMY8aUUmp2HuK/398cdhwRkTap6GNUMWYIZxT0ZfbCKho01YtIElLRx8jMuLeijE27DvP8stqw44iIfI6KPg4mjh7M2YX9+dnCauobNdWLSHJR0cdBy1S/ec9hfle5qf07iIh0IxV9nFxWOojzRwzgwUXVHG1sCjuOiMhnVPRx0jLVb9l7hGff1VQvIslDRR9HF40cyNjiPB5cVM2RBk31IpIcVPRx1DLVb9t3lN8s3Rh2HBERQEUfd+NKBnLRyIE89NpaDtdrqheR8KnoE2BGRRk7DhzlqXc2hB1FRERFnwgXFOVxaekgfvH6Wg4ebQw7joikORV9gtwzpYydB+t58m1N9SISLhV9gpw/YgATRufzy8Vr2X+kIew4IpLGVPQJNGNKGXsONTD3rZqwo4hIGlPRJ9DZhf2Zcvpg5ixexz5N9SISEhV9gt0zpYx9Rxp5bMn6sKOISJpS0SfYGQX9+NIXh/DoG+vZe0hTvYh0PxV9N7hnShn7jzbyyJJ1YUcRkTQUU9Gb2QwzW2lmK8zsGTPLMbNiM1tqZlVm9qyZ9YhX2FR1+tC+fPnMoTz+Zg27D9aHHUdE0kyXi97MCoC7gHJ3PwPIBG4GfgLMdPdSYDdwezyCprq7p5RysL6RX72hqV5Eulesp26ygFPMLAvIBbYAk4DngtvnAtfH+BiRUDakD/9w1jCeeKuGnQeOhh1HRNJIl4ve3TcDPwU20lzwe4FlwB53b3nffy1QEGvIqLhrcilHGpqYs1hTvYh0n1hO3QwArgOKgWFAL+CqNjb1E9x/mplVmlllXV1dV2OklFGDe3P9OQXMfbuGuv2a6kWke8Ry6mYKsN7d69y9AXgBuAjoH5zKARgOfNrWnd19jruXu3t5fn5+DDFSy52TS2locn7x+tqwo4hImoil6DcC48ws18wMmAx8DCwCbgy2mQq8GFvEaCke1Isbzi3gqXc2sG3fkbDjiEgaiOUc/VKan3R9D/go+LfmAN8H7jWzamAg8GgcckbKXZNKaTzmPPyapnoRSbys9jc5MXf/IfDD41avA8bG8u9G3WkDc7np/OH8ZulGvn15CUP7nRJ2JBGJML0zNiTTJ47CcR5cVB12FBGJOBV9SArzcvlKeSHPvruJ2t2Hwo4jIhGmog/R9ImjMExTvYgklIo+RMP6n8ItYwv5fWUtm3ZpqheRxFDRh+w7E0eRkWH8bGFV2FFEJKJU9CEb0jeHr144guff20zNjoNhxxGRCFLRJ4E7JpSQnWnM1lQvIgmgok8Cg/vk8PXxRfzh/c2srTsQdhwRiRgVfZKYdlkJPbMymb1AU72IxJeKPkkM6t2TqRcV8dKHn1K1bX/YcUQkQlT0SWTaZSXkZmcyS1O9iMSRij6J5PXqwTcvLuZPy7eweuu+sOOISESo6JPMty4tpk/PLGbN01QvIvGhok8y/XN7cNslxbyycisrNu8NO46IRICKPgnddkkxfXOymDVfU72IxE5Fn4T6nZLNP19awvxV21heuyfsOCKS4lT0SeobFxfRPzebmfPWhB1FRFKcij5J9cnJZtplJSz6pI73N+4OO46IpDAVfRKbOr6IvF49mKlz9SISAxV9EuvVM4s7Li9h8Zo6Kmt2hR1HRFKUij7JfW1cEYN692TmfJ2rF5Guianozay/mT1nZqvNbJWZjTezPDObZ2ZVweWAeIVNR6f0yOSOy0t4s3on76zbGXYcEUlBsU70DwCvuPsXgLOBVcB9wAJ3LwUWBMsSg6+OG0F+n57cP28N7h52HBFJMV0uejPrC1wGPArg7vXuvge4DpgbbDYXuD7WkOkuJzuT6RNG8tf1u3h7raZ6EemcWCb6EqAOeNzM3jezR8ysFzDE3bcABJeD45Az7d089jRO7ZujqV5EOi2Wos8CzgMedvdzgYN04jSNmU0zs0ozq6yrq4shRnrIyc5k+qRRVG7YzRtVO8KOIyIpJJairwVq3X1psPwczcW/zcyGAgSX29u6s7vPcfdydy/Pz8+PIUb6+Er5cAr6n6KpXkQ6pctF7+5bgU1mNjpYNRn4GHgJmBqsmwq8GFNC+UzPrEy+O2kUH2zaw2uf6LcgEemYWF91cyfwtJktB84B/gP4MVBhZlVARbAscXLj+cMpzNNULyIdlxXLnd39A6C8jZsmx/LvyollZ2Zw56RSvvfccuav2k7FmCFhRxKRJKd3xqagfzy3gBEDc5mpqV5EOkBFn4KyMjO4e3IpH2/Zx6srt4UdR0SSnIo+RV179jBK8nsxa/4ajh3TVC8iJ6aiT1EtU/3qrfv5y4qtYccRkSSmok9h15w1jFGDezNr/hqaNNWLyAmo6FNYZoZxz5RSqrYf4OXln4YdR0SSlIo+xV19xlBGD+nDA/OraGw6FnYcEUlCKvoUl5FhzKgoZd2Og7z0oaZ6Efk8FX0EXDHmVMYM7csDCzTVi8jnqegjoHmqL2PDzkO88P7msOOISJJR0UfElNMHc2ZBP2YvqKJBU72ItKKijwgz496KMmp3H+a5ZbVhxxGRJKKij5AJo/M5p7A/P19YTX2jpnoRaaaij5CWqX7znsP8rnJT2HFEJEmo6CPm0tJBlI8YwIOLqjnS0BR2HBFJAir6iGmZ6rfsPcKz72qqFxEVfSSNHzmQscV5mupFBFDRR1LLVL99/1GeXrox7DgiEjIVfUSNKxnIRSMH8vBr1Ryqbww7joiESEUfYTMqythxoJ6n3tkQdhQRCZGKPsIuKMrj0tJB/OL1dRw8qqleJF3FXPRmlmlm75vZy8FysZktNbMqM3vWzHrEHlO6akZFGbsO1jP37Zqwo4hISOIx0d8NrGq1/BNgpruXAruB2+PwGNJF5502gImj85mzeB37jzSEHUdEQhBT0ZvZcODLwCPBsgGTgOeCTeYC18fyGBK7GRVl7DnUwBNv1oQdRURCEOtEPwv4HtDywSoDgT3u3nJCuBYoaOuOZjbNzCrNrLKuri7GGHIyZw3vz5TTh/CrN9ax97CmepF00+WiN7NrgO3uvqz16jY2bfOvVrv7HHcvd/fy/Pz8rsaQDrpnSin7jjTy2JL1YUcRkW4Wy0R/MXCtmdUAv6X5lM0soL+ZZQXbDAf09+2SwBkF/bjyi6fy2JL17D2kqV4knXS56N39B+4+3N2LgJuBhe5+K7AIuDHYbCrwYswpJS7uqShl/9FGHlmyLuwoItKNEvE6+u8D95pZNc3n7B9NwGNIF3zh1L58+cyhPLZkPbsP1ocdR0S6SVyK3t1fc/drguvr3H2su49y95vc/Wg8HkPi4+4ppRxqaGLOG5rqRdKF3hmbZsqG9OEfzhrG3Ldq2HFAP4NF0oGKPg3dNbmUIw1NzFmsqV4kHajo09Cowb25/pwCnny7hu37j4QdR0QSTEWfpu6cXEpDk/OL1zTVi0Sdij5NFQ/qxT+eW8BTSzewbZ+mepEoU9GnsTsnlXLsmPPQouqwo4hIAqno09hpA3O5qXw4z/x1E5/uORx2HBFJEBV9mps+cRSO86CmepHIUtGnueEDcvkfFxTyu8pN1O4+FHYcEUkAFb0wfeIoDNNULxJRKnphaL9T+KcLT+P3lbVs3KmpXiRqVPQCwL9MGElmhvGzhVVhRxGROFPRCwBD+uZw64UjeOH9zazfcTDsOCISRyp6+cwdE0rIzjR+tkBTvUiUqOjlM4P75PD18UX84YPNVG8/EHYcEYkTFb38nW9fVkJOdiazNdWLRIaKXv7OwN49mXpREX9c/ilrtu0PO46IxIGKXj5n2qUl5GZn8sB8TfUiUaCil88Z0KsHt11SzJ8+2sKqLfvCjiMiMVLRS5u+dUkJfXpmMWv+mrCjiEiMulz0ZlZoZovMbJWZrTSzu4P1eWY2z8yqgssB8Ysr3aVfbja3X1rMqyu3sWLz3rDjiEgMYpnoG4F/dffTgXHAdDMbA9wHLHD3UmBBsCwp6LZLiumbo6leJNV1uejdfYu7vxdc3w+sAgqA64C5wWZzgetjDSnh6JuTzbTLSpi/ajsfbtoTdhwR6aK4nKM3syLgXGApMMTdt0DzDwNgcDweQ8LxjYuLGZCbzU9eWY27hx1HRLog5qI3s97A88A97t7hl2iY2TQzqzSzyrq6ulhjSIL07pnFXZNLeWvtTl5fo+MkkopiKnozy6a55J929xeC1dvMbGhw+1Bge1v3dfc57l7u7uX5+fmxxJAEu/XCEZyWl8uP/7KapmOa6kVSTSyvujHgUWCVu9/f6qaXgKnB9anAi12PJ8mgR1YG37tyNKu37ueF92rDjiMinRTLRH8x8DVgkpl9EHxdDfwYqDCzKqAiWJYU9+Uzh3L28H7cP28NRxqawo4jIp0Qy6tulri7uftZ7n5O8PVnd9/p7pPdvTS43BXPwBIOM+O+q05ny94jPP5mTdhxRKQT9M5Y6bDxIwcy+QuDeWhRNbsO1ocdR0Q6SEUvnfL9q77AwfpGfr5Qf0hcJFWo6KVTyob04abzC/n1OzVs2Kk/OSiSClT00mn3XlFGdmYGP/rjx3oTlUgKUNFLpw3pm8OMKWUsXL2deR9vCzuOiLRDRS9d8o2Lixg9pA8/+uPHHKpvDDuOiJyEil66JDszg/97wxls3nOYB/T3ZUWSmopeuuyCojxuGVvInMXreHvtzrDjiMgJqOglJv/ry2MoHtiLGc9+wG69tl4kKanoJSa9emYx+5Zz2XnwKHf99n0amo6FHUlEjqOil5idUdCP/7jhTN6o2sH3n1/OMX3CpUhSyQo7gETDTeWFbNl7hPvnraHpmPNfN55NjyzNESLJQEUvcXPnpFFkZRr/+conbNp1iJ/edDYl+b3DjiWS9jRySdyYGd+ZMIrZt5xL9fYDXPnAG/yfl1ayZe/hsKOJpDVN9BJ31549jAuL8/jpq5/w1DsbmPt2DeefNoDziwYwekgfigf1YmCvnuT17kGvHpk0/w0bEUkUS4bPKikvL/fKysqwY0gCbNp1iOeW1bJw9XY+2bqf+uNelZOdafTIzCArM4PszAyyM40MM07U/Sf7mWCc+MZE/yxJ9I+qRP8wTPiPWn3/T+jmCwr51qUlXbqvmS1z9/L2ttNELwlVmJfLjIoyZlSU0dB0jA07D7Jh5yF2H2pg98F6dh+qp77xGA1Nx2g45jQ0HqPpRMPHSWaSk40riR5mEj0qJXoWS3z+1P7+J/oBBvXumdgHQEUv3Sg7M4NRg/swanCfsKOIpBU9GSsiEnEqehGRiFPRi4hEXMKK3syuNLNPzKzazO5L1OOIiMjJJaTozSwTeBC4ChgD3GJmYxLxWCIicnKJmujHAtXuvs7d64HfAtcl6LFEROQkElX0BcCmVsu1wToREelmiSr6tt5G9ndvOzCzaWZWaWaVdXV1CYohIiKJesNULVDYank48GnrDdx9DjAHwMzqzGxDFx9rELCji/dNVdrn9KB9Tg+x7POIjmyUkM+6MbMsYA0wGdgMvAv8k7uvTMBjVXbksx6iRPucHrTP6aE79jkhE727N5rZd4FXgUzgsUSUvIiItC9hn3Xj7n8G/pyof19ERDomCu+MnRN2gBBon9OD9jk9JHyfk+Lz6EVEJHGiMNGLiMhJpHTRR/XzdMys0MwWmdkqM1tpZncH6/PMbJ6ZVQWXA4L1Zmazg+/DcjM7L9w96BozyzSz983s5WC52MyWBvv7rJn1CNb3DJarg9uLwswdCzPrb2bPmdnq4HiPj/JxNrMZwf/pFWb2jJnlRPE4m9ljZrbdzFa0Wtfp42pmU4Ptq8xsalfzpGzRR/zzdBqBf3X304FxwPRg3+4DFrh7KbAgWIbm70Fp8DUNeLj7I8fF3cCqVss/AWYG+7sbuD1Yfzuw291HATOD7VLVA8Ar7v4F4Gya9z+Sx9nMCoC7gHJ3P4PmV+TdTDSP8xPAlcet69RxNbM84IfAhTR/rMwPW344dJq7p+QXMB54tdXyD4AfhJ0rQfv6IlABfAIMDdYNBT4Jrv8SuKXV9p9tlypfNL+pbgEwCXiZ5ndX7wCyjj/eNL9sd3xwPSvYzsLehy7sc19g/fHZo3qc+dtHo+QFx+1l4EtRPc5AEbCiq8cVuAX4Zav1f7ddZ75SdqInTT5PJ/h19VxgKTDE3bcABJeDg82i8L2YBXwPaPnr4QOBPe7eGCy33qfP9je4fW+wfaopAeqAx4NTVo+YWS8iepzdfTPwU2AjsIXm47aM6B/nFp09rnE73qlc9O1+nk6qM7PewPPAPe6+72SbtrEuZb4XZnYNsN3dl7Ve3cam3oHbUkkWcB7wsLufCxzkb7/OtyWl9zs47XAdUAwMA3rRfNrieFE7zu050X7Gbf9Tuejb/TydVGZm2TSX/NPu/kKwepuZDQ1uHwpsD9an+vfiYuBaM6uh+SOtJ9E84fcPPk4D/n6fPtvf4PZ+wK7uDBwntUCtuy8Nlp+jufijepynAOvdvc7dG4AXgIuI/nFu0dnjGrfjncpF/y5QGjxj34PmJ3VeCjlTXJiZAY8Cq9z9/lY3vQS0PPM+leZz9y3rvx48ez8O2NvyK2IqcPcfuPtwdy+i+TgudPdbgUXAjcFmx+9vy/fhxmD7lJv03H0rsMnMRgerJgMfE9HjTPMpm3Fmlhv8H2/Z30gf51Y6e1xfBa4wswHBb0NXBOs6L+wnLGJ8suNqmj88bS3wb2HnieN+XULzr2jLgQ+Cr6tpPj+5AKgKLvOC7Y3mVyCtBT6i+VUNoe9HF/d9AvBycL0E+CtQDfwe6BmszwmWq4PbS8LOHcP+ngNUBsf6D8CAKB9n4EfAamAF8GugZxSPM/AMzc9DNNA8md/eleMK3BbsfzXwza7m0TtjRUQiLpVP3YiISAeo6EVEIk5FLyIScSp6EZGIU9GLiEScil5EJOJU9CIiEaeiFxGJuP8PZVVEWIoP/7wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(iteration_num)),losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a650932ef0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# price_use_best_parameters = [price(r, best_k, best_b) for r in X_rm]\n",
    "\n",
    "plt.scatter(X_rm,y)\n",
    "plt.scatter(X_rm,price_use_current_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "+ 是否将Loss改成了“绝对值”(3')\n",
    "+ 是否完成了偏导的重新定义(5')\n",
    "+ 新的模型Loss是否能够收敛 (11’)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
